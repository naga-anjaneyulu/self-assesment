QuestionID,Question,Difficulty,Answer
Ques_1,What is Data Science?,Easy,Data science continues to evolve as one of the most promising and in-demand career paths for skilled professionals. Today  successful data professionals understand that they must advance past the traditional skills of analyzing large amounts of data  data mining  and programming skills. In order to uncover useful intelligence for their organizations  data scientists must master the full spectrum of the data science life cycle and possess a level of flexibility and understanding to maximize returns at each phase of the process.
Ques_2,What is the differences between supervised and unsupervised learning.,Easy,Supervised learning and Unsupervised learning are machine learning tasks. Supervised learning is simply a process of learning algorithm from the training dataset. Supervised learning is where you have input variables and an output variable and you use an algorithm to learn the mapping function from the input to the output. The aim is to approximate the mapping function so that when we have new input data we can predict the output variables for that data. Unsupervised learning is modeling the underlying or hidden structure or distribution in the data in order to learn more about the data. Unsupervised learning is where you only have input data and no corresponding output variables.
Ques_3,What are the important skills to have in Python with regard to data analysis?,Medium,The following are some of the important skills to possess which will come handy when performing data analysis using Python. Good understanding of the built-in data types especially lists  dictionaries  tuples  and sets. Mastery of N-dimensional NumPy Arrays. Mastery of Pandas dataframes. Ability to perform element-wise vector and matrix operations on NumPy arrays. Knowing that you should use the Anaconda distribution and the conda package manager. Familiarity with Scikit-learn. **Scikit-Learn Cheat Sheet** Ability to write efficient list comprehensions instead of traditional for loops. Ability to write small  clean functions (important for any developer)  preferably pure functions that donâ€™t alter objects. Knowing how to profile the performance of a Python script and how to optimize bottlenecks. The following will help to tackle any problem in data analytics and machine learning.
Ques_4,What is Selection Bias?,Easy,Selection bias is a kind of error that occurs when the researcher decides who is going to be studied. It is usually associated with research where the selection of participants isnâ€™t random. It is sometimes referred to as the selection effect. It is the distortion of statistical analysis  resulting from the method of collecting samples. If the selection bias is not taken into account  then some conclusions of the study may not be accurate.  
Ques_5,What is the goal of A/B Testing?,Medium,It is a statistical hypothesis testing for a randomized experiment with two variables A and B. The goal of A/B Testing is to identify any changes to the web page to maximize or increase the outcome of an interest. A/B testing is a fantastic method for figuring out the best online promotional and marketing strategies for your business. It can be used to test everything from website copy to sales emails to search ads An example of this could be identifying the click-through rate for a banner ad.
Ques_6,What do you understand by statistical power of sensitivity and how do you calculate it?,Hard,Sensitivity is commonly used to validate the accuracy of a classifier (Logistic  SVM  Random Forest etc.). Sensitivity is nothing but â€œPredicted True events/ Total eventsâ€. True events here are the events which were true and model also predicted them as true. Calculation of seasonality is pretty straightforward. Seasonality = ( True Positives ) / ( Positives in Actual Dependent Variable )
Ques_7,What are the differences between overfitting and underfitting?,Easy,In statistics and machine learning  one of the most common tasks is to fit a model to a set of training data  so as to be able to make reliable predictions on general untrained data. In overfitting  a statistical model describes random error or noise instead of the underlying relationship. Overfitting occurs when a model is excessively complex  such as having too many parameters relative to the number of observations. A model that has been overfit has poor predictive performance  as it overreacts to minor fluctuations in the training data. Underfitting occurs when a statistical model or machine learning algorithm cannot capture the underlying trend of the data. Underfitting would occur  for example  when fitting a linear model to non-linear data. Such a model too would have poor predictive performance.
Ques_8,How does data cleaning plays a vital role in analysis?,Easy,Data cleaning can help in analysis because: Cleaning data from multiple sources helps to transform it into a format that data analysts or data scientists can work with. Data Cleaning helps to increase the accuracy of the model in machine learning. It is a cumbersome process because as the number of data sources increases  the time taken to clean the data increases exponentially due to the number of sources and the volume of data generated by these sources. It might take up to 80% of the time for just cleaning data making it a critical part of analysis task.
Ques_9,Differentiate between univariate  bivariate and multivariate analysis.,Medium,Univariate analyses are descriptive statistical analysis techniques which can be differentiated based on the number of variables involved at a given point of time. For example  the pie charts of sales based on territory involve only one variable and can the analysis can be referred to as univariate analysis. The bivariate analysis attempts to understand the difference between two variables at a time as in a scatterplot. For example  analyzing the volume of sale and spending can be considered as an example of bivariate analysis. Multivariate analysis deals with the study of more than two variables to understand the effect of variables on the responses.
Ques_10,What is Cluster Sampling?,Medium,Cluster sampling is a technique used when it becomes difficult to study the target population spread across a wide area and simple random sampling cannot be applied. Cluster Sample is a probability sample where each sampling unit is a collection or cluster of elements. For eg.  A researcher wants to survey the academic performance of high school students in Japan. He can divide the entire population of Japan into different clusters (cities). Then the researcher selects a number of clusters depending on his research through simple or systematic random sampling.
Ques_11,What are Eigenvectors and Eigenvalues?,Hard,Eigenvectors are used for understanding linear transformations. In data analysis  we usually calculate the eigenvectors for a correlation or covariance matrix. Eigenvectors are the directions along which a particular linear transformation acts by flipping  compressing or stretching. Eigenvalue can be referred to as the strength of the transformation in the direction of eigenvector or the factor by which the compression occurs.
Ques_12,Can you cite some examples where a false positive is important than a false negative?,Medium,False Positives are the cases where you wrongly classified a non-event as an event a.k.a Type I error. False Negatives are the cases where you wrongly classify events as non-events  a.k.a Type II error. Example 1: In the medical field  assume you have to give chemotherapy to patients. Assume a patient comes to that hospital and he is tested positive for cancer  based on the lab prediction but he actually doesnâ€™t have cancer. This is a case of false positive. Here it is of utmost danger to start chemotherapy on this patient when he actually does not have cancer. In the absence of cancerous cell  chemotherapy will do certain damage to his normal healthy cells and might lead to severe diseases  even cancer. Example 2: Letâ€™s say an e-commerce company decided to give $1000 Gift voucher to the customers whom they assume to purchase at least $10 000 worth of items. They send free voucher mail directly to 100 customers without any minimum purchase condition because they assume to make at least 20% profit on sold items above $10 000. Now the issue is if we send the $1000 gift vouchers to customers who have not actually purchased anything but are marked as having made $10 000 worth of purchase.
Ques_13,Can you explain the difference between a Validation Set and a Test Set?,Medium,A Validation set can be considered as a part of the training set as it is used for parameter selection and to avoid overfitting of the model being built. On the other hand  a Test Set is used for testing or evaluating the performance of a trained machine learning model. In simple terms  the differences can be summarized as; training set is to fit the parameters i.e. weights and test set is to assess the performance of the model i.e. evaluating the predictive power and generalization.
Ques_14,Explain cross-validation,Easy,Cross-validation is a model validation technique for evaluating how the outcomes of statistical analysis will generalize to an Independent dataset. Mainly used in backgrounds where the objective is forecast and one wants to estimate how accurately a model will accomplish in practice. The goal of cross-validation is to term a data set to test the model in the training phase (i.e. validation data set) in order to limit problems like overfitting and get an insight on how the model will generalize to an independent data set.
Ques_15,What is logistic regression? State an example when you have used logistic regression recently.,Easy,Logistic Regression often referred as logit model is a technique to predict the binary outcome from a linear combination of predictor variables. For example  if you want to predict whether a particular political leader will win the election or not. In this case  the outcome of prediction is binary i.e. 0 or 1 (Win/Lose). The predictor variables here would be the amount of money spent for election campaigning of a particular candidate  the amount of time spent in campaigning  etc.
Ques_16,What are Recommender Systems?,Medium,Recommender Systems are a subclass of information filtering systems that are meant to predict the preferences or ratings that a user would give to a product. Recommender systems are widely used in movies  news  research articles  products  social tags  music  etc. Examples include movie recommenders in IMDB  Netflix & BookMyShow  product recommenders in e-commerce sites like Amazon  eBay & Flipkart  YouTube video recommendations and game recommendations in Xbox.
Ques_17,What is Collaborative filtering?,Medium,Like many machine learning techniques  a recommender system makes prediction based on usersâ€™ historical behaviors. Specifically  itâ€™s to predict user preference for a set of items based on past experience. To build a recommender system  the most two popular approaches are Content-based and Collaborative Filtering. Content-based approach requires a good amount of information of itemsâ€™ own features  rather than using usersâ€™ interactions and feedbacks. For example  it can be movie attributes such as genre  year  director  actor etc.  or textual content of articles that can extracted by applying Natural Language Processing. Collaborative Filtering  on the other hand  doesnâ€™t need anything else except usersâ€™ historical preference on a set of items. Because itâ€™s based on historical data  the core assumption here is that the users who have agreed in the past tend to also agree in the future. In terms of user preference  it usually expressed by two categories. Explicit Rating  is a rate given by a user to an item on a sliding scale  like 5 stars for Titanic. This is the most direct feedback from users to show how much they like an item. Implicit Rating  suggests users preference indirectly  such as page views  clicks  purchase records  whether or not listen to a music track  and so on. In this article  I will take a close look at collaborative filtering that is a traditional and powerful tool for recommender systems.
Ques_18,How can outlier values be treated?,Medium,Outlier values can be identified by using univariate or any other graphical analysis method. If the number of outlier values is few then they can be assessed individually but for a large number of outliers  the values can be substituted with either the 99th or the 1st percentile values. All extreme values are not outlier values. The most common ways to treat outlier values can be: 1. To change the value and bring in within a range. 2. To just remove the value.
Ques_19,During analysis  how do you treat missing values?,Medium,The extent of the missing values is identified after identifying the variables with missing values. If any patterns are identified the analyst has to concentrate on them as it could lead to interesting and meaningful business insights. If there are no patterns identified  then the missing values can be substituted with mean or median values (imputation) or they can simply be ignored. Assigning a default value which can be mean  minimum or maximum value. Getting into the data is important. If it is a categorical variable  the default value is assigned. The missing value is assigned a default value. If you have a distribution of data coming  for normal distribution give the mean value. If 80% of the values for a variable are missing then you can answer that you would be dropping the variable instead of treating the missing values.
Ques_20,How will you define the number of clusters in a clustering algorithm?,Hard,A number of empirical approaches have been used to determine the number of clusters in a data set. They usually fit into two categories: Model fitting techniques: an example is using a mixture model to fit with your data  and determine the optimum number of components; or use density estimation techniques  and test for the number of modes (see here.) Sometimes  the fit is compared with that of a model where observations are uniformly distributed on the entire support domain  thus with no cluster; you may have to estimate the support domain in question  and assume that it is not  made of disjoint sub-domains; in many cases  the convex hull of your data set  as an estimate of the support domain  is good enough. Visual techniques: for instance  the silhouette or elbow rule (very popular.) In both cases  you need a criterion to determine the optimum number of clusters. In the case of the elbow rule  one typically uses the percentage of unexplained variance.
Ques_21,What do you mean by Deep Learning and Why has it become popular now?,Easy,Deep Learning is nothing but a paradigm of machine learning which has shown incredible promise in recent years. This is because of the fact that Deep Learning shows a great analogy with the functioning of the human brain. Now although Deep Learning has been around for many years  the major breakthroughs from these techniques came just in recent years. This is because of two main reasons: The increase in the amount of data generated through various sources The growth in hardware resources required to run these models GPUs are multiple times faster and they help us build bigger and deeper deep learning models in comparatively less time than we required previously
Ques_22,What are Artificial Neural Networks?,Easy,Artificial Neural networks are a specific set of algorithms that have revolutionized machine learning. They are inspired by biological neural networks. Neural Networks can adapt to changing input so the network generates the best possible result without needing to redesign the output criteria.
Ques_23,Describe the structure of Artificial Neural Networks?,Medium,Artificial Neural Networks works on the same principle as a biological Neural Network. It consists of inputs which get processed with weighted sums and Bias  with the help of Activation Functions.
Ques_24,Explain Gradient Descent.,Medium,To Understand Gradient Descent  Letâ€™s understand what is a Gradient first. A gradient measures how much the output of a function changes if you change the inputs a little bit. It simply measures the change in all weights with regard to the change in error. You can also think of a gradient as the slope of a function. Gradient Descent can be thought of climbing down to the bottom of a valley  instead of climbing up a hill.  This is because it is a minimization algorithm that minimizes a given function (Activation Function).
Ques_25,What is Back Propagation and Explain itâ€™s Working.,Hard,Backpropagation is a training algorithm used for multilayer neural network. In this method  we move the error from an end of the network to all weights inside the network and thus allowing efficient computation of the gradient. It has the following steps: 1. Forward Propagation of Training Data 2. Derivatives are computed using output and target 3. Back Propagate for computing derivative of error wrt output activation 4. Using previously calculated derivatives for output 5. Update the Weights
Ques_26,What are the variants of Back Propagation?,Hard,Stochastic Gradient Descent: We use only single training example for calculation of gradient and update parameters. Batch Gradient Descent: We calculate the gradient for the whole dataset and perform the update at each iteration. Mini-batch Gradient Descent: Itâ€™s one of the most popular optimization algorithms. Itâ€™s a variant of Stochastic Gradient Descent and here instead of single training example  mini-batch of samples is used.
Ques_27,What is the role of Activation Function?,Medium,The Activation function is used to introduce non-linearity into the neural network helping it to learn more complex function. Without which the neural network would be only able to learn linear function which is a linear combination of its input data. An activation function is a function in an artificial neuron that delivers an output based on inputs
Ques_28,What is an Auto-Encoder?,Hard,Autoencoders are simple learning networks that aim to transform inputs into outputs with the minimum possible error. This means that we want the output to be as close to input as possible. We add a couple of layers between the input and the output  and the sizes of these layers are smaller than the input layer. The autoencoder receives unlabeled input which is then encoded to reconstruct the input.
Ques_29,What is a Boltzmann Machine?,Hard,Boltzmann machines have a simple learning algorithm that allows them to discover interesting features that represent complex regularities in the training data. The Boltzmann machine is basically used to optimize the weights and the quantity for the given problem. The learning algorithm is very slow in networks with many layers of feature detectors. â€œRestricted Boltzmann Machinesâ€ algorithm has a single layer of feature detectors which makes it faster than the rest.
Ques_30,How is KNN different from k-means clustering?,Medium,K-Nearest Neighbors is a supervised classification algorithm  while k-means clustering is an unsupervised clustering algorithm. While the mechanisms may seem similar at first  what this really means is that in order for K-Nearest Neighbors to work  you need labeled data you want to classify an unlabeled point into (thus the nearest neighbor part). K-means clustering requires only a set of unlabeled points and a threshold: the algorithm will take unlabeled points and gradually learn how to cluster them into groups by computing the mean of the distance between different points. The critical difference here is that KNN needs labeled points and is thus supervised learning  while k-means doesnâ€™t â€” and is thus unsupervised learning.
Ques_31,Explain how a ROC curve works.,Hard,The ROC curve is a graphical representation of the contrast between true positive rates and the false positive rate at various thresholds. Itâ€™s often used as a proxy for the trade-off between the sensitivity of the model (true positives) vs the fall-out or the probability it will trigger a false alarm (false positives).
Ques_32,Why is â€œNaiveâ€ Bayes naive?,Medium,Despite its practical applications  especially in text mining  Naive Bayes is considered â€œNaiveâ€ because it makes an assumption that is virtually impossible to see in real-life data: the conditional probability is calculated as the pure product of the individual probabilities of components. This implies the absolute independence of features â€” a condition probably never met in real life.
Ques_33,Explain the difference between L1 and L2 regularization,Hard,L2 regularization tends to spread error among all the terms  while L1 is more binary/sparse  with many variables either being assigned a 1 or 0 in weighting. L1 corresponds to setting a Laplacean prior on the terms  while L2 corresponds to a Gaussian prior.
Ques_34,Whatâ€™s the difference between Type I and Type II error?,Medium,Type I error is a false positive  while Type II error is a false negative. Briefly stated  Type I error means claiming something has happened when it hasnâ€™t  while Type II error means that you claim nothing is happening when in fact something is. A clever way to think about this is to think of Type I error as telling a man he is pregnant  while Type II error means you tell a pregnant woman she isnâ€™t carrying a baby.
Ques_35,Whatâ€™s the difference between a generative and discriminative model?,Hard,A generative model will learn categories of data while a discriminative model will simply learn the distinction between different categories of data. Discriminative models will generally outperform generative models on classification tasks.
Ques_36,How is a decision tree pruned?,Medium,Pruning is what happens in decision trees when branches that have weak predictive power are removed in order to reduce the complexity of the model and increase the predictive accuracy of a decision tree model. Pruning can happen bottom-up and top-down  with approaches such as reduced error pruning and cost complexity pruning. Reduced error pruning is perhaps the simplest version: replace each node. If it doesnâ€™t decrease predictive accuracy  keep it pruned. While simple  this heuristic actually comes pretty close to an approach that would optimize for maximum accuracy.
Ques_37,How would you handle an imbalanced dataset?,Hard,An imbalanced dataset is when you have  for example  a classification test and 90% of the data is in one class. That leads to problems: an accuracy of 90% can be skewed if you have no predictive power on the other category of data! Here are a few tactics to get over the hump: 1- Collect more data to even the imbalances in the dataset. 2- Resample the dataset to correct for imbalances. 3- Try a different algorithm altogether on your dataset. Whatâ€™s important here is that you have a keen sense for what damage an unbalanced dataset can cause  and how to balance that.
Ques_38,Name an example where ensemble techniques might be useful.,Hard,Ensemble techniques use a combination of learning algorithms to optimize better predictive performance. They typically reduce overfitting in models and make the model more robust (unlikely to be influenced by small changes in the training data). You could list some examples of ensemble methods  from bagging to boosting to a â€œbucket of modelsâ€ method and demonstrate how they could increase predictive power.
Ques_39,What evaluation approaches would you work to gauge the effectiveness of a machine learning model?,Medium,You would first split the dataset into training and test sets  or perhaps use cross-validation techniques to further segment the dataset into composite sets of training and test sets within the data. You should then implement a choice selection of performance metrics: here is a fairly comprehensive list. You could use measures such as the F1 score  the accuracy  and the confusion matrix. Whatâ€™s important here is to demonstrate that you understand the nuances of how a model is measured and how to choose the right performance measures for the right situations.
Ques_40,Whatâ€™s the â€œkernel trickâ€ and how is it useful?,Medium,The Kernel trick involves kernel functions that can enable in higher-dimension spaces without explicitly calculating the coordinates of points within that dimension: instead  kernel functions compute the inner products between the images of all pairs of data in a feature space. This allows them the very useful attribute of calculating the coordinates of higher dimensions while being computationally cheaper than the explicit calculation of said coordinates. Many algorithms can be expressed in terms of inner products. Using the kernel trick enables us effectively run algorithms in a high-dimensional space with lower-dimensional data.
Ques_41,Do you have experience with Spark or big data tools for machine learning?,Medium,Youâ€™ll want to get familiar with the meaning of big data for different companies and the different tools theyâ€™ll want. Spark is the big data tool most in demand now  able to handle immense datasets with speed. Be honest if you donâ€™t have experience with the tools demanded  but also take a look at job descriptions and see what tools pop up: youâ€™ll want to invest in familiarizing yourself with them.
Ques_42,Pick an algorithm. Write the psuedo-code for a parallel implementation.,Medium,This kind of question demonstrates your ability to think in parallelism and how you could handle concurrency in programming implementations dealing with big data. Take a look at pseudocode frameworks such as Peril-L and visualization tools such as Web Sequence Diagrams to help you demonstrate your ability to write code that reflects parallelism.
Ques_43,How would you approach the â€œNetflix Prizeâ€ competition?,Medium,The Netflix Prize was a famed competition where Netflix offered $1 000 000 for a better collaborative filtering algorithm. The team that won called BellKor had a 10% improvement and used an ensemble of different methods to win. Some familiarity with the case and its solution will help demonstrate youâ€™ve paid attention to machine learning for a while.
Ques_44,How do you think Google is training data for self-driving cars?,Medium,In order to answer this question well  you will need to have knowledge in computer vision and deep learning. Basically building a self driving car comes down to 3 key tasks 1) Precise Localization 2) Obstacle Detection 3) Path Planning
Ques_45,What is the significance of TF-IDF?,Medium,Tf-idf stands for term frequency-inverse document frequency  and the tf-idf weight is a weight often used in information retrieval and text mining. This weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus. The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus. Variations of the tf-idf weighting scheme are often used by search engines as a central tool in scoring and ranking a document's relevance given a user query. One of the simplest ranking functions is computed by summing the tf-idf for each query term; many more sophisticated ranking functions are variants of this simple model. Tf-idf can be successfully used for stop-words filtering in various subject fields including text summarization and classification.
Ques_46,What is latent semantic indexing? Where it is applied.,Hard,Latent semantic analysis (LSA) is a technique in natural language processing  in particular distributional semantics  of analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms. LSA assumes that words that are close in meaning will occur in similar pieces of text (the distributional hypothesis). A matrix containing word counts per document (rows represent unique words and columns represent each document) is constructed from a large piece of text and a mathematical technique called singular value decomposition (SVD) is used to reduce the number of rows while preserving the similarity structure among columns. Documents are then compared by taking the cosine of the angle between the two vectors (or the dot product between the normalizations of the two vectors) formed by any two columns. Values close to 1 represent very similar documents while values close to 0 represent very dissimilar documents
Ques_47,Explain dependency parsing in NLP?,Medium,Dependency Parsing is also known as Syntactic Parsing. It is the task of recognizing a sentence and assigning a syntactic structure to it. The most widely used syntactic structure is the parse tree which can be generated using some parsing algorithms. These parse trees are useful in various applications like grammar checking or more importantly it plays a critical role in the semantic analysis stage.
Ques_48,Explain the Masked Language Model?,Hard,Masked language modeling is an example of autoencoding language modeling (the output is reconstructed from corrupted input) - we typically mask one or more of words in a sentence and have the model predict those masked words given the other words in sentence. By training the model with such an objective  it can essentially learn certain (but not all) statistical properties of word sequences. BERT is a model that is trained on a masked language modeling objective.
Ques_49,There are several tagging using for processing natural languages. In all those tagging part of speech (POS)  tagging is one of the popular ones in our industry. Please explain in details about part of speech (POS) tagging  and how it can be used properly?,Medium,Part of speech tagger is a very interesting and most important tool for processing natural language with proper manner. This part of speech (POS) tagger is a normal tool or software which helps for reading some critical text independent of any languages  then assign entire sentence in part of speech for each word or some other tokenization logic define in the software  such as adjective  verb or noun etc. It is normally holding some specific algorithm which helps to label some of the terms in the entire text body. It has some varieties categories which are more complex than define above utility. The above define functionality is one of the very basic features of the POS tag.
Ques_50,There have some classification model define in NLP. What kind of features can be followed by NLP for improving accuracy in the classification model?,Medium,There have several classifications followed by NLP  explaining the same below: Counting frequency of define terms. Notation of vector for every sentence. Part of Speech (POS) tagging. Grammatical dependency or some define dictionary or library.
Ques_51,What is cryptography?,Easy,ryptography is a specialized area of cybersecurity  but it has a broad array of applications that we will examine later. Kaspersky Lab has defined it as follows: â€œCryptography is the study of secure communications techniques that allow only the sender and intended recipient of a message to view its contents. In addition  cryptography also covers the obfuscation of information in images using techniques such as microdots or merging.â€ 
Ques_52,What is encryption and decryption?,Easy,The terms â€œscramblingâ€ and â€œdescramblingâ€ are commonly known. In terms of decryption  scrambling and descrambling are also known as â€œencryptionâ€ and â€œdecryption.â€ For example: when the written message â€œI AM AWARE OF THE FACTâ€ is scrambled by the sending party  it becomes what is known as the â€œencrypted message.â€ This means that the written message has been disguised in such a manner that it would be totally meaningless  or in the terms of cryptography  it would be undecipherable. Encryption can also be described as conversion of information from a readable state to apparent nonsense. When the receiving party receives this encrypted written message  it must be unscrambled into an understandable and comprehensible state of context. This process of unscrambling is also known as decryption
Ques_53,What is ciphertext?,Easy,When the message is encrypted into a state which is totally incomprehensible and undecipherable  this is known as the ciphertext. So  to illustrate all of this  with the previous example  when the sending party creates the written message of â€œI LOVE YOUâ€  this is the plaintext or the cleartext. Once this message is encrypted into the format of â€œUYO I VEOLâ€ and while it is in transit  it becomes known as the ciphertext. Then  once the receiving party gets this ciphertext and then decrypts it into a comprehensible and understandable form of â€œI LOVE YOU â€ this message then becomes the plaintext or the cleartext again.
Ques_54,What is the difference between a private key and a public key?,Medium,  one of the main purposes of cryptography is to scramble forms of content and images into an undecipherable state. You may be wondering how this is all exactly done. The answer is that it primarily involves the use of a key. Traditionally  this is a private key. With this particular key  the sending party can encrypt the plaintext  and from there the content or image will be sent in its garbled state across the network medium to the receiving party. A private key is private to the sender or the receiver  while a public key may be available to a group.
Ques_55,What are symmetric and asymmetric key systems?,Medium,A symmetric key system uses only the private key  and the asymmetric key system makes use of both the public key and the private key. The latter used primarily in what is known as a Public Key Infrastructure  or PKI for short. It will be discussed in more detail later on.
Ques_56,What kinds of threats exist for a cryptographic system?,Medium,There are three traditional types of attacks  and they are as follows: Ciphertext-only attack: With this type of attack  only the ciphertext is known to the attacker. But if this particular individual is well-trained in statistics  then he or she can use various statistical techniques to break the ciphertext back into the plaintext Known-plaintext attack: This occurs when the hacker knows some aspect of either the letter pairings; thus  they can consequently crack the ciphertext back into the plaintext Chosen-plaintext attack: With this type of attack  the hacker can choose the plaintext and view the encrypted output which is being transmitted across the network medium. From this  they can reverse-engineer it back into its ciphertext form in an attempt to figure out the specific encryption scheme
Ques_57,What is polyalphabetic encryption?,Medium,A polyalphabetic cipher is simply a substitution cipher that uses multiple alphabets for substitution.
Ques_58,What is cipher block chaining?,Medium,The initialization vectors are part of a larger process known as cipher block chaining  or CBC. Within this methodology  multiple loops of encryption are created in order to further totally scramble the ciphertext. Here is the how the process works: The Initialization Vector is created first Through a mathematical process known as XOR (which stands for exclusive OR and is used quite frequently to determine if the bits of two strings of data match or not)  the first created Initialization Vector is XORâ€™d with the first block of ciphertext data The first chunk of data which has been XORâ€™d is further broken down by another layer of encryption This process is then continued until all of the blocks of ciphertext have been XORâ€™d and enveloped with another layer of encryption
Ques_59,How is a Key Distribution Center (KDC) used?,Medium,The Key Distribution Center consists of a database of all of the end users at the place of business or corporation and their respective passwords  as well other trusted servers and computers along the network.  If an end user wishes to communicate with another end user on a different computer system    the sending party enters their password into the KDC using a specialized software called â€œKerberos.â€ When the password is received by the KDC  the Kerberos then uses a special mathematical algorithm which adds the receiving partyâ€™s information and converts it over to a   cryptographic key.  Once this encrypted key has been established  the KDC then sets up and establishes other keys for the encryption of the communication session between the sending and the receiving party. These other keys are also referred to as tickets. These tickets will actually expire at a predetermined point in time in order to prevent unauthorized use  and it would also be rendered useless if it is stolen  hijacked or intercepted by a third party.
Ques_60,What are the mathematical algorithms used in symmetric cryptography?,Medium,They are as follows: The Needham-Schroder algorithm The Digital Encryption Standard algorithm (DES) The Triple Digit Encryption Standard algorithm (3DES) The International Data Encryption Algorithm (IDEA) The Advanced Encryption Standard algorithm (AES) 
Ques_61,What is asymmetric key cryptography?,Hard, asymmetric cryptography can be likened to that of a safety deposit box at a local bank. In this example  there are normally two set of keys used. One key is the one which the bank gives to you. This can be referred to as the public key  because it is used over and over again. The second key is the private key which the bank keeps in their possession at all times  and only the bank personnel know where it is kept. The world of asymmetric cryptography is just like this example  though of course  it is much more complex than this in practice.  Let us refer to the public key as â€œpkâ€ and the private key as â€œsk.â€ So  to represent both of these keys together  it would be mathematically demonstrated as (pk  sk). It is then the sending party which uses the public key (pk) to encrypt the message they wish to send to the receiving party  which then uses the private key (sk) to decrypt the ciphertext from the sending party.
Ques_62,What are the key differences between asymmetric and symmetric cryptography?,Hard,With symmetric cryptography  the complete secrecy of the key must be assured. Whereas asymmetric cryptography requires only half of the secrecy  namely that of the private key (sk). Secondly  symmetric cryptography utilizes the same secret key for the encryption and decryption of the ciphertext  but in asymmetric cryptography two different keys (namely the public and the private keys) are used for the encryption and the decryption of the ciphertext. 
Ques_63,What is the Public Key Infrastructure (PKI)?,Hard,The PKI consists of the following components: The Certificate Authority (CA): This is the party who issues the digital certificates The Digital Certificate: This serves to verify the identity of the certificate holder and is issued by the CA. These digital certificates are typically kept in the local computer of the employee  or even the central server at the place of business or organization The LDAP or X.500 Directories: These are the databases which collect and distribute the digital certificates from the CA The Registration Authority (RA): If the place of business or organization is very large (such as a multinational corporation)  this entity usually handles and processes the requests for the required digital certificates and then transmits those requests to the CA to process and create the required digital certificates
Ques_64,What is the LDAP protocol and how is it used in a Public Key Infrastructure (PKI)?,Hard,LDAP is an acronym which stands for Lightweight Directory Access Protocol. This is a database protocol used for the updating and searching of the directories which run over the TCP/IP network protocol (this is the network protocol which is primarily used by the PKI infrastructure). It is the job of the LDAP server of the Public Key Infrastructure to contain information and data as it relates to the digital certificates and the public and the private key storage locations  as well as the matching public and private key labels. The Certificate Authority uses a combination of the end user name and the matching tags to specifically locate the digital certificates on the LDAP server. From that point onwards  the LDAP server checks to see if the requested digital certificate is valid or not  and it if it is valid  it then retrieves a digital certificate which can then be sent to the end user.  Although all digital certificates have a finite lifespan when they are first issued  they can also be revoked for any reason at any time by the Public Key Infrastructure Administrator.
Ques_65,What are the security vulnerabilities of hashing functions?,Hard,One major security vulnerability of using hashes is that they can be altered while it is en route. In other words  a cyber-attacker can intercept the ciphertext and its associated hash  alter both and create a brand-new ciphertext and hash.  As a result  the receiving party is fooled into believing that this new  altered ciphertext and new  altered hash are the original sent by the sending party while the cyber-attacker keeps the actual ciphertext and hash which was generated the first time around. To fix this  the ciphertext is combined with a â€œsecret keyâ€ at the point of origination first  then the hash is created. As a result  this hash will contain specific information and data about the secret itself. As a result  the receiving party can even be further convinced that the ciphertext they have received is the original one sent by the sending party. This is so because even if the ciphertext  the hash and the associated secret key were to be intercepted  there is very little that a hacker can do to alter the ciphertext and its associated hash. This is because they have to have the information and data about the secret key  which is of course something they will never gain access to.
Ques_66,What Is Scan Conversion?,Easy,A major task of the display processor is digitizing a picture definition given in an application program into a set of pixel-intensity values for storage in the frame buffer. This digitization process is called scan conversion.
Ques_67,What Is Rasterization?,Easy,The process of determining the appropriate pixels for representing picture or graphics object is known as rasterization.
Ques_68,What Are The Raster And Vector Graphics?,Medium, The Raster and Vector graphics can be explained as- RASTER- In computer graphics image  or BITMAP  is a dot matrix data structure representing a generally rectangular grid of pixels or points of color  viewable via a monitor  paper  or other display medium. Raster image are stored in image files with varying formats. VECTOR- Vector graphics is the use of geometrical primitives such as points  lines  curves  and shapes or polygon  which are all based on mathematical expressions  to represent image in computer graphics. â€œVectorâ€  in this context  implies more than a straight line.
Ques_69,What Is Scaling In Computer Graphics?,Medium, In computer graphic  image scaling is the process of resizing a digital image. scaling is a non-trivial process that involves a trade off between efficiency  smoothness and sharpness. With bitmap graphics  as the size of an image is reduced or enlarged  the pixels which comprise the image become increasingly visible  making the image appear â€œsoftâ€ if pixels are averaged  or jagged if not.
Ques_70,Define Aspect Ratio?,Medium,Aspect ratio is the ratio of the vertical points to horizontal points essential to produce equivalent length line in both direction on the screen. An aspect ratio of 3/4 defines that a vertical line plotted with three points has the same length as a horizontal lines plotted with four points.
Ques_71,What is the need for space partitioning representation?,Medium,Space partitioning representations are used to define interior methods  by partitioning the spatial domain including an object into a set of small non-overlapping  and contiguous solids. A common space partitioning description for a three object is an octree representation.
Ques_72,What is critical fusion frequency?,Hard,Frequency of light simulation at which it becomes perceived as a stable  continuous sensation. The frequency depends upon various factors like luminance  color  contrast  etc.
Ques_73,Difference between CMY and HSV color models.,Hard,CMY Model - A color model described with the primary colors cyan  magenta  and yellow (CMY) is useful for defining color output to hard-copy devices.Hard-copy devices such as plotters produce a Color image by coating a paper with color pigments.HSV Model - The HSV model uses color descriptors that have a more natural appeal to the user. Color function in this model is hue (H)  saturation (S) and value(V).To give color specification  a user selects a spectral color and the amounts of black and white that are to be added to obtain different shades  tints  and tones.
Ques_74,What is dithering?,Hard,The name dithering is used in different contexts. Primarily  it defines techniques for approximating halftones without reducing resolutions pixel: grid patterns do. But the term is also applied to halftone approximation methods using pixel grids and sometimes it is used to define to color halftone approximation only. Random values added to pixel intensities to breakup contours are referred to as dither noise.
Ques_75,Define Keyframe systems ?,Hard,Key-frame systems are specialized animation languages designed to generate the in-between frames from user-specified keyframes. Each object in the scene is described as a set of rigid bodies connected at the joints and with a limited number of degree of freedom. In-between frames are generated from the specification of two or more fey frames. Motion paths can be given by kinematic description as a set of spline curves or physically based by specifying the force acting on the object to be animated.
Ques_76,What is the Koch curve?,Medium,The Koch curve can be drawn by separate line into 4 equal segments with scaling method 1/3.  and middle 2 segments are so adapted that they form adjustment sides of an equilateral triangle.
Ques_77,What are Peano curves?,Medium,A fractal curve can fill the plane and therefore have a dimension of two. Such curves are called Peano curves.
Ques_78,What are the Spline curves?,Medium,The name spline is a flexible strip used to generate a smooth curve through a designated set of points. In computer Graphics  the name spline curves define to any combined curve create with polynomial portions fulfilling specified continuity methods at the edge of the pieces.
Ques_79,What is OOPS?,Easy,OOPS is abbreviated as Object Oriented Programming system in which programs are considered as a collection of objects. Each object is nothing but an instance of a class.
Ques_80,What is Encapsulation?,Easy,Encapsulation is an attribute of an object  and it contains all data which is hidden. That hidden data can be restricted to the members of that class. Levels are Public  Protected  Private  Internal  and Protected Internal
Ques_81,What is Polymorphism?,Easy,Polymorphism is nothing but assigning behavior or value in a subclass to something that was already declared in the main class. Simply  polymorphism takes more than one form.
Ques_82,What is Inheritance?,Easy,Inheritance is a concept where one class shares the structure and behavior defined in another class. If Inheritance applied to one class is called Single Inheritance  and if it depends on multiple classes  then it is called multiple Inheritance.
Ques_83,What is a virtual function?,Medium,A virtual function is a member function of a class  and its functionality can be overridden in its derived class. This function can be implemented by using a keyword called virtual  and it can be given during function declaration. A virtual function can be declared using a token(virtual) in C++. It can be achieved in C/Python Language by using function pointers or pointers to function.
Ques_84,What is the super keyword?,Medium,The super keyword is used to invoke the overridden method  which overrides one of its superclass methods. This keyword allows to access overridden methods and also to access hidden members of the superclass. It also forwards a call from a constructor  to a constructor in the superclass.
Ques_85,What are sealed modifiers?,Medium,Sealed modifiers are the access modifiers where the methods can not inherit it. Sealed modifiers can also be applied to properties  events  and methods. This modifier cannot be used to static members.
Ques_86,What is early and late Binding? ,Medium,Early binding refers to the assignment of values to variables during design time  whereas late Binding refers to the assignment of values to variables during run time.
Ques_87,What is dynamic or run time polymorphism?,Hard,Dynamic or Run time polymorphism is also known as method overriding in which call to an overridden function is resolved during run time  not at the compile time. It means having two or more methods with the same name  same signature but with different implementation.
Ques_88,What is a copy constructor?,Hard,This is a special constructor for creating a new object as a copy of an existing object. There will always be only one copy constructor that can be either defined by the user or the system.
Ques_89,What is Aggregation?,Hard,Aggregation is also known as â€œHAS-Aâ€ relationship. When class Car has a member reference variable of type Wheel then the relationship between the classes Car and Wheel is known as Aggregation. Aggregation can be understood as â€œwhole to its partsâ€ relationship. 
Ques_90,What is Composition?,Hard,Composition is a special form of Aggregation where the part cannot exist without the whole. Composition is a strong Association. Composition relationship is represented like aggregation with one difference that the diamond shape is filled. 
Ques_91,What is demand paging?,Easy,Demand paging is referred when not all of a processâ€™s pages are in the RAM  then the OS brings the missing(and required) pages from the disk into the RAM.
Ques_92,What is kernel?,Easy,A kernel is the core of every operating system. It connects applications to the actual processing of data. It also manages all communications between software and hardware components to ensure usability and reliability.
Ques_93,What is a virtual memory?,Easy,Virtual memory is a memory management technique for letting processes execute outside of memory. This is very useful especially is an executing program cannot fit in the physical memory.
Ques_94,What is a thread?,Easy,A thread is a basic unit of CPU utilization. In general  a thread is composed of a thread ID  program counter  register set  and the stack.
Ques_95,What is RR scheduling algorithm?,Medium,RR (round-robin) scheduling algorithm is primarily aimed for time-sharing systems. A circular queue is a setup in such a way that the CPU scheduler goes around that queue  allocating CPU to each process for a time interval of up to around 10 to 100 milliseconds.
Ques_96,Describe Bankerâ€™s algorithm,Medium,Bankerâ€™s algorithm is one form of deadlock-avoidance in a system. It gets its name from a banking system wherein the bank never allocates available cash in such a way that it can no longer satisfy the needs of all of its customers.
Ques_97,What are overlays?,Medium,Overlays are used to enable a process to be larger than the amount of memory allocated to it. The basic idea of this is that only instructions and data that are needed at any given time are kept in memory.
Ques_98,What is fragmentation?,Medium,Fragmentation is memory wasted. It can be internal if we are dealing with systems that have fixed-sized allocation units  or external if we are dealing with systems that have variable-sized allocation units.
Ques_99,What is a socket?,Medium,A socket provides a connection between two applications. Each endpoint of a communication is a socket.
Ques_100,What are the primary functions of VFS?,Medium,VFS  or Virtual File System  separate file system generic operations from their implementation by defining a clean VFS interface. It is based on a file-representation structure known as vnode  which contains a numerical designator needed to support network file systems.
Ques_101,What is spooling?,Hard,Spooling is normally associated with printing. When different applications want to send an output to the printer at the same time  spooling takes all of these print jobs into a disk file and queues them accordingly to the printer.
Ques_102,What is preemptive multitasking?,Hard,Preemptive multitasking allows an operating system to switch between software programs. This  in turn  allows multiple programs to run without necessarily taking complete control over the processor and resulting in system crashes.
Ques_103,What is plumbing/piping?,Hard,It is the process of using the output of one program as an input to another. For example  instead of sending the listing of a folder or drive to the main screen  it can be piped and sent to a file  or sent to the printer to produce a hard copy.
Ques_104,What is NOS?,Hard,NOS is short for Network Operating System. It is a specialized software that will allow a computer to communicate with other devices over the network  including file/folder sharing.
Ques_105,Explain the purpose of using a libaio package in Ubuntu?,Hard,Libaio is Linux Kernel Asynchronous I/O (A/O).  A/O allows even a single application thread to overlap I/O operations with other processing  by providing an interface for submitting one or more I/O requests in one system call without waiting for completion.  And a separate interface to reap completed I/O operations associated with a given completion group.
Ques_106,What is the use of volatile keyword?,Easy,The C's volatile keyword is a qualifier that tells the compiler not to optimize when applied to a variable. By declaring a variable volatile  we can tell the compiler that the value of the variable may change any moment from outside of the scope of the program. A variable should be declared volatile whenever its value could change unexpectedly and beyond the comprehension of the compiler.  In those cases it is required not to optimize the code  doing so may lead to erroneous result and load the variable every time it is used in the program. Volatile keyword is useful for memory-mapped peripheral registers  global variables modified by an interrupt service routine  global variables accessed by multiple tasks within a multi-threaded application.
Ques_107,Can a pointer be volatile?,Easy,If we see the declaration volatile int *p  it means that the pointer itself is not volatile and points to an integer that is volatile. This is to inform the compiler that pointer p is pointing to an integer and the value of that integer may change unexpectedly even if there is no code indicating so in the program.
Ques_108,What is ISR?,Easy,An ISR(Interrupt Service Routine) is an interrupt handler  a callback subroutine which is called when a interrupt is encountered.
Ques_109,What are inline functions?,Easy,The ARM compilers support inline functions with the keyword __inline. These functions have a small definition and the function body is substituted in each call to the inline function. The argument passing and stack maintenance is skipped and it results in faster code execution  but it increases code size  particularly if the inline function is large or one inline function is used often.
Ques_110,What is a RISC processor ?,Medium,RISC (Reduced Instruction Set Computer) could carry out a few sets of simple instructions simultaneously. Fewer transistors are used to manufacture RISC  which makes RISC cheaper. RISC has uniform instruction set and those instructions are also fewer in number. Due to the less number of instructions as well as instructions being simple  the RISC computers are faster. RISC emphasise on software rather than hardware. RISC can execute instructions in one machine cycle.
Ques_111,Whats is a CISC processor ?,Medium,CISC (Complex Instruction Set Computer) is capable of executing multiple operations through a single instruction. CISC have rich and complex instruction set and more number of addressing modes. CISC emphasise on hardware rather that software  making it costlier than RISC. It has a small code size  high cycles per second and it is slower compared to RISC.
Ques_112,What is RTOS?,Medium,In an operating system  there is a module called the scheduler  which schedules different tasks and determines when a process will execute on the processor. This way  the multi-tasking is achieved. The scheduler in a Real Time Operating System (RTOS) is designed to provide a predictable execution pattern. In an embedded system  a certain event must be entertained in strictly defined time. To meet real time requirements  the behaviour of the scheduler must be predictable. This type of OS which have a scheduler with predictable execution pattern is called Real Time OS(RTOS).
Ques_113,What is priority inversion?,Medium,If two tasks share a resource  the one with higher priority will run first. However  if the lower-priority task is using the shared resource when the higher-priority task becomes ready  then the higher-priority task must wait for the lower-priority task to finish. In this scenario  even though the task has higher priority it needs to wait for the completion of the lower-priority task with the shared resource. This is called priority inversion.
Ques_114,What is priority inheritance?,Medium,Priority inheritance is a solution to the priority inversion problem. The process waiting for any resource which has a resource lock will have the maximum priority. This is priority inheritance. When one or more high priority jobs are blocked by a job  the original priority assignment is ignored and execution of critical section will be assigned to the job with the highest priority in this elevated scenario. The job returns to the original priority level soon after executing the critical section.
Ques_115,What is semaphore?,Hard,Semaphore is actually a variable or abstract data type which controls access to a common resource by multiple processes. Semaphores are of two types -Binary semaphore â€“ It can have only two values (0 and 1). The semaphore value is set to 1 by the process in charge  when the resource is available.Counting semaphore â€“ It can have value greater than one. It is used to control access to a pool of resources.
Ques_116,What is spin lock?,Hard,If a resource is locked  a thread that wants to access that resource may repetitively check whether the resource is available. During that time  the thread may loop and check the resource without doing any useful work. Suck a lock is termed as spin lock.
Ques_117,Significance of watchdog timer in Embedded Systems ?,Hard,The watchdog timer is a timing device with a predefined time interval. During that interval  some event may occur or else the device generates a time out signal. It is used to reset to the original state whenever some inappropriate events take place which can result in system malfunction. It is usually operated by counter devices.
Ques_118,What is wild pointer?,Hard,A pointer that is not initialized to any valid address or NULL is considered as wild pointer.
Ques_119,What is dangling pointer?,Hard,If a pointer is de-allocated or freed and the pointer is not assigned to NULL  then it may still contain that address and accessing the pointer means that we are trying to access that location and it will give an error. This type of pointer is called dangling pointer.
Ques_120,Whatâ€™s the difference between an implicit and an explicit intent?,Easy,An explicit intent is where you tell the system which Activity or system component it should use to respond to this intent. Implicit intents allow you to declare the action you want to perform; the Android system will then check which components are registered to handle that action.
Ques_121,What is a ThreadPool?,Easy,ThreadPool consists of a task queue and a group of worker threads  which allows it to run multiple parallel instances of a task.Using ThreadPool is more efficient than having multiple operations waiting to run on a single thread  but it also helps you avoid the considerable overhead of creating and destroying a thread every time you require a worker thread.
Ques_122,What is the relationship between the lifecycle of an AsyncTask and the lifecycle of an Activity? ,Easy,An AsyncTask is not tied to the lifecycle of the Activity that contains it. If the Activity is destroyed and a new instance of the Activity is created  the AsyncTask wonâ€™t be destroyed. 
Ques_123,What is the difference between Serializable and Parcelable?,Easy,Serializable is a standard Java interface thatâ€™s easy to integrate into your app  as it doesnâ€™t require any methods. Despite being easy to implement  Serializable uses the Java reflection API  which makes it a slow process that creates lots of temporary objects.Parcelable is optimized for Android  so itâ€™s faster than Serializable. Itâ€™s also fully customizable  so you can be explicit about the serialization process  which results in less garbage objects.
Ques_124,What is an Adapter?,Medium,An Adapter acts as this bridge  and is also responsible for converting each data entry into a View that can then be added to the AdapterView.
Ques_125,What are containers?,Medium,Containers  as the name itself implies  holds objects and widgets together  depending on which specific items are needed and in what particular arrangement that is wanted. Containers may hold labels  fields  buttons  or even child containers  as examples. 
Ques_126,What is adb?,Medium,Adb is short for Android Debug Bridge. It allows developers the power to execute remote shell commands. Its basic function is to allow and control communication towards and from the emulator port. 
Ques_127,What is AIDL?,Medium,AIDL  or Android Interface Definition Language  handles the interface requirements between a client and a service so both can communicate at the same level through interprocess communication or IPC. This process involves breaking down objects into primitives that Android can understand. This part is required simply because a process cannot access the memory of the other process. 
Ques_128,How can ANR be prevented?,Hard,One technique that prevents the Android system from concluding a code that has been unresponsive for a long period of time is to create a child thread. Within the child thread  most of the actual tasks of the codes can be placed so that the main thread runs with minimal periods of unresponsive time.
Ques_129,Difference between Service & Intent Service?,Hard,Service is the base class for Android services that can be extended to create any service. A class that directly extends Service runs on the main thread so it will block the UI (if there is one) and should therefore either be used only for short tasks or should make use of other threads for longer tasks.IntentService is a subclass of Service that handles asynchronous requests (expressed as â€œIntentsâ€) on demand. Clients send requests through startService(Intent) calls. The service is started as needed  handles each Intent in turn using a worker thread  and stops itself when it runs out of work.
Ques_130,What is the onTrimMemory() method,Hard,onTrimMemory(): Called when the operating system has determined that it is a good time for a process to trim unneeded memory from its process. This will happen for example when it goes in the background and there is not enough memory to keep as many background processes running as desired.Android can reclaim memory for from your app in several ways or kill your app entirely if necessary to free up memory for critical tasks. To help balance the system memory and avoid the systemâ€™s need to kill your app process  you can implement the ComponentCallbacks2 interface in your Activity classes. The provided onTrimMemory() callback method allows your app to listen for memory related events when your app is in either the foreground or the background  and then release objects in response to app lifecycle or system events that indicate the system needs to reclaim memory
Ques_131,What is Android bound service ?,Hard,A bound service is a service that allows other android components (like activity) to bind to it and send and receive data. A bound service is a service that can be used not only by components running in the same process as local service  but activities and services  running in different processes  can bind to it and send and receive data.
Ques_132,What is Bitmap pooling in android?,Hard,Bitmap pooling is a simple technique  that aims to reuse bitmaps instead of creating new ones every time. When you need a bitmap  you check a bitmap stack to see if there are any bitmaps available. If there are not bitmaps available you create a new bitmap otherwise you pop a bitmap from the stack and reuse it. Then when you are done with the bitmap  you can put it on a stack.
Ques_133,What is Cocoa and Cocoa Touch?,Easy,Cocoa -Application development environments for OS X.Includes the Foundation and AppKit frameworks.Used to refer any class/object which is based on the Objective-C runtime & inherits from the root class.Cocoa Touch-Application development environments for iOS.Includes Foundation and UIKit frameworks.Used to refer the application development using any programmatic interface
Ques_134,What is Swift and what is Objective-C?,Easy,Objective-C is the primary programming language you use to write software for OS X and iOS. Itâ€™s a superset of the C programming language and provides object-oriented capabilities and a dynamic runtime. Objective-C inherits the syntax  primitive types  and flow control statements of C and adds syntax for defining classes and methods. It also adds language-level support for object graph management and object literals while providing dynamic typing and binding  deferring many responsibilities until runtime.Swift is a new programming language for iOS  OS X  watchOS  and tvOS apps that builds on the best of C and Objective-C  without the constraints of C compatibility. Swift adopts safe programming patterns and adds modern features to make programming easier  more flexible  and more fun. Swift feels familiar to Objective-C developers and is friendly to new programmers.
Ques_135,What is SpriteKit and what is SceneKit?,Easy,SpriteKit is a framework for easy development of animated 2D objects.SceneKit is a framework inherited from OS X that assists with 3D graphics rendering.SpriteKit  SceneKit  and Metal are expected to power a new generation of mobile games that redefine what iOS devicesâ€™ powerful GPUs can offer.
Ques_136,What are iBeacons?,Medium,iBeacon as Appleâ€™s technology standard which allows Mobile Apps to listen for signals from beacons in the physical world and react accordingly. iBeacon technology allows Mobile Apps to understand their position on a micro-local scale  and deliver hyper-contextual content to users based on location. The underlying communication technology is Bluetooth Low Energy.
Ques_137,What is autorealease pool?,Medium,Every time -autorelease is sent to an object  it is added to the inner-most autorelease pool. When the pool is drained  it simply sends -release to all the objects in the pool. Autorelease pools are a convenience that allows you to defer sending -release until â€œlaterâ€. That â€œlaterâ€ can happen in several places  but the most common in Cocoa GUI apps is at the end of the current run loop cycle.
Ques_138,What are layer objects?,Medium,Layer objects are data objects which represent visual content and are used by views to render their content. Custom layer objects can also be added to the interface to implement complex animations and other types of sophisticated visual effects.
Ques_139,What is ABI? ,Medium,ABIs are important when it comes to applications that use external libraries. If a program is built to use a particular library and that library is later updated  you donâ€™t want to have to re-compile that application (and from the end user's standpoint  you may not have the source). If the updated library uses the same ABI  then your program will not need to change.
Ques_140,What is Grand Central Dispatch (GCD) ?,Medium,GCD is a library that provides a low-level and object-based API to run tasks concurrently while managing threads behind the scenes. Terminology;Dispatch Queues  A dispatch queue is responsible for executing a task in the first-in  first-out order.Serial Dispatch Queue A serial dispatch queue runs tasks one at a time.Concurrent Dispatch Queue A concurrent dispatch queue runs as many tasks as it can without waiting for the started tasks to finish.Main Dispatch Queue A globally available serial queue that executes tasks on the applicationâ€™s main thread.
Ques_141,What is KVC â€” KVO ?,Medium,KVC adds stands for Key-Value Coding. Itâ€™s a mechanism by which an objectâ€™s properties can be accessed using stringâ€™s at runtime rather than having to statically know the property names at development time.KVO stands for Key-Value Observing and allows a controller or class to observe changes to a property value. In KVO  an object can ask to be notified of any changes to a specific property  whenever that property changes value  the observer is automatically notified.
Ques_142,What is NSFetchRequest?,Hard,NSFetchRequest is the class responsible for fetching from Core Data. Fetch requests are both powerful and flexible. You can use fetch requests to fetch a set of objects meeting the provided criteria  individual values and more.
Ques_143,Explain NSPersistentContainer ?,Hard,The persistent container creates and returns a container  having loaded the store for the application to it. This property is optional since there are legitimate error conditions that could cause the creation of the store to fail.
Ques_144,Explain NSFetchedResultsController ?,Hard,NSFetchedResultsController is a controller  but itâ€™s not a view controller. It has no user interface. Its purpose is to make developersâ€™ lives easier by abstracting away much of the code needed to synchronize a table view with a data source backed by Core Data.
Ques_145,Explain Forced Unwrapping ?,Hard,When we defined a variable as optional  then to get the value from this variable  we will have to unwrap it. This just means putting an exclamation mark at the end of the variable. The example of the implicitly unwrapped optional type is the IBOutlets we created for your view controller.
Ques_146,Explain Swift Standard Library Protocol ?,Hard,There are a few different protocol. Equatable protocol  that governs how we can distinguish between two instances of the same type. That means we can analyze. If we have a specific value is in our array. The comparable protocol  to compare two instances of the same type and sequence protocol: prefix(while:) and drop(while:) 
Ques_147,What is HTML5 Web Storage?,Easy,HTML5 Web Storage is a convenient alternative to cookies  which allows a webpage to store data locally within the userâ€™s browser as name/value pairs. Web Storage is faster and more secure than cookies. It is also able to store more data  capping off at approximately 5MB. Data is only accessed as needed and information is never transferred to the server. While Web Storage is more secure than cookies  it is not encrypted and sensitive items like security tokens should never be stored there.
Ques_148,Explain the difference between localStorage and sessionStorage in HTML?,Easy,Data stored on a userâ€™s browser via localStorage will remain on the userâ€™s computer until they clean their browser or a web app deletes it. Data stored using sessionStorage is temporary  and will only remain until the top-level window or tab running the script is closed. It is worth noting that the scope of sessionStorage is confined to the window that it is operating withinâ€”opening multiple tabs displaying content from the same origin will create separate sessionStorage data that cannot be altered by scripts running in other tabs.
Ques_149,What is the Geolocation API in HTML5 ?,Hard,The HTML5 Geolocation API allows users to share their location with a website. This is a useful feature that asks a user permission to share their latitude and longitude with the app to receive the benefits of location-aware features  such as the ability to present the landing page of the business closest to the userâ€™s location.
Ques_150,What are CSS media queries and what are they used for?,Hard,CSS media queries are the filters that make responsive web design (RWD) possible. With the introduction of laptops  tablets  and smartphones with screens of varying sizes and aspect ratios  RWD is crucial for modern day app development. CSS media queries adjust content style based on device characteristics like width  height  orientation  resolution  and display type. When used properly  the end result is a website or app capable of providing a sleek UI/UX across multiple devices.
Ques_151,What is a CSS preprocessor?,Hard,A preprocessor is an abstraction layer built on top of CSS. Preprocessors extend the functionality of CSS by offering powerful features like variables  inheritable â€œclassesâ€ called extends  and â€œfunction-likeâ€ mixins. Sass  LESS  and Stylus are some of the more well-known preprocessorsâ€”try asking the developer which one they prefer more. Selecting a preprocessor really boils down to preference  and it can be revealing to see how a particular developer might decide to use one over the other for your project. 
Ques_152,What are CSS vendor prefixes?,Hard, Vendor prefixes are extensions to CSS standards that can be added to these features to prevent incompatibilities from arising when the standard is extended. 
Ques_153,What is an image map?,Medium,Image map lets you link to many different web pages using a single image. You can define shapes in images that you want to make part of an image mapping.
Ques_154,What is a marquee?,Medium,A marquee allows you to put a scrolling text in a web page. To do this  place whatever text you want to appear scrolling within the <marquee> and </marquee> tags.
Ques_155,What is Pseudo-elements ?,Medium,Pseudo-elements are used to add special effects to some selectors.  CSS in used to apply styles in HTML mark-up. In some cases when extra mark-up or styling is not possible for the document  then there is a feature available in CSS known as pseudo-elements. It will allow extra mark-up to the document without disturbing the actual document.
Ques_156,What is graceful degradation?,Medium,In case the component fails  it will continue to work properly in the presence of a graceful degradation. The latest browser application is used when a webpage is designed. As it is not available to everyone  there is a basic functionality  which enables its use to a wider audience. In case the image is unavailable for viewing  text is shown with the alt tag.
Ques_157,What is progressive enhancement?,Medium,Itâ€™s an alternative to graceful degradation  which concentrates on the matter of the web. The functionality is same  but it provides an extra edge to users having the latest bandwidth. It has been into prominent use recently with mobile internet connections expanding their base.
Ques_158,What is Z index ?,Medium,Overlapping may occur while using CSS for positioning HTML elements. Z index helps in specifying the overlapping element. It is a number which can be positive or negative  the default value being zero.
Ques_159,What is SVG in HTML5 ?  ,Medium,SVG stands for scalable vector graphics. Itâ€™s a text based graphic language which draws images using text  lines  dots etc. This makes it lightweight and renders faster.
Ques_160,What is SQL?,Easy,SQL stands for Structured Query Language. It is the standard language for relational database management systems. It is especially useful in handling organized data comprised of entities (variables) and relations between different entities of the data. 
Ques_161,What are Constraints in SQL? ,Easy,Constraints are used to specify the rules concerning data in the table. It can be applied for single or multiple fields in an SQL table during creation of table or after creationg using the ALTER TABLE command.
Ques_162,What is a Primary Key? ,Easy,The PRIMARY KEY constraint uniquely identifies each row in a table. It must contain UNIQUE values and has an implicit NOT NULL constraint.A table in SQL is strictly restricted to have one and only one primary key  which is comprised of single or multiple fields (columns).
Ques_163,What is a Foreign Key? ,Easy,A FOREIGN KEY comprises of single or collection of fields in a table that essentially refer to the PRIMARY KEY in another table. Foreign key constraint ensures referential integrity in the relation between two tables.  The table with the foreign key constraint is labelled as the child table  and the table containing the candidate key is labelled as the referenced or parent table. 
Ques_164,What is a UNIQUE constraint? ,Medium,A UNIQUE constraint ensures that all values in a column are different. This provides uniqueness for the column(s) and helps identify each row uniquely. Unlike primary key  there can be multiple unique constraints defined per table.
Ques_165,What is a Self-Join? ,Medium,A self JOIN is a case of regular join where a table is joined to itself based on some relation between its own column(s). Self-join uses the INNER JOIN or LEFT JOIN clause and a table alias is used to assign different names to the table within the query. 
Ques_166,What is a Cross-Join?,Medium,Cross join can be defined as a cartesian product of the two tables included in the join. The table after join contains the same number of rows as in the cross-product of number of rows in the two tables. If a WHERE clause is used in cross join then the query will work like an INNER JOIN. 
Ques_167,What is an Index?,Medium,A database index is a data structure that provides quick lookup of data in a column or columns of a table. It enhances the speed of operations accessing data from a database table at the cost of additional writes and memory to maintain the index data structure. 
Ques_168,What is Cursor?,Medium,A database cursor is a control structure that allows for traversal of records in a database. Cursors  in addition  facilitates processing after traversal  such as retrieval  addition and deletion of database records. They can be viewed as a pointer to one row in a set of rows. 
Ques_169,What is a View? ,Hard,A view in SQL is a virtual table based on the result-set of an SQL statement. A view contains rows and columns  just like a real table. The fields in a view are fields from one or more real tables in the database.
Ques_170,What is Denormalization? ,Hard,Denormalization is the inverse process of normalization  where the normalized schema is converted into a schema which has redundant information. The performance is improved by using redundancy and keeping the redundant data consistent. The reason for performing denormalization is the overheads produced in query processor by an over-normalized structure. 
Ques_171,What is second Normal form ?,Hard,A relation is in second normal form if it satisfies the conditions for first normal form and does not contain any partial dependency. A relation in 2NF has no partial dependency  i.e.  it has no non-prime attribute that depends on any proper subset of any candidate key of the table. Often  specifying a single column Primary Key is the solution to the problem
Ques_172,What is OLTP? ,Hard,OLTP stands for Online Transaction Processing  is a class of software applications capable of supporting transaction-oriented programs. An essential attribute of an OLTP system is its ability to maintain concurrency. To avoid single points of failure  OLTP systems are often decentralized. These systems are usually designed for a large number of users who conduct short transactions. Database queries are usually simple  require sub-second response times and return relatively few records. 
Ques_173,What is a Stored Procedure? ,Hard,A stored procedure is a subroutine available to applications that access a relational database management system (RDBMS). Such procedures are stored in the database data dictionary. The sole disadvantage of stored procedure is that it can be executed nowhere except in the database and occupies more memory in the database server. It also provides a sense of security and functionality as users who can't access the data directly can be granted access via stored procedures. 
Ques_174,What is NoSQL?,Easy,NoSQL encompasses a wide variety of different database technologies that were developed in response to a rise in the volume of data stored about users  objects and products. The frequency in which this data is accessed  and performance and processing needs. Relational databases  on the other hand  were not designed to cope with the scale and agility challenges that face modern applications  nor were they built to take advantage of the cheap storage and processing power available today.
Ques_175,What is â€œPolyglot Persistenceâ€ in NoSQL?,Easy,In 2006  Neal Ford coined the term polyglot programming  to express the idea that applications should be written in a mix of languages to take advantage of the fact that different languages are suitable for tackling different problems. Complex applications combine different types of problems  so picking the right language for each job may be more productive than trying to fit all aspects into a single language.Similarly  when working on an e-commerce business problem  using a data store for the shopping cart which is highly available and can scale is important  but the same data store cannot help you find products bought by the customersâ€™ friendsâ€”which is a totally different question. We use the term polyglot persistence to define this hybrid approach to persistence.
Ques_176,What Is Apache Hbase ?,Medium,Apache HBase is an open source columnar database built to run on top of the Hadoop Distributed File System (HDFS). Hadoop is a framework for handling large datasets in a distributed computing environment.HBase is designed to support high tableÂ­update rates and to scale out horizontally in distributed compute clusters. Its focus on scale enables it to support very large database tables e.g. ones containing billions of rows and millions of columns.
Ques_177,What is Oracle NoSQL database?,Medium,The Oracle NoSQL Database is a distributed key-value database. It is designed to provide highly reliable  scalable and available data storage across a configurable set of systems that function as storage nodes.Data is stored as key-value pairs  which are written to particular storage node(s)  based on the hashed value of the primary key. Storage nodes are replicated to ensure high availability  rapid failover in the event of  a node failure and optimal load balancing of queries. Customer applications are written using an easy-to-use Java/C API to read and write data.
Ques_178,What Is Eventual Consistency In Nosql Stores?,Hard, Eventual consistency means eventually  when all service logic is executed  the system is left in a consistent state. This concept is widely used in distributed systems to achieve high availability. It informally guarantees that  if no new updates are made to a given data item  eventually all accesses to that item will return the last updated value. In NoSQL systems  the eventual consistent services are often classified as providing BASE (Basically Available  Soft state  Eventual consistency) and in RDMS  it is classified as ACID (Availability  Consistency  Isolation and Durability). Leading NoSQL databases like Riak  Couchbase  and DynamoDB provide client applications with a guarantee of â€œeventual consistencyâ€. Others  like MongoDB and Cassandra are eventually consistent in some configurations.
Ques_179,What Is Impala?,Medium,"Impala is a SQL query system for Hadoop from Cloudera. The Cloudera positions Impala as a ""real-time"" query engine for Hadoop and by ""realÂ­time"" they imply that rather than running batch oriented jobs like with MapReduce  we can get much faster query results for a certain  types of queries using Impala over an SQL based frontÂ­end.It does not rely on the MapReduce infrastructure of Hadoop  instead Impala implements a completely separate engine for processing queries. So this engine is a specialized distributed query engine that is similar to what you can find in some of the commercial pattern related databases. So in essence it bypasses MapReduce."
Ques_180,What Is Hive?,Easy,Hive can be thought of as a data warehouse infrastructure for providing summarization  query and analysis of data that is managed by Hadoop.Hive provides a SQL interface for data that is stored in Hadoop.And  it implicitly converts queries into MapReduce jobs so that the programmer can work at a higher level than he or she would when writing MapReduce jobs in Java. Hive is an integral part of the Hadoop ecosystem that was initially developed at Facebook and is now an active Apache open source project.
Ques_181,What is a Discrete Time Signal?,Easy, A discrete time signal x (n) is a function of an independent variable that is an integer. a discrete time signal is not defined at instant between two successive samples.
Ques_182,What is a Discrete Time System?,Easy,A discrete or an algorithm that performs some prescribed operation on a discrete time signal is called discrete time system.
Ques_183,Define Sectional Convolution?,Easy, If the data sequence x(n) is of long duration it is very difficult to obtain the output sequence y(n) due to limited memory of a digital computer. Therefore  the data sequence is divided up into smaller sections. These sections are processed separately one at a time and controlled later to get the output.
Ques_184,What Is Overlap-save Method?,Medium,n this method the data sequence is divided into N point sections xi(n).Each section contains the last M-1 data points of the previous section followed by L new data points to form a data sequence of length N=L+M-1.In circular convolution of xi(n) with h(n) the first M-1 points will not agree with the linear convolution of xi(n) and h(n) because of aliasing  the remaining points will agree with linear convolution. Hence we discard the first (M-1) points of filtered section xi(n) N h(n). This process is repeated for all sections and the filtered sections are abutted together.
Ques_185,What Is Fft?,Medium, The Fast Fourier Transform is an algorithm used to compute the DFT. It makes use of the symmetry and periodicity properties of twiddle factor to effectively reduce the DFT computation time.It is based on the fundamental principle of decomposing the computation of DFT of a sequence of length N into successively smaller DFTs.
Ques_186,What Is Meant By Radix-2 Fft?,Medium, The FFT algorithm is most efficient in calculating N point DFT. If the number of output points N can be expressed as a power of 2 that is N=2M  where M is an integer  then this algorithm is known as radix-2 algorithm.
Ques_187,What Is Dit Algorithm?,Hard, Decimation-In-Time algorithm is used to calculate the DFT of a N point sequence. The idea is to break the N point sequence into two sequences  the DFTs of which can be combined to give the DFt of the original N point sequence.This algorithm is called DIT because the sequence x(n) is often splitted into smaller sub- sequences.
Ques_188,What Dif Algorithm?,Hard, It is a popular form of the FFT algorithm. In this the output sequence X(k) is divided into smaller and smaller sub-sequences   that is why the name Decimation In Frequency.
Ques_189,What Is Gibbâ€™s Phenomenon?,Hard,One possible way of finding an FIR filter that approximates H(ejw) would be to truncate the infinite Fourier series at n=Â±(N-1/2).Direct truncation of the series will lead to fixed percentage overshoots and undershoots before and after an approximated discontinuity in the frequency response.
Ques_190,What is a Node?,Easy,Two or more computers are connected directly by an optical fiber or any other cable. A node is a point where a connection established. It is a network component that is used to send  receive and forward the electronic information. A device connected to a network is also termed as Node. Let's consider that in a network there are 2 computers  2 printers  and a server are connected  then we can say that there are five nodes on the network.
Ques_191,What is a Network?,Easy,A network is a set of devices connected to each other using a physical transmission medium.Example: A Computer Network is a group of computers connected with each other to communicate and share information and resources like hardware  data  and software across each other.
Ques_192,What is the OSI reference model?,Easy,Open System Interconnection  the name itself suggests that it is a reference model that defines how applications can communicate with each other over a networking system.It also helps to understand the relationship between networks and defines the process of communication in a network.
Ques_193,What is a Firewall?,Medium,NIC stands for Network Interface Card. It is also known as Network Adapter or Ethernet Card. It is in the form of an add-in card and is installed on a computer so that the computer can be connected to a network.
Ques_194,What is a VPN?,Medium,VPN is the Virtual Private Network and is built on the Internet as a private wide area network. Internet-based VPNs are less expensive and can be connected from anywhere in the world.VPNs are used to connect offices remotely and are less expensive when compared to WAN connections. VPNs are used for secure transactions and confidential data can be transferred between multiple offices. VPN keeps company information secure against any potential intrusion.
Ques_195,What is DHCP ?,Medium,DHCP stands for Dynamic Host Configuration Protocol and it automatically assigns IP addresses to the network devices. It completely removes the process of manual allocation of IP addresses and reduces the errors caused due to this.This entire process is centralized so that the TCP/IP configuration can also be completed from a central location. DHCP has a â€œpool of IP addressesâ€ from which it allocates the IP address to the network devices. DHCP cannot recognize if any device is configured manually and assigned with the same IP address from the DHCP pool
Ques_196,What is SNMP?,Medium,SNMP stands for Simple Network Management Protocol. It is a network protocol used for collecting organizing and exchanging information between network devices. SNMP is widely used in network management for configuring network devices like switches  hubs  routers  printers  servers.
Ques_197,What is Piggybacking?,Hard,In data transmission if the sender sends any data frame to the receiver then the receiver should send the acknowledgment to the sender. The receiver will temporarily delay (waits for the network layer to send the next data packet) the acknowledgment and hooks it to the next outgoing data frame  this process is called as Piggybacking
Ques_198,What is Brouter?,Hard,Brouter or Bridge Router is a device that acts as both a bridge and a router. As a bridge  it forwards data between the networks. And as a router  it routes the data to specified systems within a network.
Ques_199,What is Beaconing?,Hard,If a network self-repair its problem then it is termed as Beaconing. Mainly it is used in the token ring and FDDI (Fiber Distributed Data Interface) networks. If a device in the network is facing any problem  then it notifies the other devices that they are not receiving any signal. Likewise  the problem gets repaired within the network.
Ques_200,Design and implement an iterator to flatten a 2d vector. It should support the following operations: next and hasNext.   Example: Vector2D iterator = new Vector2D([[1 2] [3] [4]]);  iterator.next(); // return 1 iterator.next(); // return 2 iterator.next(); // return 3 iterator.hasNext(); // return true iterator.hasNext(); // return true iterator.next(); // return 4 iterator.hasNext(); // return false   Notes: Please remember to RESET your class variables declared in Vector2D  as static/class variables are persisted across multiple test cases. Please see here for more details. You may assume that next() call will always be valid  that is  there will be at least a next element in the 2d vector when next() is called.   Follow up: As an added challenge  try to code it using only iterators in C++ or iterators in Java.,Medium,Subscription Needed ####LeetCode####  Since the OJ does while (i.hasNext()) cout << i.next();  i.e.  always calls hasNext before next  I don't really have to call it myself so I could save that line in next. But I think that would be bad  we shouldn't rely on that.  C++  class Vector2D {      vector<vector<int>>::iterator i  iEnd;      int j = 0;  public:      Vector2D(vector<vector<int>>& vec2d) {          i = vec2d.begin();          iEnd = vec2d.end();      }        int next() {          hasNext();          return (*i)[j++];      }        bool hasNext() {          while (i != iEnd && j == (*i).size())              i++  j = 0;          return i != iEnd;      }  };  Java  public class Vector2D {        private Iterator<List<Integer>> i;      private Iterator<Integer> j;        public Vector2D(List<List<Integer>> vec2d) {          i = vec2d.iterator();      }        public int next() {          hasNext();          return j.next();      }        public boolean hasNext() {          while ((j == null || !j.hasNext()) && i.hasNext())              j = i.next().iterator();          return j != null && j.hasNext();      }  } ####LeetCode#### public class Vector2D {      int indexList  indexEle;      List<List<Integer>> vec;             public Vector2D(List<List<Integer>> vec2d) {          indexList = 0;          indexEle = 0;          vec = vec2d;      }        public int next() {          return vec.get(indexList).get(indexEle++);      }        public boolean hasNext() {          while(indexList < vec.size()){              if(indexEle < vec.get(indexList).size())                  return true;              else{                  indexList++;                  indexEle = 0;              }          }          return false;      }  } ####LeetCode#### I first hold the 2D List inside a Iterator of List this allows me to operate on the subsequent list once needed. I then remove the first list from the 2D List and store as row which is evaluated when next() & hasNext() are called. I then want to ensure row iterator is pointing to a none empty list so i call the getNextRow() method. next() and hashNext() are now very simple. next() returns the next element of the current list then ensures row isn't empty. hasNext() checks row isn't null and has a next value.  public class Vector2D {      Iterator<List<Integer>> itrs;      Iterator<Integer> row;      public Vector2D(List<List<Integer>> vec2d) {          if(vec2d == null || vec2d.size() == 0) return;          itrs = vec2d.iterator();          row = itrs.next().iterator();          getNextRow();      }            private void getNextRow(){          while(!row.hasNext() && itrs.hasNext()) row = itrs.next().iterator();      }        public int next() {          int next = row.next();          getNextRow();          return next;      }        public boolean hasNext() {          return row != null && row.hasNext();      }  } ####LeetCode#### 
Ques_201,Given an array of meeting time intervals consisting of start and end times [[s1 e1] [s2 e2] ...] (si < ei)  determine if a person could attend all meetings. Example 1: Input: [[0 30] [5 10] [15 20]] Output: false Example 2: Input: [[7 10] [2 4]] Output: true NOTE: input types have been changed on April 15  2019. Please reset to default code definition to get new method signature.,Easy,Solution Approach #1 (Brute Force) [Accepted] The straight-forward solution is to compare every two meetings in the array  and see if they conflict with each other (i.e. if they overlap). Two meetings overlap if one of them starts while the other is still taking place. Java public boolean canAttendMeetings(Interval[] intervals) {     for (int i = 0; i < intervals.length; i++) {         for (int j = i + 1; j < intervals.length; j++) {             if (overlap(intervals[i]  intervals[j])) return false;         }     }     return true; }  private boolean overlap(Interval i1  Interval i2) {     return ((i1.start >= i2.start && i1.start < i2.end)          || (i2.start >= i1.start && i2.start < i1.end)); } Overlap Condition The overlap condition in the code above can be written in a more concise way. Consider two non-overlapping meetings. The earlier meeting ends before the later meeting begins. Therefore  the minimum end time of the two meetings (which is the end time of the earlier meeting) is smaller than or equal the maximum start time of the two meetings (which is the start time of the later meeting). Figure 1. Two non-overlapping intervals. Figure 2. Two overlapping intervals. So the condition can be rewritten as follows. private boolean overlap(Interval i1  Interval i2) {     return (Math.min(i1.end  i2.end) >             Math.max(i1.start  i2.start)); } Complexity Analysis Because we have two check every meeting with every other meeting  the total run time is O(n^2) O(n 2 ). No additional space is used  so the space complexity is O(1) O(1). Approach #2 (Sorting) [Accepted] The idea here is to sort the meetings by starting time. Then  go through the meetings one by one and make sure that each meeting ends before the next one starts. Java public boolean canAttendMeetings(Interval[] intervals) {     Arrays.sort(intervals  new Comparator<Interval>() {         public int compare(Interval i1  Interval i2) {             return i1.start - i2.start;         }             });     for (int i = 0; i < intervals.length - 1; i++) {         if (intervals[i].end > intervals[i + 1].start) return false;     }     return true; } Complexity Analysis Time complexity : O(n \log n) O(nlogn). The time complexity is dominated by sorting. Once the array has been sorted  only O(n) O(n) time is taken to go through the array and determine if there is any overlap. Space complexity : O(1) O(1). Since no additional space is allocated. Analysis written by: @noran ####LeetCode####  public boolean canAttendMeetings(Interval[] intervals) {    if (intervals == null)      return false;      // Sort the intervals by start time    Arrays.sort(intervals  new Comparator<Interval>() {      public int compare(Interval a  Interval b) { return a.start - b.start; }    });        for (int i = 1; i < intervals.length; i++)      if (intervals[i].start < intervals[i - 1].end)        return false;        return true;  } ####LeetCode#### public boolean canAttendMeetings(Interval[] intervals) {          int len=intervals.length;          if(len==0){              return true;          }          int[]begin=new int[len];          int[]stop=new int[len];          for(int i=0;i<len;i++){              begin[i]=intervals[i].start;              stop[i]=intervals[i].end;          }          Arrays.sort(begin);          Arrays.sort(stop);          int endT=0;          for(int i=1;i<len;i++){              if(begin[i]<stop[i-1]){                  return false;              }          }          return true;  } ####LeetCode#### def canAttendMeetings(self  intervals):      intervals.sort(key=lambda x: x.start)            for i in range(1  len(intervals)):          if intervals[i].start < intervals[i-1].end:              return False                return True ####LeetCode#### 
Ques_202,Given an array of meeting time intervals consisting of start and end times [[s1 e1] [s2 e2] ...] (si < ei)  find the minimum number of conference rooms required. Example 1: Input: [[0  30] [5  10] [15  20]] Output: 2 Example 2: Input: [[7 10] [2 4]] Output: 1 NOTE: input types have been changed on April 15  2019. Please reset to default code definition to get new method signature.,Medium,"Solution  Intuition This problem is very similar to something that employees of a company can face potentially on daily basis. Suppose you work at a company and you belong to the IT department and one of your job responsibilities is securing rooms for meetings that are to happen throughout the day in the office. You have multiple meeting rooms in the office and you want to make judicious use of them. You don't really want to keep people waiting and want to give a group of employees a room to hold the meeting right on time. At the same time  you don't really want to use too many rooms unless absolutely necessary. It would make sense to hold meetings in different rooms provided that the meetings are colliding with each other  otherwise you want to make use of as less rooms as possible to hold all of the meetings. How do you go about it ? I just represented a common scenario at an office where given the start and end times for meetings to happen throughout the day  you  as an IT guy need to setup and allocate the room numbers to different teams. Let's approach this problem from the perspective of a group of people who want to hold a meeting and have not been allocated a room yet. What would they do? This group would essentially go from one room to another and check if any meeting room is free. If they find a room that is indeed free  they would start their meeting in that room. Otherwise  they would wait for a room to be free. As soon as the room frees up  they would occupy it. This is the basic approach that we will follow in this question. So  it is a kind of simulation but not exactly. In the worst case we can assign a new room to all of the meetings but that is not really optimal right? Unless of course they all collide with each other. We need to be able to find out efficiently if a room is available or not for the current meeting and assign a new room only if none of the assigned rooms is currently free. Let's look at the first approach based on the idea we just discussed.  Approach 1: Priority Queues We can't really process the given meetings in any random order. The most basic way of processing the meetings is in increasing order of their start times and this is the order we will follow. After all if you're an IT guy  you should allocate a room to the meeting that is scheduled for 9 a.m. in the morning before you worry about the 5 p.m. meeting  right? Let's do a dry run of an example problem with sample meeting times and see what our algorithm should be able to do efficiently. We will consider the following meeting times for our example (1  10)  (2  7)  (3  19)  (8  12)  (10  20)  (11  30). The first part of the tuple is the start time for the meeting and the second value represents the ending time. We are considering the meetings in a sorted order of their start times. The first diagram depicts the first three meetings where each of them requires a new room because of collisions. The next 3 meetings start to occupy some of the existing rooms. However  the last one requires a new room altogether and overall we have to use 4 different rooms to accommodate all the meetings. Sorting part is easy  but for every meeting how do we find out efficiently if a room is available or not? At any point in time we have multiple rooms that can be occupied and we don't really care which room is free as long as we find one when required for a new meeting. A naive way to check if a room is available or not is to iterate on all the rooms and see if one is available when we have a new meeting at hand. However  we can do better than this by making use of Priority Queues or the Min-Heap data structure. Instead of manually iterating on every room that's been allocated and checking if the room is available or not  we can keep all the rooms in a min heap where the key for the min heap would be the ending time of meeting. So  every time we want to check if any room is free or not  simply check the topmost element of the min heap as that would be the room that would get free the earliest out of all the other rooms currently occupied. If the room we extracted from the top of the min heap isn't free  then no other room is. So  we can save time here and simply allocate a new room. Let us look at the algorithm before moving onto the implementation. Algorithm Sort the given meetings by their start time. Initialize a new min-heap and add the first meeting's ending time to the heap. We simply need to keep track of the ending times as that tells us when a meeting room will get free. For every meeting room check if the minimum element of the heap i.e. the room at the top of the heap is free or not. If the room is free  then we extract the topmost element and add it back with the ending time of the current meeting we are processing. If not  then we allocate a new room and add it to the heap. After processing all the meetings  the size of the heap will tell us the number of rooms allocated. This will be the minimum number of rooms needed to accommodate all the meetings. Let us not look at the implementation for this algorithm. Complexity Analysis Time Complexity: O(N\log N) O(NlogN). There are two major portions that take up time here. One is sorting of the array that takes O(N\log N) O(NlogN) considering that the array consists of N N elements. Then we have the min-heap. In the worst case  all N N meetings will collide with each other. In any case we have N N add operations on the heap. In the worst case we will have N N extract-min operations as well. Overall complexity being (NlogN) (NlogN) since extract-min operation on a heap takes O(\log N) O(logN). Space Complexity: O(N) O(N) because we construct the min-heap and that can contain N N elements in the worst case as described above in the time complexity section. Hence  the space complexity is O(N) O(N).  Approach 2: Chronological Ordering Intuition The meeting timings given to us define a chronological order of events throughout the day. We are given the start and end timings for the meetings which can help us define this ordering. Arranging the meetings according to their start times helps us know the natural order of meetings throughout the day. However  simply knowing when a meeting starts doesn't tell us much about its duration. We also need the meetings sorted by their ending times because an ending event essentially tells us that there must have been a corresponding starting event and more importantly  an ending event tell us that a previously occupied room has now become free. A meeting is defined by its start and end times. However  for this specific algorithm  we need to treat the start and end times individually. This might not make sense right away because a meeting is defined by its start and end times. If we separate the two and treat them individually  then the identity of a meeting goes away. This is fine because: When we encounter an ending event  that means that some meeting that started earlier has ended now. We are not really concerned with which meeting has ended. All we need is that some meeting ended thus making a room available. Let us consider the same example as we did in the last approach. We have the following meetings to be scheduled: (1  10)  (2  7)  (3  19)  (8  12)  (10  20)  (11  30). As before  the first diagram show us that the first three meetings are colliding with each other and they have to be allocated separate rooms. The next two diagrams process the remaining meetings and we see that we can now reuse some of the existing meeting rooms. The final result is the same  we need 4 different meeting rooms to process all the meetings. That's the best we can do here. Algorithm Separate out the start times and the end times in their separate arrays. Sort the start times and the end times separately. Note that this will mess up the original correspondence of start times and end times. They will be treated individually now. We consider two pointers: s_ptr and e_ptr which refer to start pointer and end pointer. The start pointer simply iterates over all the meetings and the end pointer helps us track if a meeting has ended and if we can reuse a room. When considering a specific meeting pointed to by s_ptr  we check if this start timing is greater than the meeting pointed to by e_ptr. If this is the case then that would mean some meeting has ended by the time the meeting at s_ptr had to start. So we can reuse one of the rooms. Otherwise  we have to allocate a new room. If a meeting has indeed ended i.e. if start[s_ptr] >= end[e_ptr]  then we increment e_ptr. Repeat this process until s_ptr processes all of the meetings. Let us not look at the implementation for this algorithm. Complexity Analysis Time Complexity: O(N\log N) O(NlogN) because all we are doing is sorting the two arrays for start timings and end timings individually and each of them would contain N N elements considering there are N N intervals. Space Complexity: O(N) O(N) because we create two separate arrays of size N N  one for recording the start times and one for the end times.   Analysis written by: @sachinmalhotra1993.1 /** 2  * Definition for an interval. public class Interval { int start; int end; Interval() { start = 0; 3  * end = 0; } Interval(int s  int e) { start = s; end = e; } } 4  */ 5 class Solution { 6 7   public int minMeetingRooms(Interval[] intervals) { 8 9     // Check for the base case. If there are no intervals  return 0 10     if (intervals.length == 0) { 11       return 0; 12     } 13 14     // Min heap 15     PriorityQueue<Integer> allocator = 16         new PriorityQueue<Integer>( 17             intervals.length  18             new Comparator<Integer>() { 19               public int (    ) { 21               } 22             }); 23 24     // Sort the intervals by start time 25     Arrays.sort( 26         intervals  27         new Comparator<Interval>() { 28           public int compare(Interval a  Interval b) {1 /** 2  * Definition for an interval. public class Interval { int start; int end; Interval() { start = 0; 3  * end = 0; } Interval(int s  int e) { start = s; end = e; } } 4  */ 5 class Solution { 6 7   public int minMeetingRooms(Interval[] intervals) { 8 9     // Check for the base case. If there are no intervals  return 0 10     if (intervals.length == 0) { 11       return 0; 12     } 13 14     Integer[] start = new Integer[intervals.length]; 15     Integer[] end = new Integer[intervals.length]; 16 17     for (int i = 0; i < intervals.length; i++) { 18       start[i] = [].; 20     } 21 22     // Sort the intervals by end time 23     Arrays.sort( 24         end  25         new Comparator<Integer>() { 26           public int compare(Integer a  Integer b) { 27             return a - b; ####LeetCode####  The solution is proposed by @pinkfloyda at ""Super Easy Java Solution Beats 98.8%""   which is amazing.  Here I would like to explain why it works a little bit.  The code from @pinkfloyda:  public class Solution {      public int minMeetingRooms(Interval[] intervals) {          int[] starts = new int[intervals.length];          int[] ends = new int[intervals.length];          for(int i=0; i<intervals.length; i++) {              starts[i] = intervals[i].start;              ends[i] = intervals[i].end;          }          Arrays.sort(starts);          Arrays.sort(ends);          int rooms = 0;          int endsItr = 0;          for(int i=0; i<starts.length; i++) {              if(starts[i]<ends[endsItr])                  rooms++;              else                  endsItr++;          }          return rooms;      }  }  To understand why it works  first letâ€™s define two events:  Meeting Starts  Meeting Ends  Next  we acknowledge three facts:  The numbers of the intervals give chronological orders  When an ending event occurs  there must be a starting event has happened before that  where â€œhappen beforeâ€ is defined by the chronological orders given by the intervals  Meetings that started which havenâ€™t ended yet have to be put into different meeting rooms  and the number of rooms needed is the number of such meetings  So  what this algorithm works as follows:  for example  we have meetings that span along time as follows:  |_____|        |______|  |________|          |_______|  Then  the start time array and end time array after sorting appear like follows:  ||    ||       |   |   |  |  Initially  endsItr points to the first end event  and we move i which is the start event pointer. As we examine the start events  weâ€™ll find the first two start events happen before the end event that endsItr points to  so we need two rooms (we magically created two rooms)  as shown by the variable rooms. Then  as i points to the third start event  weâ€™ll find that this event happens after the end event pointed by endsItr  then we increment endsItr so that it points to the next end event. What happens here can be thought of as one of the two previous meetings ended  and we moved the newly started meeting into that vacant room  thus we donâ€™t need to increment rooms at this time and move both of the pointers forward.  Next  because endsItr moves to the next end event  weâ€™ll find that the start event pointed by i happens before the end event pointed by endsItr. Thus  now we have 4 meetings started but only one ended  so we need one more room. And it goes on as this. ####LeetCode#### Just want to share another idea that uses min heap  average time complexity is O(nlogn).  public int minMeetingRooms(Interval[] intervals) {      if (intervals == null || intervals.length == 0)          return 0;                // Sort the intervals by start time      Arrays.sort(intervals  new Comparator<Interval>() {          public int compare(Interval a  Interval b) { return a.start - b.start; }      });            // Use a min heap to track the minimum end time of merged intervals      PriorityQueue<Interval> heap = new PriorityQueue<Interval>(intervals.length  new Comparator<Interval>() {          public int compare(Interval a  Interval b) { return a.end - b.end; }      });            // start with the first meeting  put it to a meeting room      heap.offer(intervals[0]);            for (int i = 1; i < intervals.length; i++) {          // get the meeting room that finishes earliest          Interval interval = heap.poll();                    if (intervals[i].start >= interval.end) {              // if the current meeting starts right after               // there's no need for a new room  merge the interval              interval.end = intervals[i].end;          } else {              // otherwise  this meeting needs a new room              heap.offer(intervals[i]);          }                    // don't forget to put the meeting room back          heap.offer(interval);      }            return heap.size();  } ####LeetCode####  # Very similar with what we do in real life. Whenever you want to start a meeting     # you go and check if any empty room available (available > 0) and   # if so take one of them ( available -=1 ). Otherwise    # you need to find a new room someplace else ( numRooms += 1 ).     # After you finish the meeting  the room becomes available again ( available += 1 ).      def minMeetingRooms(self  intervals):          starts = []          ends = []          for i in intervals:              starts.append(i.start)              ends.append(i.end)                    starts.sort()          ends.sort()          s = e = 0          numRooms = available = 0          while s < len(starts):              if starts[s] < ends[e]:                  if available == 0:                      numRooms += 1                  else:                      available -= 1                                        s += 1              else:                  available += 1                  e += 1                    return numRooms ####LeetCode#### "
Ques_203,Numbers can be regarded as product of its factors. For example  8 = 2 x 2 x 2;   = 2 x 4. Write a function that takes an integer n and return all possible combinations of its factors. Note: You may assume that n is always positive. Factors should be greater than 1 and less than n. Example 1: Input: 1 Output: [] Example 2: Input: 37 Output:[] Example 3: Input: 12 Output: [   [2  6]    [2  2  3]    [3  4] ] Example 4: Input: 32 Output: [   [2  16]    [2  2  8]    [2  2  2  4]    [2  2  2  2  2]    [2  4  4]    [4  8] ],Medium,Subscription Needed ####LeetCode####  public List<List<Integer>> getFactors(int n) {      List<List<Integer>> result = new ArrayList<List<Integer>>();      helper(result  new ArrayList<Integer>()  n  2);      return result;  }    public void helper(List<List<Integer>> result  List<Integer> item  int n  int start){      if (n <= 1) {          if (item.size() > 1) {              result.add(new ArrayList<Integer>(item));          }          return;      }            for (int i = start; i <= n; ++i) {          if (n % i == 0) {              item.add(i);              helper(result  item  n/i  i);              item.remove(item.size()-1);          }      }  } ####LeetCode#### public List<List<Integer>> getFactors(int n) {      List<List<Integer>> result = new ArrayList<List<Integer>>();      if (n <= 3) return result;      helper(n  -1  result  new ArrayList<Integer>());      return result;   }    public void helper(int n  int lower  List<List<Integer>> result  List<Integer> cur) {      if (lower != -1) {          cur.add(n);          result.add(new ArrayList<Integer>(cur));          cur.remove(cur.size() - 1);      }      int upper = (int) Math.sqrt(n);      for (int i = Math.max(2  lower); i <= upper; ++i) {          if (n % i == 0) {              cur.add(i);              helper(n / i  i  result  cur);              cur.remove(cur.size() - 1);          }      }  } ####LeetCode#### class Solution {      public:          void getResult(vector<vector<int>> &result vector<int> &row int n){              int i=row.empty()?2:row.back();              for(;i<=n/i;++i){                  if(n%i==0){                      row.push_back(i);                      row.push_back(n/i);                      result.push_back(row);                      row.pop_back();                      getResult(result row n/i);                      row.pop_back();                  }              }          }                vector<vector<int>> getFactors(int n) {              vector<vector<int>> result;              vector<int>row;              getResult(result row n);              return result;          }      }; ####LeetCode#### 
Ques_204,Given an array of numbers  verify whether it is the correct preorder traversal sequence of a binary search tree. You may assume each number in the sequence is unique. Consider the following binary search tree:       5     / \    2   6   / \  1   3 Example 1: Input: [5 2 6 1 3] Output: false Example 2: Input: [5 2 1 3 6] Output: true Follow up: Could you do it using only constant space complexity?,Medium,Subscription Needed ####LeetCode####  Solution 1  Kinda simulate the traversal  keeping a stack of nodes (just their values) of which we're still in the left subtree. If the next number is smaller than the last stack value  then we're still in the left subtree of all stack nodes  so just push the new one onto the stack. But before that  pop all smaller ancestor values  as we must now be in their right subtrees (or even further  in the right subtree of an ancestor). Also  use the popped values as a lower bound  since being in their right subtree means we must never come across a smaller number anymore.  public boolean verifyPreorder(int[] preorder) {      int low = Integer.MIN_VALUE;      Stack<Integer> path = new Stack();      for (int p : preorder) {          if (p < low)              return false;          while (!path.empty() && p > path.peek())              low = path.pop();          path.push(p);      }      return true;  }  Solution 2 ... O(1) extra space  Same as above  but abusing the given array for the stack.  public boolean verifyPreorder(int[] preorder) {      int low = Integer.MIN_VALUE  i = -1;      for (int p : preorder) {          if (p < low)              return false;          while (i >= 0 && p > preorder[i])              low = preorder[i--];          preorder[++i] = p;      }      return true;  }  Solution 3 ... Python  Same as solution 1  just in Python.  def verifyPreorder(self  preorder):      stack = []      low = float('-inf')      for p in preorder:          if p < low:              return False          while stack and p > stack[-1]:              low = stack.pop()          stack.append(p)      return True ####LeetCode#### THOUGHT: We first look at the property of preorder traversal: we print left childâ€™s value of current node all the way until we reached a leaf node (you will see numbers decreasing)  then we start printing the value of a node (let it be rc) which is the right child of one of the nodes (let it be node p) we already traversed. When do you know it's a right child node's value? It's when you see a value greater than the last one. Also till here we know  all the nodes in pâ€™s left subtree have been read in the serialized array  and this property is maintained:  left subtree â€˜s value < p â€™s value < rcâ€™s value  Since all the nodes whose value is smaller than p are already read  all the nodesâ€™ value to be read after should have greater value than pâ€™s value  so pâ€™s value becomes the lower bound for any upcoming node.  p â€™s value < upcoming value in array  Otherwise  itâ€™s not valid. So the key here is to find the lower bound for upcoming nodes  which equals to find p.  To translate this into code: looking for the trend of numbers  if itâ€™s decreasing  itâ€™s still traversing the left child node all the way down  we push the value into stack. When we read a value greater than the last one  we know the current value belongs to a right node (let it be rc: right child) of one of the previous nodes (let it be p) we pushed to stack  in other words  p is a parent node of the current node rc. Due to the property of preorder traversal  pâ€™s value is pushed to stack before its left subtree nodes  so to find the parent node  we pop all the nodes in its left subtree  and the last popped node whose value is smaller than rc is rcâ€™s parent p  whose value becomes the lower bound. Then we keep reading the serialized array  in any case we see any value not greater than the lower bound  we return false. Lower bound is updated whenever we read a right child nodeâ€™s value.  class Solution {  public:      bool verifyPreorder(vector<int>& preorder) {          stack<int> stk;          int lower_bound = INT_MIN;          for(int i = 0; i < preorder.size(); i++){              if(stk.empty() || preorder[i] < preorder[i - 1]){                  if(preorder[i] <= lower_bound) return false;                  stk.push(preorder[i]);              }else{                  while(!stk.empty() && stk.top() < preorder[i]){                      lower_bound = stk.top();                      stk.pop();                  }                  stk.push(preorder[i]);              }          }                    return true;      }  };  Using this image as an example:  Push 50  Push 17   Push 9  (read 14  14 > 9)  Pop 9 (lower bound = 9)  Push 14  Push 12  (read 23  23 > 12)  Pop 12  Pop 14  Pop 17 (lower bound = 17)  Push 23  (read 76  76 > 23)  Pop 23  Pop 50 (lowerbound = 50)  Push 76  Push 54  (read 72  72 > 54)  Pop 54 (lower bound = 54)  Push 72  Push 67 ####LeetCode#### A easy solution is O(n) time and O(n) space using a stack  def verifyPreorder(self  preorder):      stack = []      lower = -1 << 31      for x in preorder:          if x < lower:              return False          while stack and x > stack[-1]:              lower = stack.pop()          stack.append(x)      return True      # 59 / 59 test cases passed.  # Status: Accepted  # Runtime: 100 ms  # 95.31%  Then we realize that the preorder array can be reused as the stack thus achieve O(1) extra space  since the scanned items of preorder array is always more than or equal to the length of the stack.  def verifyPreorder(self  preorder):      # stack = preorder[:i]  reuse preorder as stack      lower = -1 << 31      i = 0      for x in preorder:          if x < lower:              return False          while i > 0 and x > preorder[i - 1]:              lower = preorder[i - 1]              i -= 1          preorder[i] = x          i += 1      return True      # 59 / 59 test cases passed.  # Status: Accepted  # Runtime: 112 ms  # 70.31% ####LeetCode#### 
Ques_205,There are a row of n houses  each house can be painted with one of the three colors: red  blue or green. The cost of painting each house with a certain color is different. You have to paint all the houses such that no two adjacent houses have the same color. The cost of painting each house with a certain color is represented by a n x 3 cost matrix. For example  costs[0][0] is the cost of painting house 0 with color red; costs[1][2] is the cost of painting house 1 with color green  and so on... Find the minimum cost to paint all houses. Note: All costs are positive integers. Example: Input: [[17 2 17] [16 16 5] [14 3 19]] Output: 10 Explanation: Paint house 0 into blue  paint house 1 into green  paint house 2 into blue.               Minimum cost: 2 + 5 + 3 = 10.,Easy,"Subscription Needed ####LeetCode####  The 1st row is the prices for the 1st house  we can change the matrix to present sum of prices from the 2nd row. i.e  the costs[1][0] represent minimum price to paint the second house red plus the 1st house.  public class Solution {  public int minCost(int[][] costs) {      if(costs==null||costs.length==0){          return 0;      }      for(int i=1; i<costs.length; i++){          costs[i][0] += Math.min(costs[i-1][1] costs[i-1][2]);          costs[i][1] += Math.min(costs[i-1][0] costs[i-1][2]);          costs[i][2] += Math.min(costs[i-1][1] costs[i-1][0]);      }      int n = costs.length-1;      return Math.min(Math.min(costs[n][0]  costs[n][1])  costs[n][2]);  }  } ####LeetCode#### The basic idea is when we have painted the first i houses  and want to paint the i+1 th house  we have 3 choices: paint it either red  or green  or blue. If we choose to paint it red  we have the follow deduction:  paintCurrentRed = min(paintPreviousGreen paintPreviousBlue) + costs[i+1][0]  Same for the green and blue situation. And the initialization is set to costs[0]  so we get the code:  public class Solution {  public int minCost(int[][] costs) {      if(costs.length==0) return 0;      int lastR = costs[0][0];      int lastG = costs[0][1];      int lastB = costs[0][2];      for(int i=1; i<costs.length; i++){          int curR = Math.min(lastG lastB)+costs[i][0];          int curG = Math.min(lastR lastB)+costs[i][1];          int curB = Math.min(lastR lastG)+costs[i][2];          lastR = curR;          lastG = curG;          lastB = curB;      }      return Math.min(Math.min(lastR lastG) lastB);  }  } ####LeetCode#### Before 2014  there were less than 300 problems on Leetcode  and if you were able to solve all of them you could nail any algorithm interview in the US. Also  they didn't ask many DP problems  most of the problems were related to linked lists  trees  sorting  matrix  math. I remember the hardest problem type at that time was recursion. So a problem like Paint House should be labeled as hard or at least medium back then.  In recent years  tech companies (especially Google) got crazy: they may ask you problems related to Priority Queue  Monotonic Stack  Bit Manipulation  Palindrome  KMP  DP  DFS  BFS  backtracking  Minimax tree  Fenwick tree  Dijkstra  Bellman-ford  Floyd-Warshall  Floyd's Tortoise and Hare......  So better get a job right now  imagine 10 years later the ""Easy"" problems could be: implementation of Red-Black tree  Hamiltonian cycle  Max-flow Min-cut... ####LeetCode#### "
Ques_206,"Given a binary tree  return all root-to-leaf paths. Note: A leaf is a node with no children. Example: Input:     1  /   \ 2     3  \   5  Output: [""1->2->5""  ""1->3""]  Explanation: All root-to-leaf paths are: 1->2->5  1->3",Easy,"Solution Binary tree definition First of all  here is the definition of the TreeNode which we would use in the following implementation.   Approach 1: Recursion The most intuitive way is to use a recursion here. One is going through the tree by considering at each step the node itself and its children. If node is not a leaf  one extends the current path by a node value and calls recursively the path construction for its children. If node is a leaf  one closes the current path and adds it into the list of paths. Complexity Analysis Time complexity : we visit each node exactly once  thus the time complexity is \mathcal{O}(N) O(N)  where N N is the number of nodes. Space complexity : \mathcal{O}(N) O(N). Here we use the space for a stack call and for a paths list to store the answer. paths contains as many elements as leafs in the tree and hence couldn't be larger than \log N logN for the trees containing more than one element. Hence the space complexity is determined by a stack call. In the worst case  when the tree is completely unbalanced  e.g. each node has only one child node  the recursion call would occur N N times (the height of the tree)  therefore the storage to keep the call stack would be \mathcal{O}(N) O(N). But in the best case (the tree is balanced)  the height of the tree would be \log(N) log(N). Therefore  the space complexity in this case would be \mathcal{O}(\log(N)) O(log(N)).  Approach 2: Iterations The approach above could be rewritten with the help of iterations. This way we initiate the stack by a root node and then at each step we pop out one node and its path. If the poped node is a leaf  one update the list of all paths. If not  one pushes its child nodes and corresponding paths into stack till all nodes are checked. 1 / 7 Complexity Analysis Time complexity : \mathcal{O}(N) O(N) since each node is visited exactly once. Space complexity : \mathcal{O}(N) O(N) as we could keep up to the entire tree. Analysis written by @liaison and @andvary1 /* Definition for a binary tree node. */ 2 public class TreeNode { 3   int val; 4   TreeNode left; 5   TreeNode right; 6 7   TreeNode(int x) { 8     val = x; 9   } 10 }1 class Solution { 2   public void construct_paths(TreeNode root  String path  LinkedList<String> paths) { 3     if (root != null) { 4       path += Integer.toString(root.val); 5       if ((root.left == null) && (root.right == null))  // if reach a leaf 6         paths.add(path);  // update paths 7       else { 8         path += ""->"";  // extend the current path 9         construct_paths(root.left  path  paths); 10         construct_paths(root.right  path  paths); 11       } 14 15   public List<String> binaryTreePaths(TreeNode root) { 16     LinkedList<String> paths = new LinkedList(); 17     construct_paths(root  """"  paths); 18     return paths; 19   } 20 }1 class Solution { 2   public List<String> binaryTreePaths(TreeNode root) { 3     LinkedList<String> paths = new LinkedList(); 4     if (root == null) 5       return paths; 6 7     LinkedList<TreeNode> node_stack = new LinkedList(); 8     LinkedList<String> path_stack = new LinkedList(); 9     node_stack.add(root); 10     path_stack.add(Integer.toString(root.val)); 11     TreeNode node; 12     String path; 13     while ( !node_stack.isEmpty() ) { 14       node = node_stack.pollLast(); 15       path = path_stack.pollLast(); 16       if ((node.left == null) && (node.right == null)) 17         paths.add(path); 18       if (node.left != null) { 19         node_stack.add(node.left); 20 21       } 22       if (node.right != null) { 23         node_stack.add(node.right); 24         path_stack.add(path + ""->"" + Integer.toString(node.right.val)); 25       } 26     } 27     return paths; 28   } 29 } ####LeetCode####  public List<String> binaryTreePaths(TreeNode root) {      List<String> answer = new ArrayList<String>();      if (root != null) searchBT(root  """"  answer);      return answer;  }  private void searchBT(TreeNode root  String path  List<String> answer) {      if (root.left == null && root.right == null) answer.add(path + root.val);      if (root.left != null) searchBT(root.left  path + root.val + ""->""  answer);      if (root.right != null) searchBT(root.right  path + root.val + ""->""  answer);  } ####LeetCode#### # dfs + stack  def binaryTreePaths1(self  root):      if not root:          return []      res  stack = []  [(root  """")]      while stack:          node  ls = stack.pop()          if not node.left and not node.right:              res.append(ls+str(node.val))          if node.right:              stack.append((node.right  ls+str(node.val)+""->""))          if node.left:              stack.append((node.left  ls+str(node.val)+""->""))      return res        # bfs + queue  def binaryTreePaths2(self  root):      if not root:          return []      res  queue = []  collections.deque([(root  """")])      while queue:          node  ls = queue.popleft()          if not node.left and not node.right:              res.append(ls+str(node.val))          if node.left:              queue.append((node.left  ls+str(node.val)+""->""))          if node.right:              queue.append((node.right  ls+str(node.val)+""->""))      return res        # dfs recursively  def binaryTreePaths(self  root):      if not root:          return []      res = []      self.dfs(root  """"  res)      return res    def dfs(self  root  ls  res):      if not root.left and not root.right:          res.append(ls+str(root.val))      if root.left:          self.dfs(root.left  ls+str(root.val)+""->""  res)      if root.right:          self.dfs(root.right  ls+str(root.val)+""->""  res) ####LeetCode#### Lot of recursive solutions on this forum involves creating a helper recursive function with added parameters. The added parameter which usually is of the type List   carries the supplementary path information. However  the approach below doesn't use such a helper function.  public List<String> binaryTreePaths(TreeNode root) {                    List<String> paths = new LinkedList<>();            if(root == null) return paths;                    if(root.left == null && root.right == null){              paths.add(root.val+"""");              return paths;          }             for (String path : binaryTreePaths(root.left)) {               paths.add(root.val + ""->"" + path);           }             for (String path : binaryTreePaths(root.right)) {               paths.add(root.val + ""->"" + path);           }             return paths;                } ####LeetCode#### "
Ques_207,Given a non-negative integer num  repeatedly add all its digits until the result has only one digit. Example: Input: 38 Output: 2  Explanation: The process is like: 3 + 8 = 11  1 + 1 = 2.               Since 2 has only one digit  return it. Follow up: Could you do it without any loop/recursion in O(1) runtime?,Easy,"Subscription Needed ####LeetCode####  The problem  widely known as digit root problem  has a congruence formula:  https://en.wikipedia.org/wiki/Digital_root#Congruence_formula  For base b (decimal case b = 10)  the digit root of an integer is:  dr(n) = 0 if n == 0  dr(n) = (b-1) if n != 0 and n % (b-1) == 0  dr(n) = n mod (b-1) if n % (b-1) != 0  or  dr(n) = 1 + (n - 1) % 9  Note here  when n = 0  since (n - 1) % 9 = -1  the return value is zero (correct).  From the formula  we can find that the result of this problem is immanently periodic  with period (b-1).  Output sequence for decimals (b = 10):  ~input: 0 1 2 3 4 ...  output: 0 1 2 3 4 5 6 7 8 9 1 2 3 4 5 6 7 8 9 1 2 3 ....  Henceforth  we can write the following code  whose time and space complexities are both O(1).  class Solution {  public:      int addDigits(int num) {          return 1 + (num - 1) % 9;      }  };  Thanks for reading. :) ####LeetCode#### Iteration method    class Solution(object):    def addDigits(self  num):      """"""      :type num: int      :rtype: int      """"""      while(num >= 10):          temp = 0          while(num > 0):              temp += num % 10              num /= 10          num = temp      return num  Digital Root  this method depends on the truth:  N=(a[0] * 1 + a[1] * 10 + ...a[n] * 10 ^n) and a[0]...a[n] are all between [0 9]  we set M = a[0] + a[1] + ..a[n]  and another truth is that:  1 % 9 = 1  10 % 9 = 1  100 % 9 = 1  so N % 9 = a[0] + a[1] + ..a[n]  means N % 9 = M  so N = M (% 9)  as 9 % 9 = 0 so we can make (n - 1) % 9 + 1 to help us solve the problem when n is 9.as N is 9  ( 9 - 1) % 9 + 1 = 9  class Solution(object):  def addDigits(self  num):      """"""      :type num: int      :rtype: int      """"""      if num == 0 : return 0      else:return (num - 1) % 9 + 1 ####LeetCode#### public class Solution {      public int addDigits(int num) {          if (num == 0){              return 0;          }          if (num % 9 == 0){              return 9;          }          else {              return num % 9;          }      }  } ####LeetCode#### "
Ques_208,Given an array of n integers nums and a target  find the number of index triplets i  j  k with 0 <= i < j < k < n that satisfy the condition nums[i] + nums[j] + nums[k] < target. Example: Input: nums = [-2 0 1 3]  and target = 2 Output: 2  Explanation: Because there are two triplets which sums are less than 2:              [-2 0 1]              [-2 0 3] Follow up: Could you solve it in O(n2) runtime?,Medium,Solution Approach #1 (Brute Force) [Time Limit Exceeded] The brute force approach is to find every possible triplets (i  j  k) (i j k) subjected to i < j < k i<j<k and test for the condition. Complexity analysis Time complexity : O(n^3) O(n 3 ). The total number of such triplets is \binom{n}{3} ( 3 n )  which is \frac{n!}{(n - 3)! \times 3!} = \frac{n \times (n - 1) \times (n - 2)}{6} (nâˆ’3)!Ã—3! n! = 6 nÃ—(nâˆ’1)Ã—(nâˆ’2) . Therefore  the time complexity of the brute force approach is O(n^3) O(n 3 ). Space complexity : O(1) O(1). Approach #2 (Binary Search) [Accepted] Before we solve this problem  it is helpful to first solve this simpler twoSum version. Given a nums nums array  find the number of index pairs i i  j j with 0 \leq i < j < n 0â‰¤i<j<n that satisfy the condition nums[i] + nums[j] < target nums[i]+nums[j]<target If we sort the array first  then we could apply binary search to find the largest index j j such that nums[i] + nums[j] < target nums[i]+nums[j]<target for each i i. Once we found that largest index j j  we know there must be j-i jâˆ’i pairs that satisfy the above condition with i i's value fixed. Finally  we can now apply the twoSum solution to threeSum directly by wrapping an outer for-loop around it. public int threeSumSmaller(int[] nums  int target) {     Arrays.sort(nums);     int sum = 0;     for (int i = 0; i < nums.length - 2; i++) {         sum += twoSumSmaller(nums  i + 1  target - nums[i]);     }     return sum; }  private int twoSumSmaller(int[] nums  int startIndex  int target) {     int sum = 0;     for (int i = startIndex; i < nums.length - 1; i++) {         int j = binarySearch(nums  i  target - nums[i]);         sum += j - i;     }     return sum; }  private int binarySearch(int[] nums  int startIndex  int target) {     int left = startIndex;     int right = nums.length - 1;     while (left < right) {         int mid = (left + right + 1) / 2;         if (nums[mid] < target) {             left = mid;         } else {             right = mid - 1;         }     }     return left; } Note that in the above binary search we choose the upper middle element (\frac{left+right+1}{2}) ( 2 left+right+1 ) instead of the lower middle element (\frac{left+right}{2}) ( 2 left+right ). The reason is due to the terminating condition when there are two elements left. If we chose the lower middle element and the condition nums[mid] < target nums[mid]<target evaluates to true  then the loop will never terminate. Choosing the upper middle element will guarantee termination. Complexity analysis Time complexity : O(n^2 \log n) O(n 2 logn). The binarySearch function takes O(\log n) O(logn) time  therefore the twoSumSmaller takes O(n \log n) O(nlogn) time. The threeSumSmaller wraps with another for-loop  and therefore is O(n^2 \log n) O(n 2 logn) time. Space complexity : O(1) O(1). Approach #3 (Two Pointers) [Accepted] Let us try sorting the array first. For example  nums = [3 5 2 8 1] nums=[3 5 2 8 1] becomes [1 2 3 5 8] [1 2 3 5 8]. Let us look at an example nums = [1 2 3 5 8] nums=[1 2 3 5 8]  and target = 7 target=7. [1  2  3  5  8]  â†‘           â†‘ left       right Let us initialize two indices  left left and right right pointing to the first and last element respectively. When we look at the sum of first and last element  it is 1 + 8 = 9 1+8=9  which is \geq target â‰¥target. That tells us no index pair will ever contain the index right right. So the next logical step is to move the right pointer one step to its left. [1  2  3  5  8]  â†‘        â†‘ left    right Now the pair sum is 1 + 5 = 6 1+5=6  which is < target <target. How many pairs with one of the index = left index=left that satisfy the condition? You can tell by the difference between right right and left left which is 3 3  namely (1 2)  (1 3)  (1 2) (1 3)  and (1 5) (1 5). Therefore  we move left left one step to its right. public int threeSumSmaller(int[] nums  int target) {     Arrays.sort(nums);     int sum = 0;     for (int i = 0; i < nums.length - 2; i++) {         sum += twoSumSmaller(nums  i + 1  target - nums[i]);     }     return sum; }  private int twoSumSmaller(int[] nums  int startIndex  int target) {     int sum = 0;     int left = startIndex;     int right = nums.length - 1;     while (left < right) {         if (nums[left] + nums[right] < target) {             sum += right - left;             left++;         } else {             right--;         }     }     return sum; } Complexity analysis Time complexity : O(n^2) O(n 2 ). The twoSumSmaller function takes O(n) O(n) time because both left and right traverse at most n steps. Therefore  the overall time complexity is O(n^2) O(n 2 ). Space complexity : O(1) O(1). ####LeetCode####  public class Solution {      int count;            public int threeSumSmaller(int[] nums  int target) {          count = 0;          Arrays.sort(nums);          int len = nums.length;                for(int i=0; i<len-2; i++) {              int left = i+1  right = len-1;              while(left < right) {                  if(nums[i] + nums[left] + nums[right] < target) {                      count += right-left;                      left++;                  } else {                      right--;                  }              }          }                    return count;      }  } ####LeetCode#### We sort the array first. Then  for each element  we use the two pointer approach to find the number of triplets that meet the requirements.  Let me illustrate how the two pointer technique works with an example:  target = 2      i  lo    hi  [-2  0  1  3]  We use a for loop (index i) to iterate through each element of the array. For each i  we create two pointers  lo and hi  where lo is initialized as the next element of i  and hi is initialized at the end of the array. If we know that nums[i] + nums[lo] + nums[hi] < target  then we know that since the array is sorted  we can replace hi with any element from lo+1 to nums.length-1  and the requirements will still be met. Just like in the example above  we know that since -2 + 0 + 3 < 2  we can replace hi (3) with 1  and it would still work. Therefore  we can just add hi - lo to the triplet count.  public class Solution {      public int threeSumSmaller(int[] nums  int target) {          int result = 0;          Arrays.sort(nums);          for(int i = 0; i <= nums.length-3; i++) {              int lo = i+1;              int hi = nums.length-1;              while(lo < hi) {                  if(nums[i] + nums[lo] + nums[hi] < target) {                      result += hi - lo;                      lo++;                  } else {                      hi--;                  }              }          }          return result;      }  } ####LeetCode#### # O(n*n) time  def threeSumSmaller(self  nums  target):      count = 0      nums.sort()      for i in xrange(len(nums)):          j  k = i+1  len(nums)-1          while j < k:              s = nums[i] + nums[j] + nums[k]              if s < target:                  # if (i j k) works  then (i j k)  (i j k-1) ...                    # (i j j+1) all work  totally (k-j) triplets                  count += k-j                  j += 1              else:                  k -= 1      return count ####LeetCode#### 
Ques_209,Given an array of numbers nums  in which exactly two elements appear only once and all the other elements appear exactly twice. Find the two elements that appear only once. Example: Input:  [1 2 1 3 2 5] Output: [3 5] Note: The order of the result is not important. So in the above example  [5  3] is also correct. Your algorithm should run in linear runtime complexity. Could you implement it using only constant space complexity?,Medium,Solution Overview The problem could be solved in \mathcal{O}(N) O(N) time and \mathcal{O}(N) O(N) space by using hashmap. To solve the problem in a constant space is a bit tricky but could be done with the help of two bitmasks.   Approach 1: Hashmap Build a hashmap : element -> its frequency. Return only the elements with the frequency equal to 1. Implementation Complexity Analysis Time complexity : \mathcal{O}(N) O(N) to iterate over the input array. Space complexity : \mathcal{O}(N) O(N) to keep the hashmap of N N elements.  Approach 2: Two bitmasks Prerequisites This article will use two bitwise tricks  discussed in details last week : If one builds an array bitmask with the help of XOR operator  following bitmask ^= x strategy  the bitmask would keep only the bits which appear odd number of times. That was discussed in details in the article Single Number II. x & (-x) is a way to isolate the rightmost 1-bit  i.e. to keep the rightmost 1-bit and to set all the others bits to zero. Please refer to the article Power of Two for the detailed explanation. Intuition An interview tip. Imagine  you have a problem to indentify an array element (or elements)  which appears exactly given number of times. Probably  the key is to build first an array bitmask using XOR operator. Examples: In-Place Swap  Single Number  Single Number II. So let's create an array bitmask : bitmask ^= x. This bitmask will not keep any number which appears twice because XOR of two equal bits results in a zero bit a^a = 0. Instead  the bitmask would keep only the difference between two numbers (let's call them x and y) which appear just once. The difference here it's the bits which are different for x and y. Could we extract x and y directly from this bitmask? No. Though we could use this bitmask as a marker to separate x and y. Let's do bitmask & (-bitmask) to isolate the rightmost 1-bit  which is different between x and y. Let's say this is 1-bit for x  and 0-bit for y. Now let's use XOR as before  but for the new bitmask x_bitmask  which will contain only the numbers which have 1-bit in the position of bitmask & (-bitmask). This way  this new bitmask will contain only number x x_bitmask = x  because of two reasons: y has 0-bit in the position bitmask & (-bitmask) and hence will not enter this new bitmask. All numbers but x will not be visible in this new bitmask because they appear two times. Voila  x is identified. Now to identify y is simple: y = bitmask^x. Implementation Complexity Analysis Time complexity : \mathcal{O}(N) O(N) to iterate over the input array. Space complexity : \mathcal{O}(1) O(1)  it's a constant space solution. Analysis written by @liaison and @andvary1 class Solution { 2   public int[] singleNumber(int[] nums) { 3     Map<Integer  Integer> hashmap = new HashMap(); 4     for (int n : nums) 5       hashmap.put(n  hashmap.getOrDefault(n  0) + 1); 6 7     int[] output = new int[2]; 8     int idx = 0; 9     for (Map.Entry<Integer  Integer> item : hashmap.entrySet()) 10       if (item.getValue() == 1) output[idx++] = item.getKey(); 13   } 14 }1 class Solution { 2   public int[] singleNumber(int[] nums) { 3     // difference between two numbers (x and y) which were seen only once 4     int bitmask = 0; 5     for (int num : nums) bitmask ^= num; 6 7     // rightmost 1-bit diff between x and y 8     int diff = bitmask & (-bitmask); 9 10     int x = 0; 11     // bitmask which will contain only x 13 14     return new int[]{x  bitmask^x}; 15   } 16 } ####LeetCode####  Once again  we need to use XOR to solve this problem. But this time  we need to do it in two passes:  In the first pass  we XOR all elements in the array  and get the XOR of the two numbers we need to find. Note that since the two numbers are distinct  so there must be a set bit (that is  the bit with value '1') in the XOR result. Find  out an arbitrary set bit (for example  the rightmost set bit).  In the second pass  we divide all numbers into two groups  one with the aforementioned bit set  another with the aforementinoed bit unset. Two different numbers we need to find must fall into thte two distrinct groups. XOR numbers in each group  we can find a number in either group.  Complexity:  Time: O (n)  Space: O (1)  A Corner Case:  When diff == numeric_limits<int>::min()  -diff is also numeric_limits<int>::min(). Therefore  the value of diff after executing diff &= -diff is still numeric_limits<int>::min(). The answer is still correct.  C++:  class Solution  {  public:      vector<int> singleNumber(vector<int>& nums)       {          // Pass 1 :           // Get the XOR of the two numbers we need to find          int diff = accumulate(nums.begin()  nums.end()  0  bit_xor<int>());          // Get its last set bit          diff &= -diff;            // Pass 2 :          vector<int> rets = {0  0}; // this vector stores the two numbers we will return          for (int num : nums)          {              if ((num & diff) == 0) // the bit is not set              {                  rets[0] ^= num;              }              else // the bit is set              {                  rets[1] ^= num;              }          }          return rets;      }  };  Java:  public class Solution {      public int[] singleNumber(int[] nums) {          // Pass 1 :           // Get the XOR of the two numbers we need to find          int diff = 0;          for (int num : nums) {              diff ^= num;          }          // Get its last set bit          diff &= -diff;                    // Pass 2 :          int[] rets = {0  0}; // this array stores the two numbers we will return          for (int num : nums)          {              if ((num & diff) == 0) // the bit is not set              {                  rets[0] ^= num;              }              else // the bit is set              {                  rets[1] ^= num;              }          }          return rets;      }  }  Thanks for reading :)  Acknowledgements:  Thank @jianchao.li.fighter for introducing this problem and for your encouragement.  Thank @StefanPochmann for your valuable suggestions and comments. Your idea of diff &= -diff is very elegent! And yes  it does not need to XOR for both group in the second pass. XOR for one group suffices. I revise my code accordingly.  Thank @Nakagawa_Kanon for posting this question and presenting the same idea in a previous thread (prior to this thread).  Thank @caijun for providing an interesting test case. ####LeetCode#### If you were stuck by this problem  it's easy to find a solution in the discussion. However  usually  the solution lacks some explanations.  I'm sharing my understanding here:  The two numbers that appear only once must differ at some bit  this is how we can distinguish between them. Otherwise  they will be one of the duplicate numbers.  One important point is that by XORing all the numbers  we actually get the XOR of the two target numbers (because XORing two duplicate numbers always results in 0). Consider the XOR result of the two target numbers; if some bit of the XOR result is 1  it means that the two target numbers differ at that location.  Letâ€™s say the at the ith bit  the two desired numbers differ from each other. which means one number has bit i equaling: 0  the other number has bit i equaling 1.  Thus  all the numbers can be partitioned into two groups according to their bits at location i.  the first group consists of all numbers whose bits at i is 0.  the second group consists of all numbers whose bits at i is 1.  Notice that  if a duplicate number has bit i as 0  then  two copies of it will belong to the first group. Similarly  if a duplicate number has bit i as 1  then  two copies of it will belong to the second group.  by XoRing all numbers in the first group  we can get the first number.  by XoRing all numbers in the second group  we can get the second number. ####LeetCode#### vector<int> singleNumber(vector<int>& nums) {      int aXorb = 0;  // the result of a xor b;      for (auto item : nums) aXorb ^= item;      int lastBit = (aXorb & (aXorb - 1)) ^ aXorb;  // the last bit that a diffs b      int intA = 0  intB = 0;      for (auto item : nums) {          // based on the last bit  group the items into groupA(include a) and groupB          if (item & lastBit) intA = intA ^ item;          else intB = intB ^ item;      }      return vector<int>{intA  intB};     } ####LeetCode#### 
Ques_210,Given n nodes labeled from 0 to n-1 and a list of undirected edges (each edge is a pair of nodes)  write a function to check whether these edges make up a valid tree. Example 1: Input: n = 5  and edges = [[0 1]  [0 2]  [0 3]  [1 4]] Output: true Example 2: Input: n = 5  and edges = [[0 1]  [1 2]  [2 3]  [1 3]  [1 4]] Output: false Note: you can assume that no duplicate edges will appear in edges. Since all edges are undirected  [0 1] is the same as [1 0] and thus will not appear together in edges.,Medium,Subscription Needed ####LeetCode####  public class Solution {      public boolean validTree(int n  int[][] edges) {          // initialize n isolated islands          int[] nums = new int[n];          Arrays.fill(nums  -1);                    // perform union find          for (int i = 0; i < edges.length; i++) {              int x = find(nums  edges[i][0]);              int y = find(nums  edges[i][1]);                            // if two vertices happen to be in the same set              // then there's a cycle              if (x == y) return false;                            // union              nums[x] = y;          }                    return edges.length == n - 1;      }            int find(int nums[]  int i) {          if (nums[i] == -1) return i;          return find(nums  nums[i]);      }  } ####LeetCode#### class Solution {  public:      bool validTree(int n  vector<pair<int  int>>& edges) {          vector<int> nodes(n 0);          for(int i=0; i<n; i++) nodes[i] = i;          for(int i=0; i<edges.size(); i++){              int f = edges[i].first;              int s = edges[i].second;              while(nodes[f]!=f) f = nodes[f];              while(nodes[s]!=s) s = nodes[s];              if(nodes[f] == nodes[s]) return false;              nodes[s] = f;          }          return edges.size() == n-1;      }  };  To tell whether a graph is a valid tree  we have to:  Make sure there is no cycle in the graph - this has to be a none-cyclic graph;  Make sure every node is reached - this has to be a connected graph;  Reference: https://en.wikipedia.org/wiki/Tree_(graph_theory)  Solution:  To test cyclic  we can make an array for each node (as array index)  and the array will store the parent of the node (as array index). Every time we fetch a new pair of nodes  we trace the root node (the deepest parent node) of these two nodes  if it has the same root  then is will be a cycle; otherwise  we set the parent of second node to be the first node;  After we make sure there is node cycle in the graph  we simple test if there is enough edges to make this graph connected. ####LeetCode#### public class Solution {      public boolean validTree(int n  int[][] edges) {          // initialize adjacency list          List<List<Integer>> adjList = new ArrayList<List<Integer>>(n);                    // initialize vertices          for (int i = 0; i < n; i++)              adjList.add(i  new ArrayList<Integer>());                    // add edges              for (int i = 0; i < edges.length; i++) {              int u = edges[i][0]  v = edges[i][1];              adjList.get(u).add(v);              adjList.get(v).add(u);          }                    boolean[] visited = new boolean[n];                    // make sure there's no cycle          if (hasCycle(adjList  0  visited  -1))              return false;                    // make sure all vertices are connected          for (int i = 0; i < n; i++) {              if (!visited[i])                   return false;          }                    return true;      }            // check if an undirected graph has cycle started from vertex u      boolean hasCycle(List<List<Integer>> adjList  int u  boolean[] visited  int parent) {          visited[u] = true;                    for (int i = 0; i < adjList.get(u).size(); i++) {              int v = adjList.get(u).get(i);                            if ((visited[v] && parent != v) || (!visited[v] && hasCycle(adjList  v  visited  u)))                  return true;          }                    return false;      }  } ####LeetCode#### 
Ques_211,SQL Schema The Trips table holds all taxi trips. Each trip has a unique Id  while Client_Id and Driver_Id are both foreign keys to the Users_Id at the Users table. Status is an ENUM type of (â€˜completedâ€™  â€˜cancelled_by_driverâ€™  â€˜cancelled_by_clientâ€™). +----+-----------+-----------+---------+--------------------+----------+ | Id | Client_Id | Driver_Id | City_Id |        Status      |Request_at| +----+-----------+-----------+---------+--------------------+----------+ | 1  |     1     |    10     |    1    |     completed      |2013-10-01| | 2  |     2     |    11     |    1    | cancelled_by_driver|2013-10-01| | 3  |     3     |    12     |    6    |     completed      |2013-10-01| | 4  |     4     |    13     |    6    | cancelled_by_client|2013-10-01| | 5  |     1     |    10     |    1    |     completed      |2013-10-02| | 6  |     2     |    11     |    6    |     completed      |2013-10-02| | 7  |     3     |    12     |    6    |     completed      |2013-10-02| | 8  |     2     |    12     |    12   |     completed      |2013-10-03| | 9  |     3     |    10     |    12   |     completed      |2013-10-03|  | 10 |     4     |    13     |    12   | cancelled_by_driver|2013-10-03| +----+-----------+-----------+---------+--------------------+----------+ The Users table holds all users. Each user has an unique Users_Id  and Role is an ENUM type of (â€˜clientâ€™  â€˜driverâ€™  â€˜partnerâ€™). +----------+--------+--------+ | Users_Id | Banned |  Role  | +----------+--------+--------+ |    1     |   No   | client | |    2     |   Yes  | client | |    3     |   No   | client | |    4     |   No   | client | |    10    |   No   | driver | |    11    |   No   | driver | |    12    |   No   | driver | |    13    |   No   | driver | +----------+--------+--------+ Write a SQL query to find the cancellation rate of requests made by unbanned users (both client and driver must be unbanned) between Oct 1  2013 and Oct 3  2013. The cancellation rate is computed by dividing the number of canceled (by client or driver) requests made by unbanned users by the total number of requests made by unbanned users. For the above tables  your SQL query should return the following rows with the cancellation rate being rounded to two decimal places. +------------+-------------------+ |     Day    | Cancellation Rate | +------------+-------------------+ | 2013-10-01 |       0.33        | | 2013-10-02 |       0.00        | | 2013-10-03 |       0.50        | +------------+-------------------+ Credits: Special thanks to @cak1erlizhou for contributing this question  writing the problem description and adding part of the test cases.,Hard,"Subscription Needed ####LeetCode####  SELECT Request_at as Day          ROUND(COUNT(IF(Status != 'completed'  TRUE  NULL)) / COUNT(*)  2) AS 'Cancellation Rate'  FROM Trips  WHERE (Request_at BETWEEN '2013-10-01' AND '2013-10-03')        AND Client_id NOT IN (SELECT Users_Id FROM Users WHERE Banned = 'Yes')  GROUP BY Request_at; ####LeetCode#### select   t.Request_at Day    round(sum(case when t.Status like 'cancelled_%' then 1 else 0 end)/count(*) 2) Rate  from Trips t   inner join Users u   on t.Client_Id = u.Users_Id and u.Banned='No'  where t.Request_at between '2013-10-01' and '2013-10-03'  group by t.Request_at ####LeetCode#### The where condition checks for two things.  The current client is not 'Banned'  Date is between the range  The second cluase (column) of SELECT statement is tricky. Let's understand what each function is used for  LOWER(column) to change the values to lowercase. This is because MySQL doesn't have ILIKE function to have case-insensitive comparision  LIKE for partial text matches  CASE for counting only cancelled rides instead of all non-null values  1.000 to account for decimals after divisions  COUNT(id) to get total trips for that day  ROUND(value  2) to round off the result to 2 decimal places Do not confuse with TRUNCATE function  Rename the processed column to the expected one  SELECT      request_at AS ""Day""                ROUND(((SUM(CASE WHEN LOWER(Status) LIKE ""cancelled%"" THEN 1.000 ELSE 0 END)) / COUNT(id))  2) AS ""Cancellation Rate""   FROM        trips  WHERE       client_id NOT IN (SELECT users_id FROM users WHERE banned = 'Yes')  AND         request_at BETWEEN '2013-10-01' AND '2013-10-03'  GROUP BY    request_at;  This solution is faster than the above one  We are joining and hence each record doesn't need to run an inner query to check for the banned status. If it did  it takes a lot of time repeating the same task over and over.  SELECT      request_at AS ""Day""                ROUND(((SUM(CASE WHEN LOWER(Status) LIKE ""cancelled%"" THEN 1.000 ELSE 0 END)) / COUNT(id))  2) AS ""Cancellation Rate""   FROM        trips AS t  JOIN        users AS u  ON          t.client_id = u.users_id  AND         u.banned ='No'  WHERE       request_at BETWEEN '2013-10-01' AND '2013-10-03'  GROUP BY    request_at; ####LeetCode#### "
Ques_212,Write a program to find the n-th ugly number. Ugly numbers are positive numbers whose prime factors only include 2  3  5.  Example: Input: n = 10 Output: 12 Explanation: 1  2  3  4  5  6  8  9  10  12 is the sequence of the first 10 ugly numbers. Note:   1 is typically treated as an ugly number. n does not exceed 1690.,Medium,"Solution Two levels of optimisation Let's imagine that the problem is solved somehow for the number n and we've put the solution directly in nthUglyNumber method of the Solution class. Now let's check the context: there are 596 test cases  for the most of them n is larger than 50  and n is known to be smaller than 1691. Hence instead of computing 596 \times 50 = 29800 596Ã—50=29800 ugly numbers in total  one could precompute all 1690 numbers  and significantly speed up the submission. How to precompute? Use another class Ugly with all computations in the constructor and then declare Ugly instance as a static variable of Solution class. Now let's consider two different approaches to perform the preliminary computations.  Approach 1: Heap Intuition Let's start from the heap which contains just one number: 1. To compute next ugly numbers  pop 1 from the heap and push instead three numbers: 1 \times 2 1Ã—2  1 \times 3 1Ã—3  and 1 \times 5 1Ã—5. Now the smallest number in the heap is 2. To compute next ugly numbers  pop 2 from the heap and push instead three numbers: 2 \times 2 2Ã—2  2 \times 3 2Ã—3  and 2 \times 5 2Ã—5. One could continue like this to compute first 1690 ugly numbers. At each step  pop the smallest ugly number k from the heap  and push instead three ugly numbers: k \times 2 kÃ—2  k \times 3 kÃ—3  and k \times 5 kÃ—5. 1 / 5 Algorithm Precompute 1690 ugly numbers: Initiate array of precomputed ugly numbers nums  heap heap and hashset seen to track all elements already pushed in the heap in order to avoid duplicates. Make a loop of 1690 steps. At each step: Pop the smallest element k out of heap and add it into the array of precomputed ugly numbers. Push 2k  3k and 5k in the heap if they are not yet in the hashset. Update the hashset of seen ugly numbers as well. Retrieve needed ugly number from the array of precomputed numbers. Implementation Complexity Analysis Time complexity : \mathcal{O}(1) O(1) to retrieve preliminary computed ugly number  and more than 12 \times 10^6 12Ã—10 6 operations for preliminary computations. Let's estimate the number of operations needed for the preliminary computations. For loop here has 1690 steps  and each step performs 1 pop  not more than 3 pushes and 3 contains / in operations for the hashset. Pop and push have logarithmic time complexity and hence much cheaper than the linear search  so let's estimate only the last term. This arithmetic progression is easy to estimate: 1 + 2 + 3 + ... + 1690 \times 3 = \frac{(1 + 1690 \times 3) \times 1690 \times 3}{2} > 4.5 \times 1690^2 1+2+3+...+1690Ã—3= 2 (1+1690Ã—3)Ã—1690Ã—3 >4.5Ã—1690 2 Space complexity : constant space to keep an array of 1690 ugly numbers  the heap of not more than 1690 \times 2 1690Ã—2 elements and the hashset of not more than 1690 \times 3 1690Ã—3 elements.  Approach 2: Dynamic Programming Intuition Preliminary computations in Approach 1 are quite heavy  and could be optimised with dynamic programming. Let's start from the array of ugly numbers which contains just one number - 1. Let's use three pointers i_2 i 2   i_3 i 3 and i_5 i 5   to mark the last ugly number which was multiplied by 2  3 and 5  correspondingly. The algorithm is straightforward: choose the smallest ugly number among 2 \times \textrm{nums}[i_2] 2Ã—nums[i 2 ]  3 \times \textrm{nums}[i_3] 3Ã—nums[i 3 ]  and 5 \times \textrm{nums}[i_5] 5Ã—nums[i 5 ] and add it into the array. Move the corresponding pointer by one step. Repeat till you'll have 1690 ugly numbers. 1 / 11 Algorithm Precompute 1690 ugly numbers: Initiate array of precomputed ugly numbers nums and three pointers i2  i3 and i5 to track the index of the last ugly number used to produce the next ones. Make a loop of 1690 steps. At each step: Choose the smallest number among nums[i2] * 2  nums[i3] * 3  and nums[i5] * 5 and add it into nums. Move by one the pointer which corresponds to the ""ancestor"" of the added number. Retrieve needed ugly number from the array of precomputed numbers. Implementation Complexity Analysis Time complexity : \mathcal{O}(1) O(1) to retrieve preliminary computed ugly number  and about 1690 \times 5 = 8450 1690Ã—5=8450 operations for preliminary computations. Space complexity : constant space to keep an array of 1690 ugly numbers. Analysis written by @liaison and @andvary1 class Ugly { 2   public int[] nums = new int[1690]; 3   Ugly() { 4     HashSet<Long> seen = new HashSet(); 5     PriorityQueue<Long> heap = new PriorityQueue<Long>(); 6     seen.add(1L); 7     heap.add(1L); 8 9     long currUgly  newUgly; 10     int[] primes = new int[]{2  3  5}; 11     for(int i = 0; i < 1690; ++i) { 12       currUgly = heap.poll(); 13       nums[i] = (int)currUgly; 14       for(int j : primes) { 15         newUgly = currUgly * j; 16         if (!seen.contains(newUgly)) { 17           seen.add(newUgly); 18           heap.add(newUgly); 19         } 20       } 21     } 22   } 23 25 class Solution { 26   public static Ugly u = new Ugly(); 27   public int nthUglyNumber(int n) { 28     return u.nums[n - 1]; 29   } 30 }1 class Ugly { 2   public int[] nums = new int[1690]; 3   Ugly() { 4     nums[0] = 1; 5     int ugly  i2 = 0  i3 = 0  i5 = 0; 6 7     for(int i = 1; i < 1690; ++i) { 8       ugly = Math.min(Math.min(nums[i2] * 2  nums[i3] * 3)  nums[i5] * 5); 9       nums[i] = ugly; 10 11       if (ugly == nums[i2] * 2) ++i2; 12       if (ugly == nums[i3] * 3) ++i3; 13       if (ugly == nums[i5] * 5) ++i5; 14     } 15   } 16 } 17         Ugly(); 20   public int nthUglyNumber(int n) { 21     return u.nums[n - 1]; 22   } 23 } ####LeetCode####  We have an array k of first n ugly number. We only know  at the beginning  the first one  which is 1. Then  k[1] = min( k[0]x2  k[0]x3  k[0]x5). The answer is k[0]x2. So we move 2's pointer to 1. Then we test:  k[2] = min( k[1]x2  k[0]x3  k[0]x5). And so on. Be careful about the cases such as 6  in which we need to forward both pointers of 2 and 3.  x here is multiplication.  class Solution {  public:      int nthUglyNumber(int n) {          if(n <= 0) return false; // get rid of corner cases           if(n == 1) return true; // base case          int t2 = 0  t3 = 0  t5 = 0; //pointers for 2  3  5          vector<int> k(n);          k[0] = 1;          for(int i  = 1; i < n ; i ++)          {              k[i] = min(k[t2]*2 min(k[t3]*3 k[t5]*5));              if(k[i] == k[t2]*2) t2++;               if(k[i] == k[t3]*3) t3++;              if(k[i] == k[t5]*5) t5++;          }          return k[n-1];      }  }; ####LeetCode#### The idea of this solution is from this page:http://www.geeksforgeeks.org/ugly-numbers/  The ugly-number sequence is 1  2  3  4  5  6  8  9  10  12  15  â€¦  because every number can only be divided by 2  3  5  one way to look at the sequence is to split the sequence to three groups as below:  (1) 1Ã—2  2Ã—2  3Ã—2  4Ã—2  5Ã—2  â€¦  (2) 1Ã—3  2Ã—3  3Ã—3  4Ã—3  5Ã—3  â€¦  (3) 1Ã—5  2Ã—5  3Ã—5  4Ã—5  5Ã—5  â€¦  We can find that every subsequence is the ugly-sequence itself (1  2  3  4  5  â€¦) multiply 2  3  5.  Then we use similar merge method as merge sort  to get every ugly number from the three subsequence.  Every step we choose the smallest one  and move one step after including nums with same value.  Thanks for this author about this brilliant idea. Here is my java solution  public class Solution {      public int nthUglyNumber(int n) {          int[] ugly = new int[n];          ugly[0] = 1;          int index2 = 0  index3 = 0  index5 = 0;          int factor2 = 2  factor3 = 3  factor5 = 5;          for(int i=1;i<n;i++){              int min = Math.min(Math.min(factor2 factor3) factor5);              ugly[i] = min;              if(factor2 == min)                  factor2 = 2*ugly[++index2];              if(factor3 == min)                  factor3 = 3*ugly[++index3];              if(factor5 == min)                  factor5 = 5*ugly[++index5];          }          return ugly[n-1];      }  } ####LeetCode#### struct Solution {      int nthUglyNumber(int n) {          vector <int> results (1 1);          int i = 0  j = 0  k = 0;          while (results.size() < n)          {              results.push_back(min(results[i] * 2  min(results[j] * 3  results[k] * 5)));              if (results.back() == results[i] * 2) ++i;              if (results.back() == results[j] * 3) ++j;              if (results.back() == results[k] * 5) ++k;          }          return results.back();      }  };  Explanation:  The key is to realize each number can be and have to be generated by a former number multiplied by 2  3 or 5  e.g.  1 2 3 4 5 6 8 9 10 12 15..  what is next?  it must be x * 2 or y * 3 or z * 5  where x  y  z is an existing number.  How do we determine x  y  z then?  apparently  you can just traverse the sequence generated by far from 1 ... 15  until you find such x  y  z that x * 2  y * 3  z * 5 is just bigger than 15. In this case x=8  y=6  z=4. Then you compare x * 2  y * 3  z * 5 so you know next number will be x * 2 = 8 * 2 = 16.  k  now you have 1 2 3 4 .... 15  16   Then what is next?  You wanna do the same process again to find the new x  y  z  but you realize  wait  do I have to  traverse the sequence generated by far again?  NO! since you know last time  x=8  y=6  z=4 and x=8 was used to generate 16  so this time  you can immediately know the new_x = 9 (the next number after 8 is 9 in the generated sequence)  y=6  z=4.  Then you need to compare new_x * 2  y * 3  z * 5. You know next number is 9 * 2 = 18;  And you also know  the next x will be 10 since new_x = 9 was used this time.  But what is next y? apparently  if y=6  6*3 = 18  which is already generated in this round. So you also need to update next y from 6 to 8.  Based on the idea above  you can actually generated x y z from very beginning  and update x  y  z accordingly. It ends up with a O(n) solution. ####LeetCode#### "
Ques_213,There are a row of n houses  each house can be painted with one of the k colors. The cost of painting each house with a certain color is different. You have to paint all the houses such that no two adjacent houses have the same color. The cost of painting each house with a certain color is represented by a n x k cost matrix. For example  costs[0][0] is the cost of painting house 0 with color 0; costs[1][2] is the cost of painting house 1 with color 2  and so on... Find the minimum cost to paint all houses. Note: All costs are positive integers. Example: Input: [[1 5 3] [2 9 4]] Output: 5 Explanation: Paint house 0 into color 0  paint house 1 into color 2. Minimum cost: 1 + 4 = 5;               Or paint house 0 into color 2  paint house 1 into color 0. Minimum cost: 3 + 2 = 5.  Follow up: Could you solve it in O(nk) runtime?,Hard,Subscription Needed ####LeetCode####  The idea is similar to the problem Paint House I  for each house and each color  the minimum cost of painting the house with that color should be the minimum cost of painting previous houses  and make sure the previous house doesn't paint with the same color.  We can use min1 and min2 to track the indices of the 1st and 2nd smallest cost till previous house  if the current color's index is same as min1  then we have to go with min2  otherwise we can safely go with min1.  The code below modifies the value of costs[][] so we don't need extra space.  public int minCostII(int[][] costs) {      if (costs == null || costs.length == 0) return 0;                int n = costs.length  k = costs[0].length;      // min1 is the index of the 1st-smallest cost till previous house      // min2 is the index of the 2nd-smallest cost till previous house      int min1 = -1  min2 = -1;            for (int i = 0; i < n; i++) {          int last1 = min1  last2 = min2;          min1 = -1; min2 = -1;                    for (int j = 0; j < k; j++) {              if (j != last1) {                  // current color j is different to last min1                  costs[i][j] += last1 < 0 ? 0 : costs[i - 1][last1];              } else {                  costs[i][j] += last2 < 0 ? 0 : costs[i - 1][last2];              }                            // find the indices of 1st and 2nd smallest cost of painting current house i              if (min1 < 0 || costs[i][j] < costs[i][min1]) {                  min2 = min1; min1 = j;              } else if (min2 < 0 || costs[i][j] < costs[i][min2]) {                  min2 = j;              }          }      }            return costs[n - 1][min1];  } ####LeetCode#### Explanation: dp[i][j] represents the min paint cost from house 0 to house i when house i use color j; The formula will be dp[i][j] = Math.min(any k!= j| dp[i-1][k]) + costs[i][j].  Take a closer look at the formula  we don't need an array to represent dp[i][j]  we only need to know the min cost to the previous house of any color and if the color j is used on previous house to get prev min cost  use the second min cost that are not using color j on the previous house. So I have three variable to record: prevMin  prevMinColor  prevSecondMin. and the above formula will be translated into: dp[currentHouse][currentColor] = (currentColor == prevMinColor? prevSecondMin: prevMin) + costs[currentHouse][currentColor].  public class Solution {  public int minCostII(int[][] costs) {      if(costs == null || costs.length == 0 || costs[0].length == 0) return 0;            int n = costs.length  k = costs[0].length;      if(k == 1) return (n==1? costs[0][0] : -1);            int prevMin = 0  prevMinInd = -1  prevSecMin = 0;//prevSecMin always >= prevMin      for(int i = 0; i<n; i++) {          int min = Integer.MAX_VALUE  minInd = -1  secMin = Integer.MAX_VALUE;          for(int j = 0; j<k;  j++) {              int val = costs[i][j] + (j == prevMinInd? prevSecMin : prevMin);              if(minInd< 0) {min = val; minInd = j;}//when min isn't initialized              else if(val < min) {//when val < min                    secMin = min;                  min = val;                  minInd = j;              } else if(val < secMin) { //when min<=val< secMin                  secMin = val;              }          }          prevMin = min;          prevMinInd = minInd;          prevSecMin = secMin;      }      return prevMin;  }  } ####LeetCode#### To solve this DP problem:  If there's no constraint  we choose min cost for each house.  Since house[i] and house[i - 1] cannot have the same color j  we should choose 2nd min color for house[i - 1].  If we choose the 3rd min color for house[i - 1]  we might miss potential min cost.  min(i) = min(cost[i][j] + 1st min / 2nd min)  0 < j < n.  Since current row only relies on last row for getting mins and avoiding same color  O(1) space is enough.  public int minCostII(int[][] costs) {      if (costs.length == 0) {          return 0;      }      int min1 = 0  min2 = 0  index1 = -1;            for (int i = 0; i < costs.length; i++) {          int m1 = Integer.MAX_VALUE  m2 = Integer.MAX_VALUE  idx1 = -1;                    for (int j = 0; j < costs[0].length; j++) {              int cost = costs[i][j] + (j != index1 ? min1 : min2);                 if (cost < m1) {           // cost < m1 < m2                  m2 = m1; m1 = cost; idx1 = j;                             } else if (cost < m2) {    // m1 < cost < m2                  m2 = cost;              }          }                    min1 = m1; min2 = m2; index1 = idx1;      }      return min1;  } ####LeetCode#### 
Ques_214,"Given a string  determine if a permutation of the string could form a palindrome. Example 1: Input: ""code"" Output: false Example 2: Input: ""aab"" Output: true Example 3: Input: ""carerac"" Output: true",Easy,Solution Approach #1 Brute Force [Accepted] If a string with an even length is a palindrome  every character in the string must always occur an even number of times. If the string with an odd length is a palindrome  every character except one of the characters must always occur an even number of times. Thus  in case of a palindrome  the number of characters with odd number of occurences can't exceed 1(1 in case of odd length and 0 in case of even length). Based on the above observation  we can find the solution for the given problem. The given string could contain atmost all the ASCII characters from 0 to 127. Thus  we iterate over all the characters from 0 to 127. For every character chosen  we again iterate over the given string s s and find the number of occurences  ch ch  of the current character in s s. We also keep a track of the number of characters in the given string s s with odd number of occurences in a variable count count. If  for any character currently considered  its corresponding count  ch ch  happens to be odd  we increment the value of count count  to reflect the same. In case of even value of ch ch for any character  the count count remains unchanged. If  for any character  the count count becomes greater than 1  it indicates that the given string s s can't lead to the formation of a palindromic permutation based on the reasoning discussed above. But  if the value of count count remains lesser than 2 even when all the possible characters have been considered  it indicates that a palindromic permutation can be formed from the given string s s. Complexity Analysis Time complexity : O(128*n) O(128âˆ—n). We iterate constant number of times(128) over the string s s of length n n giving a time complexity of 128n 128n. Space complexity : O(1) O(1). Constant extra space is used. Approach #2 Using HashMap [Accepted] Algorithm From the discussion above  we know that to solve the given problem  we need to count the number of characters with odd number of occurences in the given string s s. To do so  we can also make use of a hashmap  map map. This map map takes the form (character_i  number of occurences of character_i) (character i  numberofoccurencesofcharacter i ). We traverse over the given string s s. For every new character found in s s  we create a new entry in the map map for this character with the number of occurences as 1. Whenever we find the same character again  we update the number of occurences appropriately. At the end  we traverse over the map map created and find the number of characters with odd number of occurences. If this count count happens to exceed 1 at any step  we conclude that a palindromic permutation isn't possible for the string s s. But  if we can reach the end of the string with count count lesser than 2  we conclude that a palindromic permutation is possible for s s. The following animation illustrates the process. 1 / 13 Complexity Analysis Time complexity : O(n) O(n). We traverse over the given string s s with n n characters once. We also traverse over the map map which can grow upto a size of n n in case all characters in s s are distinct. Space complexity : O(n) O(n). The hashmap can grow upto a size of n n  in case all the characters in s s are distinct. Approach #3 Using Array [Accepted] Algorithm Instead of making use of the inbuilt Hashmap  we can make use of an array as a hashmap. For this  we make use of an array map map with length 128. Each index of this map map corresponds to one of the 128 ASCII characters possible. We traverse over the string s s and put in the number of occurences of each character in this map map appropriately as done in the last case. Later on  we find the number of characters with odd number of occurences to determine if a palindromic permutation is possible for the string s s or not as done in previous approaches. **Complexity Analysis** Time complexity : O(n) O(n). We traverse once over the string s s of length n n. Then  we traverse over the map map of length 128(constant). Space complexity : O(1) O(1). Constant extra space is used for map map of size 128. Approach #4 Single Pass [Accepted]: Algorithm Instead of first traversing over the string s s for finding the number of occurences of each element and then determining the count count of characters with odd number of occurences in s s  we can determine the value of count count on the fly while traversing over s s. For this  we traverse over s s and update the number of occurences of the character just encountered in the map map. But  whevenever we update any entry in map map  we also check if its value becomes even or odd. We start of with a count count value of 0. If the value of the entry just updated in map map happens to be odd  we increment the value of count count to indicate that one more character with odd number of occurences has been found. But  if this entry happens to be even  we decrement the value of count count to indicate that the number of characters with odd number of occurences has reduced by one. But  in this case  we need to traverse till the end of the string to determine the final result  unlike the last approaches  where we could stop the traversal over map map as soon as the count count exceeded 1. This is because  even if the number of elements with odd number of occurences may seem very large at the current moment  but their occurences could turn out to be even when we traverse further in the string s s. At the end  we again check if the value of count count is lesser than 2 to conclude that a palindromic permutation is possible for the string s s. Complexity Analysis Time complexity : O(n) O(n). We traverse over the string s s of length n n once only. Space complexity : O(128) O(128). A map map of constant size(128) is used. Approach #5 Using Set [Accepted]: Algorithm Another modification of the last approach could be by making use of a set set for keeping track of the number of elements with odd number of occurences in s s. For doing this  we traverse over the characters of the string s s. Whenver the number of occurences of a character becomes odd  we put its entry in the set set. Later on  if we find the same element again  lead to its number of occurences as even  we remove its entry from the set set. Thus  if the element occurs again(indicating an odd number of occurences)  its entry won't exist in the set set. Based on this idea  when we find a character in the string s s that isn't present in the set set(indicating an odd number of occurences currently for this character)  we put its corresponding entry in the set set. If we find a character that is already present in the set set(indicating an even number of occurences currently for this character)  we remove its corresponding entry from the set set. At the end  the size of set set indicates the number of elements with odd number of occurences in s s. If it is lesser than 2  a palindromic permutation of the string s s is possible  otherwise not. Below code is inspired by @StefanPochmann Complexity Analysis Time complexity : O(n) O(n). We traverse over the string s s of length n n once only. Space complexity : O(n) O(n). The set set can grow upto a maximum size of n n in case of all distinct elements. Analysis written by: @vinod231 public class Solution { 2     public boolean canPermutePalindrome(String s) { 3         int count = 0; 4         for (char i = 0; i < 128 && count <= 1; i++) { 5             int ct = 0; 6             for (int j = 0; j < s.length(); j++) { 7                 if (s.charAt(j) == i) 8                     ct++; 9             } 10             count += ct % 2; 11         } 12         return count <= 1; 151 public class Solution { 2  public boolean canPermutePalindrome(String s) { 3      HashMap < Character  Integer > map = new HashMap < > (); 4      for (int i = 0; i < s.length(); i++) { 5          map.put(s.charAt(i)  map.getOrDefault(s.charAt(i)  0) + 1); 6      } 7      int count = 0; 8      for (char key: map.keySet()) { 9          count += map.get(key) % 2; 10      } 13 } 141 public class Solution { 2     public boolean canPermutePalindrome(String s) { 3         int[] map = new int[128]; 4         for (int i = 0; i < s.length(); i++) { 5             map[s.charAt(i)]++; 6         } 7         int count = 0; 8         for (int key = 0; key < map.length && count <= 1; key++) { 9             count += map[key] % 2; 10         } 11         return count <= 1;1 public class Solution { 2     public boolean canPermutePalindrome(String s) { 3         int[] map = new int[128]; 4         int count = 0; 5         for (int i = 0; i < s.length(); i++) { 6             map[s.charAt(i)]++; 7             if (map[s.charAt(i)] % 2 == 0) 8                 count--; 9             else 10                 count++; 11         } 12         return count <= 1;1 public class Solution { 2     public boolean canPermutePalindrome(String s) { 3         Set < Character > set = new HashSet < > (); 4         for (int i = 0; i < s.length(); i++) { 5             if (!set.add(s.charAt(i))) 6                 set.remove(s.charAt(i)); 7         } 10 } 11 ####LeetCode####  The idea is to iterate over string  adding current character to set if set doesn't contain that character  or removing current character from set if set contains it.  When the iteration is finished  just return set.size()==0 || set.size()==1.  set.size()==0 corresponds to the situation when there are even number of any character in the string  and  set.size()==1 corresponsds to the fact that there are even number of any character except one.  public class Solution {      public boolean canPermutePalindrome(String s) {          Set<Character> set=new HashSet<Character>();          for(int i=0; i<s.length(); ++i){              if (!set.contains(s.charAt(i)))                  set.add(s.charAt(i));              else                   set.remove(s.charAt(i));          }          return set.size()==0 || set.size()==1;      }  } ####LeetCode#### Just check that no more than one character appears an odd number of times. Because if there is one  then it must be in the middle of the palindrome. So we can't have two of them.  Python  First count all characters in a Counter  then count the odd ones.  def canPermutePalindrome(self  s):      return sum(v % 2 for v in collections.Counter(s).values()) < 2  Ruby  Using an integer as a bitset (Ruby has arbitrarily large integers).  def can_permute_palindrome(s)    x = s.chars.map { |c| 1 << c.ord }.reduce(0  :^)    x & x-1 == 0  end  C++  Using a bitset.  bool canPermutePalindrome(string s) {      bitset<256> b;      for (char c : s)          b.flip(c);      return b.count() < 2;  }  C  Tricky one. Increase odds when the increased counter is odd  decrease it otherwise.  bool canPermutePalindrome(char* s) {      int ctr[256] = {}  odds = 0;      while (*s)          odds += ++ctr[*s++] & 1 ? 1 : -1;      return odds < 2;  }  Thanks to jianchao.li.fighter for pointing out a nicer way in the comments to which I switched now because it's clearer and faster. Some speed test results (see comments for details):          odds += ++ctr[*s++] % 2 * 2 - 1;       // 1499 ms mean-of-five (my original)          odds += (ctr[*s++] ^= 1) * 2 - 1;      // 1196 ms mean-of-five          odds += ++ctr[*s++] % 2 ? 1 : -1;      // 1108 ms mean-of-five          odds += ((++ctr[*s++] & 1) << 1) - 1;  // 1217 ms mean-of-five          odds += ++ctr[*s++] & 1 ? 1 : -1;      // 1132 ms mean-of-five  Java  Using a BitSet.  public boolean canPermutePalindrome(String s) {      BitSet bs = new BitSet();      for (byte b : s.getBytes())          bs.flip(b);      return bs.cardinality() < 2;  } ####LeetCode#### Explanation  The basic idea is using HashSet to find the number of single characters  which should be at most 1.  public boolean canPermutePalindrome(String s) {   Set<Character>set = new HashSet<Character>();   for (char c : s.toCharArray())      if (set.contains(c)) set.remove(c);// If char already exists in set  then remove it from set    else set.add(c);// If char doesn't exists in set  then add it to set   return set.size() <= 1;  } ####LeetCode#### 
Ques_215,"Given a string s  return all the palindromic permutations (without duplicates) of it. Return an empty list if no palindromic permutation could be form. Example 1: Input: ""aabb"" Output: [""abba""  ""baab""] Example 2: Input: ""abc"" Output: []",Medium,"Solution Approach #1 Brute Force [Time Limit Exceeded] The simplest solution is generate every possible permutation of the given string s s and check if the generated permutation is a palindrome. After this  the palindromic permuations can be added to a set set in order to eliminate the duplicates. At the end  we can return an array comprised of the elements of this set set as the resultant array. Let's look at the way these permutations are generated. We make use of a recursive function permute which takes the index of the current element current_index current i ndex as one of the arguments. Then  it swaps the current element with every other element in the array  lying towards its right  so as to generate a new ordering of the array elements. After the swapping has been done  it makes another call to permute but this time with the index of the next element in the array. While returning back  we reverse the swapping done in the current function call. Thus  when we reach the end of the array  a new ordering of the array's elements is generated. The animation below depicts the ways the permutations are generated. 1 / 11 Complexity Analysis Time complexity : O((n+1)!) O((n+1)!). A total of n! n! permutations are possible. For every permutation generated  we need to check if it is a palindrome  each of which requires O(n) O(n) time. Space complexity : O(n) O(n). The depth of the recursion tree can go upto n n. Approach #2 Backtracking [Accepted] Algorithm It might be possible that no palindromic permutation could be possible for the given string s s. Thus  it is useless to generate the permutations in such a case. Taking this idea  firstly we check if a palindromic permutation is possible for s s. If yes  then only we proceed further with generating the permutations. To check this  we make use of a hashmap map map which stores the number of occurences of each character(out of 128 ASCII characters possible). If the number of characters with odd number of occurences exceeds 1  it indicates that no palindromic permutation is possible for s s. To look at this checking process in detail  look at Approach 4 of the article for Palindrome Permutation. Once we are sure that a palindromic permutation is possible for s s  we go for the generation of the required permutations. But  instead of wildly generating all the permutations  we can include some smartness in the generation of permutations i.e. we can generate only those permutations which are already palindromes. One idea to to do so is to generate only the first half of the palindromic string and to append its reverse string to itself to generate the full length palindromic string. Based on this idea  by making use of the number of occurences of the characters in s s stored in map map  we create a string st st which contains all the characters of s s but with the number of occurences of these characters in st st reduced to half their original number of occurences in s s. Thus  now we can generate all the permutations of this string st st and append the reverse of this permuted string to itself to create the palindromic permutations of s s. In case of a string s s with odd length  whose palindromic permutations are possible  one of the characters in s s must be occuring an odd number of times. We keep a track of this character  ch ch  and it is kept separte from the string st st. We again generate the permutations for st st similarly and append the reverse of the generated permutation to itself  but we also place the character ch ch at the middle of the generated string. In this way  only the required palindromic permutations will be generated. Even if we go with the above idea  a lot of duplicate strings will be generated. In order to avoid generating duplicate palindromic permutations in the first place itself  as much as possible  we can make use of this idea. As discussed in the last approach  we swap the current element with all the elements lying towards its right to generate the permutations. Before swapping  we can check if the elements being swapped are equal. If so  the permutations generated even after swapping the two will be duplicates(redundant). Thus  we need not proceed further in such a case. See this animation for a clearer understanding. 1 / 9 Complexity Analysis Time complexity : O\big((\frac{n}{2}+1)!\big) O(( 2 n +1)!). Atmost \frac{n}{2}! 2 n ! permutations need to be generated in the worst case. Further  for each permutation generated  string.reverse() function will take n/4 n/4 time. Space complexity : O(n) O(n). The depth of recursion tree can go upto n/2 n/2 in the worst case. Analysis written by: @vinod231 public class Solution { 2     Set < String > set = new HashSet < > (); 3     public List < String > generatePalindromes(String s) { 4         permute(s.toCharArray()  0); 5         return new ArrayList < String > (set); 6     } 7     public boolean isPalindrome(char[] s) { 8         for (int i = 0; i < s.length; i++) { 9             if (s[i] != s[s.length - 1 - i]) 10                 return false; 11         } 12         return true; 13     } 14     public void swap(char[] s  int i  int j) { 15         char temp = s[i]; 16         s[i] = s[j]; 17         s[j] = temp; 18     } 19     void permute(char[] s  int l) { 20         if (l == s.length) {                 .( String(s)); 23         } else { 24             for (int i = l; i < s.length; i++) { 25                 swap(s  l  i); 26                 permute(s  l + 1); 27                 swap(s  l  i); 28             } 29         }1 public class Solution { 2     Set < String > set = new HashSet < > (); 3     public List < String > generatePalindromes(String s) { 4         int[] map = new int[128]; 5         char[] st = new char[s.length() / 2]; 6         if (!canPermutePalindrome(s  map)) 7             return new ArrayList < > (); 8         char ch = 0; 9         int k = 0; 10         for (int i = 0; i < map.length; i++) { 11             if (map[i] % 2 == 1) 12                 ch = (char) i; 13             for (int j = 0; j < map[i] / 2; j++) { 14                 st[k++] = (char) i; 15             } 16         } 17         permute(st  0  ch); 18         return new ArrayList < String > (set); 19     } 20     public boolean (   [] ) { 21         int count = 0; 22         for (int i = 0; i < s.length(); i++) { 23             map[s.charAt(i)]++; 24             if (map[s.charAt(i)] % 2 == 0) 25                 count--; 26             else 27                 count++; 28         } 29         return count <= 1; ####LeetCode####  Basically  the idea is to perform permutation on half of the palindromic string and then form the full palindromic result.  public List<String> generatePalindromes(String s) {      int odd = 0;      String mid = """";      List<String> res = new ArrayList<>();      List<Character> list = new ArrayList<>();      Map<Character  Integer> map = new HashMap<>();        // step 1. build character count map and count odds      for (int i = 0; i < s.length(); i++) {          char c = s.charAt(i);          map.put(c  map.containsKey(c) ? map.get(c) + 1 : 1);          odd += map.get(c) % 2 != 0 ? 1 : -1;      }        // cannot form any palindromic string      if (odd > 1) return res;        // step 2. add half count of each character to list      for (Map.Entry<Character  Integer> entry : map.entrySet()) {          char key = entry.getKey();          int val = entry.getValue();            if (val % 2 != 0) mid += key;            for (int i = 0; i < val / 2; i++) list.add(key);      }        // step 3. generate all the permutations      getPerm(list  mid  new boolean[list.size()]  new StringBuilder()  res);        return res;  }    // generate all unique permutation from list  void getPerm(List<Character> list  String mid  boolean[] used  StringBuilder sb  List<String> res) {      if (sb.length() == list.size()) {          // form the palindromic string          res.add(sb.toString() + mid + sb.reverse().toString());          sb.reverse();          return;      }        for (int i = 0; i < list.size(); i++) {          // avoid duplication          if (i > 0 && list.get(i) == list.get(i - 1) && !used[i - 1]) continue;            if (!used[i]) {              used[i] = true; sb.append(list.get(i));              // recursion              getPerm(list  mid  used  sb  res);              // backtracking              used[i] = false; sb.deleteCharAt(sb.length() - 1);          }      }  } ####LeetCode#### For Java version  please refer to isssac3's answer.  If you find it helpful  please vote to let more people see this post. Besides  it would be great if you find other questions could be solved use this general solution. Please make a comment below.  39. Combination Sum  https://leetcode.com/problems/combination-sum/      def combinationSum(self  candidates  target):          def backtrack(tmp  start  end  target):              if target == 0:                  ans.append(tmp[:])              elif target > 0:                  for i in range(start  end):                      tmp.append(candidates[i])                      backtrack(tmp  i  end  target - candidates[i])                      tmp.pop()          ans = []           candidates.sort(reverse= True)          backtrack([]  0  len(candidates)  target)          return ans  40. Combination Sum II  https://leetcode.com/problems/combination-sum-ii/      def combinationSum2(self  candidates  target):          def backtrack(start  end  tmp  target):              if target == 0:                  ans.append(tmp[:])              elif target > 0:                  for i in range(start  end):                      if i > start and candidates[i] == candidates[i-1]:                          continue                      tmp.append(candidates[i])                      backtrack(i+1  end  tmp  target - candidates[i])                      tmp.pop()          ans = []          candidates.sort(reverse= True)          backtrack(0  len(candidates)  []  target)          return ans  78. Subsets  https://leetcode.com/problems/subsets/      def subsets(self  nums):          def backtrack(start  end  tmp):              ans.append(tmp[:])              for i in range(start  end):                  tmp.append(nums[i])                  backtrack(i+1  end  tmp)                  tmp.pop()          ans = []          backtrack(0  len(nums)  [])          return ans  90. Subsets II  https://leetcode.com/problems/subsets-ii/      def subsetsWithDup(self  nums):          def backtrack(start  end  tmp):              ans.append(tmp[:])              for i in range(start  end):                  if i > start and nums[i] == nums[i-1]:                      continue                  tmp.append(nums[i])                  backtrack(i+1  end  tmp)                  tmp.pop()          ans = []          nums.sort()          backtrack(0  len(nums)  [])          return ans  46. Permutations  https://leetcode.com/problems/permutations/      def permute(self  nums):          def backtrack(start  end):              if start == end:                  ans.append(nums[:])              for i in range(start  end):                  nums[start]  nums[i] = nums[i]  nums[start]                  backtrack(start+1  end)                  nums[start]  nums[i] = nums[i]  nums[start]                            ans = []          backtrack(0  len(nums))          return ans  47. Permutations II  https://leetcode.com/problems/permutations-ii/      def permuteUnique(self  nums):          def backtrack(tmp  size):              if len(tmp) == size:                  ans.append(tmp[:])              else:                  for i in range(size):                      if visited[i] or (i > 0 and nums[i-1] == nums[i] and not visited[i-1]):                          continue                      visited[i] = True                      tmp.append(nums[i])                      backtrack(tmp  size)                      tmp.pop()                      visited[i] = False          ans = []          visited = [False] * len(nums)          nums.sort()          backtrack([]  len(nums))          return ans  60. Permutation Sequence  https://leetcode.com/problems/permutation-sequence/      def getPermutation(self  n  k):          nums = [str(i) for i in range(1  n+1)]          fact = [1] * n          for i in range(1 n):              fact[i] = i*fact[i-1]          k -= 1          ans = []          for i in range(n  0  -1):              id = k / fact[i-1]              k %= fact[i-1]              ans.append(nums[id])              nums.pop(id)          return ''.join(ans)  131. Palindrome Partitioning  https://leetcode.com/problems/palindrome-partitioning/      def partition(self  s):          def backtrack(start  end  tmp):              if start == end:                  ans.append(tmp[:])              for i in range(start  end):                  cur = s[start:i+1]                  if cur == cur[::-1]:                      tmp.append(cur)                      backtrack(i+1  end  tmp)                      tmp.pop()          ans = []          backtrack(0  len(s)  [])          return ans  267. Palindrome Permutation II  https://leetcode.com/problems/palindrome-permutation-ii/  Related to this two:  31. Next Permutation: https://leetcode.com/problems/next-permutation/  266. Palindrome Permutation: https://leetcode.com/problems/palindrome-permutation/      def generatePalindromes(self  s):          kv = collections.Counter(s)          mid = [k for k  v in kv.iteritems() if v%2]          if len(mid) > 1:              return []          mid = '' if mid == [] else mid[0]          half = ''.join([k * (v/2) for k  v in kv.iteritems()])          half = [c for c in half]                    def backtrack(end  tmp):              if len(tmp) == end:                  cur = ''.join(tmp)                  ans.append(cur + mid + cur[::-1])              else:                  for i in range(end):                      if visited[i] or (i>0 and half[i] == half[i-1] and not visited[i-1]):                          continue                      visited[i] = True                      tmp.append(half[i])                      backtrack(end  tmp)                      visited[i] = False                      tmp.pop()                                ans = []          visited = [False] * len(half)          backtrack(len(half)  [])          return ans ####LeetCode#### We only need to generate the first part of palindrome string  and the remaining part will be a middle character with the reverse of first part.  private List<String> list = new ArrayList<>();    public List<String> generatePalindromes(String s) {      int numOdds = 0; // How many characters that have odd number of count      int[] map = new int[256]; // Map from character to its frequency      for (char c: s.toCharArray()) {          map[c]++;          numOdds = (map[c] & 1) == 1 ? numOdds+1 : numOdds-1;      }      if (numOdds > 1)   return list;            String mid = """";      int length = 0;      for (int i = 0; i < 256; i++) {          if (map[i] > 0) {              if ((map[i] & 1) == 1) { // Char with odd count will be in the middle                  mid = """" + (char)i;                  map[i]--;              }              map[i] /= 2; // Cut in half since we only generate half string              length += map[i]; // The length of half string          }      }      generatePalindromesHelper(map  length  """"  mid);      return list;  }  private void generatePalindromesHelper(int[] map  int length  String s  String mid) {      if (s.length() == length) {          StringBuilder reverse = new StringBuilder(s).reverse(); // Second half          list.add(s + mid + reverse);          return;      }      for (int i = 0; i < 256; i++) { // backtracking just like permutation          if (map[i] > 0) {              map[i]--;              generatePalindromesHelper(map  length  s + (char)i  mid);              map[i]++;          }       }  } ####LeetCode#### "
Ques_216,Given an array containing n distinct numbers taken from 0  1  2  ...  n  find the one that is missing from the array. Example 1: Input: [3 0 1] Output: 2 Example 2: Input: [9 6 4 2 3 5 7 0 1] Output: 8 Note: Your algorithm should run in linear runtime complexity. Could you implement it using only constant extra space complexity?,Easy,Approach #1 Sorting [Accepted] Intuition If nums were in order  it would be easy to see which number is missing. Algorithm First  we sort nums. Then  we check the two special cases that can be handled in constant time - ensuring that 0 is at the beginning and that n n is at the end. Given that those assumptions hold  the missing number must somewhere between (but not including) 0 and n n. To find it  we ensure that the number we expect to be at each index is indeed there. Because we handled the edge cases  this is simply the previous number plus 1. Thus  as soon as we find an unexpected number  we can simply return the expected number. Complexity Analysis Time complexity : \mathcal{O}(nlgn) O(nlgn) The only elements of the algorithm that have asymptotically nonconstant time complexity are the main for loop (which runs in \mathcal{O}(n) O(n) time)  and the sort invocation (which runs in \mathcal{O}(nlgn) O(nlgn) time for Python and Java). Therefore  the runtime is dominated by sort  and the entire runtime is \mathcal{O}(nlgn) O(nlgn). Space complexity : \mathcal{O}(1) O(1) (or \mathcal{O}(n) O(n)) In the sample code  we sorted nums in place  allowing us to avoid allocating additional space. If modifying nums is forbidden  we can allocate an \mathcal{O}(n) O(n) size copy and sort that instead. Approach #2 HashSet [Accepted] Intuition A brute force method for solving this problem would be to simply check for the presence of each number that we expect to be present. The naive implementation might use a linear scan of the array to check for containment  but we can use a HashSet to get constant time containment queries and overall linear runtime. Algorithm This algorithm is almost identical to the brute force approach  except we first insert each element of nums into a set  allowing us to later query for containment in \mathcal{O}(1) O(1) time. Complexity Analysis Time complexity : \mathcal{O}(n) O(n) Because the set allows for \mathcal{O}(1) O(1) containment queries  the main loop runs in \mathcal{O}(n) O(n) time. Creating num_set costs \mathcal{O}(n) O(n) time  as each set insertion runs in amortized \mathcal{O}(1) O(1) time  so the overall runtime is \mathcal{O}(n + n) = \mathcal{O}(n) O(n+n)=O(n). Space complexity : \mathcal{O}(n) O(n) nums contains n-1 nâˆ’1 distinct elements  so it costs \mathcal{O}(n) O(n) space to store a set containing all of them. Approach #3 Bit Manipulation [Accepted] Intuition We can harness the fact that XOR is its own inverse to find the missing element in linear time. Algorithm Because we know that nums contains n n numbers and that it is missing exactly one number on the range [0..n-1] [0..nâˆ’1]  we know that n n definitely replaces the missing number in nums. Therefore  if we initialize an integer to n n and XOR it with every index and value  we will be left with the missing number. Consider the following example (the values have been sorted for intuitive convenience  but need not be): Index 0 1 2 3 Value 0 1 3 4 \begin{aligned} missing &= 4 \wedge (0 \wedge 0) \wedge (1 \wedge 1) \wedge (2 \wedge 3) \wedge (3 \wedge 4) \\ &= (4 \wedge 4) \wedge (0 \wedge 0) \wedge (1 \wedge 1) \wedge (3 \wedge 3) \wedge 2 \\ &= 0 \wedge 0 \wedge 0 \wedge 0 \wedge 2 \\ &= 2 \end{aligned} missing =4âˆ§(0âˆ§0)âˆ§(1âˆ§1)âˆ§(2âˆ§3)âˆ§(3âˆ§4) =(4âˆ§4)âˆ§(0âˆ§0)âˆ§(1âˆ§1)âˆ§(3âˆ§3)âˆ§2 =0âˆ§0âˆ§0âˆ§0âˆ§2 =2 Complexity Analysis Time complexity : \mathcal{O}(n) O(n) Assuming that XOR is a constant-time operation  this algorithm does constant work on n n iterations  so the runtime is overall linear. Space complexity : \mathcal{O}(1) O(1) This algorithm allocates only constant additional space. Approach #4 Gauss' Formula [Accepted] Intuition One of the most well-known stories in mathematics is of a young Gauss  forced to find the sum of the first 100 natural numbers by a lazy teacher. Rather than add the numbers by hand  he deduced a closed-form expression for the sum  or so the story goes. You can see the formula below: \sum_{i=0}^{n}i = \frac{n(n+1)}{2} âˆ‘ i=0 n i= 2 n(n+1) Algorithm We can compute the sum of nums in linear time  and by Gauss' formula  we can compute the sum of the first n n natural numbers in constant time. Therefore  the number that is missing is simply the result of Gauss' formula minus the sum of nums  as nums consists of the first n n natural numbers minus some number. Complexity Analysis Time complexity : \mathcal{O}(n) O(n) Although Gauss' formula can be computed in \mathcal{O}(1) O(1) time  summing nums costs us \mathcal{O}(n) O(n) time  so the algorithm is overall linear. Because we have no information about which number is missing  an adversary could always design an input for which any algorithm that examines fewer than n n numbers fails. Therefore  this solution is asymptotically optimal. Space complexity : \mathcal{O}(1) O(1) This approach only pushes a few integers around  so it has constant memory usage. Analysis written by: @emptyset1 class Solution { 2     public int missingNumber(int[] nums) { 3         Arrays.sort(nums); 4 5         // Ensure that n is at the last index 6         if (nums[nums.length-1] != nums.length) { 7             return nums.length; 8         } 9         // Ensure that 0 is at the first index 10         else if (nums[0] != 0) { 11             return 0; 12         } 13 14         // If we get here  then the missing number is on the range (0  n) 15         for (int i = 1; i < nums.length; i++) { 16             int expectedNum = nums[i-1] + 1; 17             if (nums[i] != expectedNum) { 20         } 21 22         // Array was not missing any numbers 23         return -1; 24     } 25 }1 class Solution { 2     public int missingNumber(int[] nums) { 3         Set<Integer> numSet = new HashSet<Integer>(); 4         for (int num : nums) numSet.add(num); 5 6         int expectedNumCount = nums.length + 1; 7         for (int number = 0; number < expectedNumCount; number++) { 8             if (!numSet.contains(number)) { 9                 return number; 12         return -1; 13     } 14 }1 class Solution { 2     public int missingNumber(int[] nums) { 3         int missing = nums.length; 4         for (int i = 0; i < nums.length; i++) { 5             missing ^= i ^ nums[i]; 6         } 7         return missing; 8     }1 class Solution { 2     public int missingNumber(int[] nums) { 3         int expectedSum = nums.length*(nums.length + 1)/2; 4         int actualSum = 0; 5         for (int num : nums) actualSum += num; 6         return expectedSum - ; 8 } ####LeetCode####  The basic idea is to use XOR operation. We all know that a^b^b =a  which means two xor operations with the same number will eliminate the number and reveal the original number.  In this solution  I apply XOR operation to both the index and value of the array. In a complete array with no missing numbers  the index and value should be perfectly corresponding( nums[index] = index)  so in a missing array  what left finally is the missing number.  public int missingNumber(int[] nums) {        int xor = 0  i = 0;   for (i = 0; i < nums.length; i++) {    xor = xor ^ i ^ nums[i];   }     return xor ^ i;  } ####LeetCode#### 1.XOR  public int missingNumber(int[] nums) { //xor      int res = nums.length;      for(int i=0; i<nums.length; i++){          res ^= i;          res ^= nums[i];      }      return res;  }  2.SUM  public int missingNumber(int[] nums) { //sum      int len = nums.length;      int sum = (0+len)*(len+1)/2;      for(int i=0; i<len; i++)          sum-=nums[i];      return sum;  }  3.Binary Search  public int missingNumber(int[] nums) { //binary search      Arrays.sort(nums);      int left = 0  right = nums.length  mid= (left + right)/2;      while(left<right){          mid = (left + right)/2;          if(nums[mid]>mid) right = mid;          else left = mid+1;      }      return left;  }  Summary:  If the array is in order  I prefer Binary Search method. Otherwise  the XOR method is better. ####LeetCode#### class Solution {  public:      int missingNumber(vector<int>& nums) {          int result = nums.size();          int i=0;                    for(int num:nums){              result ^= num;              result ^= i;              i++;          }                    return result;      }  };  There are several similar problems in the problem list. ####LeetCode#### 
Ques_217,"There is a new alien language which uses the latin alphabet. However  the order among letters are unknown to you. You receive a list of non-empty words from the dictionary  where words are sorted lexicographically by the rules of this new language. Derive the order of letters in this language. Example 1: Input: [   ""wrt""    ""wrf""    ""er""    ""ett""    ""rftt"" ]  Output: ""wertf"" Example 2: Input: [   ""z""    ""x"" ]  Output: ""zx"" Example 3: Input: [   ""z""    ""x""    ""z"" ]   Output: """"   Explanation: The order is invalid  so return """". Note: You may assume all letters are in lowercase. You may assume that if a is a prefix of b  then a must appear before b in the given dictionary. If the order is invalid  return an empty string. There may be multiple valid order of letters  return any one of them is fine.",Hard,"Subscription Needed ####LeetCode####  public String alienOrder(String[] words) {      Map<Character  Set<Character>> map=new HashMap<Character  Set<Character>>();      Map<Character  Integer> degree=new HashMap<Character  Integer>();      String result="""";      if(words==null || words.length==0) return result;      for(String s: words){          for(char c: s.toCharArray()){              degree.put(c 0);          }      }      for(int i=0; i<words.length-1; i++){          String cur=words[i];          String next=words[i+1];          int length=Math.min(cur.length()  next.length());          for(int j=0; j<length; j++){              char c1=cur.charAt(j);              char c2=next.charAt(j);              if(c1!=c2){                  Set<Character> set=new HashSet<Character>();                  if(map.containsKey(c1)) set=map.get(c1);                  if(!set.contains(c2)){                      set.add(c2);                      map.put(c1  set);                      degree.put(c2  degree.get(c2)+1);                  }                  break;              }          }      }      Queue<Character> q=new LinkedList<Character>();      for(char c: degree.keySet()){          if(degree.get(c)==0) q.add(c);      }      while(!q.isEmpty()){          char c=q.remove();          result+=c;          if(map.containsKey(c)){              for(char c2: map.get(c)){                  degree.put(c2 degree.get(c2)-1);                  if(degree.get(c2)==0) q.add(c2);              }          }      }      if(result.length()!=degree.size()) return """";      return result;  } ####LeetCode#### The key to this problem is:  A topological ordering is possible if and only if the graph has no directed cycles  Let's build a graph and perform a DFS. The following states made things easier.  visited[i] = -1. Not even exist.  visited[i] = 0. Exist. Non-visited.  visited[i] = 1. Visiting.  visited[i] = 2. Visited.  Similarly  there is another simple BFS version.  private final int N = 26;  public String alienOrder(String[] words) {      boolean[][] adj = new boolean[N][N];      int[] visited = new int[N];      buildGraph(words  adj  visited);        StringBuilder sb = new StringBuilder();      for(int i = 0; i < N; i++) {          if(visited[i] == 0) {                 // unvisited              if(!dfs(adj  visited  sb  i)) return """";          }      }      return sb.reverse().toString();  }    public boolean dfs(boolean[][] adj  int[] visited  StringBuilder sb  int i) {      visited[i] = 1;                            // 1 = visiting      for(int j = 0; j < N; j++) {          if(adj[i][j]) {                        // connected              if(visited[j] == 1) return false;  // 1 => 1  cycle                 if(visited[j] == 0) {              // 0 = unvisited                  if(!dfs(adj  visited  sb  j)) return false;              }          }      }      visited[i] = 2;                           // 2 = visited      sb.append((char) (i + 'a'));      return true;  }    public void buildGraph(String[] words  boolean[][] adj  int[] visited) {      Arrays.fill(visited  -1);                 // -1 = not even existed      for(int i = 0; i < words.length; i++) {          for(char c : words[i].toCharArray()) visited[c - 'a'] = 0;          if(i > 0) {              String w1 = words[i - 1]  w2 = words[i];              int len = Math.min(w1.length()  w2.length());              for(int j = 0; j < len; j++) {                  char c1 = w1.charAt(j)  c2 = w2.charAt(j);                  if(c1 != c2) {                      adj[c1 - 'a'][c2 - 'a'] = true;                      break;                  }              }          }      }  } ####LeetCode#### The question says:  if the input is [ ""wrt""  ""wrf""  ""er""  ""ett""  ""rftt"" ]  The correct order is: ""wertf"".  but from ""rftt""  f should be lexicographically smaller than t? How can the result be ""wertf""? Correct me if I am wrong. ####LeetCode#### "
Ques_218,Given a non-empty binary search tree and a target value  find the value in the BST that is closest to the target. Note: Given target value is a floating point. You are guaranteed to have only one unique value in the BST that is closest to the target. Example: Input: root = [4 2 5 1 3]  target = 3.714286      4    / \   2   5  / \ 1   3  Output: 4,Easy,"Solution Approach 1: Recursive Inorder + Linear search  O(N) time Intuition The simplest approach (3 lines in Python) is to build inorder traversal and then find the closest element in a sorted array with built-in function min. This approach is simple stupid  and serves to identify the subproblems. Algorithm Build an inorder traversal array. Find the closest to target element in that array. Implementation Complexity Analysis Time complexity : \mathcal{O}(N) O(N) because to build inorder traversal and then to perform linear search takes linear time. Space complexity : \mathcal{O}(N) O(N) to keep inorder traversal.  Approach 2: Iterative Inorder  O(k) time Intuition Let's optimise Approach 1 in the case when index k of the closest element is much smaller than the tree heigh H. First  one could merge both steps by traversing the tree and searching the closest value at the same time. Second  one could stop just after identifying the closest value  there is no need to traverse the whole tree. The closest value is found if the target value is in-between of two inorder array elements nums[i] <= target < nums[i + 1]. Then the closest value is one of these elements. Algorithm Initiate stack as an empty array and predecessor value as a very small number. While root is not null: To build an inorder traversal iteratively  go left as far as you can and add all nodes on the way into stack. Pop the last element from stack root = stack.pop(). If target is in-between of pred and root.val  return the closest between these two elements. Set predecessor value to be equal to root.val and go one step right: root = root.right. We're here because during the loop one couldn't identify the closest value. That means that the closest value is the last value in the inorder traversal  i.e. current predecessor value. Return it. Implementation Complexity Analysis Time complexity : \mathcal{O}(k) O(k) in the average case and \mathcal{O}(H + k) O(H+k) in the worst case  where k is an index of closest element. It's known that average case is a balanced tree  in that case stack always contains a few elements  and hence one does 2k 2k operations to go to kth element in inorder traversal (k times to push into stack and then k times to pop out of stack). That results in \mathcal{O}(k) O(k) time complexity. The worst case is a completely unbalanced tree  then you first push H elements into stack and then pop out k elements  that results in \mathcal{O}(H + k) O(H+k) time complexity. Space complexity : up to \mathcal{O}(H) O(H) to keep the stack in the case of non-balanced tree.  Approach 3: Binary Search  O(H) time Intuition Approach 2 works fine when index k of closest element is much smaller than the tree height H. Let's now consider another limit and optimise Approach 1 in the case of relatively large k  comparable with N. Then it makes sense to use a binary search: go left if target is smaller than current root value  and go right otherwise. Choose the closest to target value at each step. Kudos for this solution go to @stefanpochmann. Implementation Complexity Analysis Time complexity : \mathcal{O}(H) O(H) since here one goes from root down to a leaf. Space complexity : \mathcal{O}(1) O(1). Analysis written by @liaison and @andvary1 class Solution { 2   public void inorder(TreeNode root  List<Integer> nums) { 3     if (root == null) return; 4     inorder(root.left  nums); 5     nums.add(root.val); 6     inorder(root.right  nums); 7   } 8 9   public int closestValue(TreeNode root  double target) { 10     List<Integer> nums = new ArrayList(); 11     inorder(root  nums); 12     return Collections.min(nums  new Comparator<Integer>() { 13       @Override 14       public int compare(Integer o1  Integer o2) { 15         return Math.abs(o1 - )  .(  )   : ; 16       } 17     }); 18   } 19 }1 class Solution { 2   public int closestValue(TreeNode root  double target) { 3     LinkedList<TreeNode> stack = new LinkedList(); 4     long pred = Long.MIN_VALUE; 5 6     while (!stack.isEmpty() || root != null) { 7       while (root != null) { 8         stack.add(root); 9         root = root.left; 10       } 11       root = stack.removeLast(); 12 13       if (pred <= target && target < root.val) 14         return Math.abs(pred - target) < Math.abs(root.val - target) ? (int)pred : root.val; 15 16       pred = root.val; 19     return (int)pred; 20   } 21 }1 class Solution { 2   public int closestValue(TreeNode root  double target) { 3     int val  closest = root.val; 4     while (root != null) { 5       val = root.val; 6       closest = Math.abs(val - target) < Math.abs(closest - target) ? val : closest; 7       root =  target < root.val ? root.left : root.right; 8     } 11 } ####LeetCode####  public int closestValue(TreeNode root  double target) {      int ret = root.val;         while(root != null){          if(Math.abs(target - root.val) < Math.abs(target - ret)){              ret = root.val;          }                root = root.val > target? root.left: root.right;      }           return ret;  } ####LeetCode#### Same recursive/iterative solution in different languages.  Recursive  Closest is either the root's value (a) or the closest in the appropriate subtree (b).  Ruby  def closest_value(root  target)    a = root.val    kid = target < a ? root.left : root.right or return a    b = closest_value(kid  target)    [b  a].min_by { |x| (x - target).abs }  end  C++  int closestValue(TreeNode* root  double target) {      int a = root->val;      auto kid = target < a ? root->left : root->right;      if (!kid) return a;      int b = closestValue(kid  target);      return abs(a - target) < abs(b - target) ? a : b;  }  Java  public int closestValue(TreeNode root  double target) {      int a = root.val;      TreeNode kid = target < a ? root.left : root.right;      if (kid == null) return a;      int b = closestValue(kid  target);      return Math.abs(a - target) < Math.abs(b - target) ? a : b;  }  Python  def closestValue(self  root  target):      a = root.val      kid = root.left if target < a else root.right      if not kid: return a      b = self.closestValue(kid  target)      return min((b  a)  key=lambda x: abs(target - x))  Alternative endings:      return (b  a)[abs(a - target) < abs(b - target)]      return a if abs(a - target) < abs(b - target) else b  Iterative  Walk the path down the tree close to the target  return the closest value on the path. Inspired by yd  I wrote these after reading ""while loop"".  Ruby  def closest_value(root  target)    path = []    while root      path << root.val      root = target < root.val ? root.left : root.right    end    path.reverse.min_by { |x| (x - target).abs }  end  The .reverse is only for handling targets much larger than 32-bit integer range  where different path values x have the same ""distance"" (x - target).abs. In such cases  the leaf value is the correct answer. If such large targets aren't asked  then it's unnecessary.  Or with O(1) space:  def closest_value(root  target)    closest = root.val    while root      closest = [root.val  closest].min_by { |x| (x - target).abs }      root = target < root.val ? root.left : root.right    end    closest  end  C++  int closestValue(TreeNode* root  double target) {      int closest = root->val;      while (root) {          if (abs(closest - target) >= abs(root->val - target))              closest = root->val;          root = target < root->val ? root->left : root->right;      }      return closest;  }  Python  def closestValue(self  root  target):      path = []      while root:          path += root.val           root = root.left if target < root.val else root.right      return min(path[::-1]  key=lambda x: abs(target - x))  The [::-1] is only for handling targets much larger than 32-bit integer range  where different path values x have the same ""distance"" (x - target).abs. In such cases  the leaf value is the correct answer. If such large targets aren't asked  then it's unnecessary.  Or with O(1) space:  def closestValue(self  root  target):      closest = root.val      while root:          closest = min((root.val  closest)  key=lambda x: abs(target - x))          root = root.left if target < root.val else root.right      return closest ####LeetCode#### class Solution(object):      def closestValue(self  root  target):          r = root.val          while root:              if abs(root.val - target) < abs(r - target):                  r = root.val              root = root.left if target < root.val else root.right          return r ####LeetCode#### "
Ques_219,Design an algorithm to encode a list of strings to a string. The encoded string is then sent over the network and is decoded back to the original list of strings. Machine 1 (sender) has the function: string encode(vector<string> strs) {   // ... your code   return encoded_string; } Machine 2 (receiver) has the function: vector<string> decode(string s) {   //... your code   return strs; } So Machine 1 does: string encoded_string = encode(strs); and Machine 2 does: vector<string> strs2 = decode(encoded_string); strs2 in Machine 2 should be the same as strs in Machine 1. Implement the encode and decode methods.   Note: The string may contain any possible characters out of 256 valid ascii characters. Your algorithm should be generalized enough to work on any possible characters. Do not use class member/global/static variables to store states. Your encode and decode algorithms should be stateless. Do not rely on any library method such as eval or serialize methods. You should implement your own encode/decode algorithm.,Medium,"Subscription Needed ####LeetCode####  public class Codec {   // Encodes a list of strings to a single string.      public String encode(List<String> strs) {          StringBuilder sb = new StringBuilder();          for(String s : strs) {              sb.append(s.length()).append('/').append(s);          }          return sb.toString();      }        // Decodes a single string to a list of strings.      public List<String> decode(String s) {          List<String> ret = new ArrayList<String>();          int i = 0;          while(i < s.length()) {              int slash = s.indexOf('/'  i);              int size = Integer.valueOf(s.substring(i  slash));              i = slash + size + 1;              ret.add(s.substring(slash + 1  i));          }          return ret;      }  } ####LeetCode#### Double any hashes inside the strings  then use standalone hashes (surrounded by spaces) to mark string endings. For example:  {""abc""  ""def""}    =>  ""abc # def # ""  {'abc'  '#def'}   =>  ""abc # ##def # ""  {'abc##'  'def'}  =>  ""abc#### # def # ""  For decoding  just do the reverse: First split at standalone hashes  then undo the doubling in each string.  public String encode(List<String> strs) {      StringBuffer out = new StringBuffer();      for (String s : strs)          out.append(s.replace(""#""  ""##"")).append("" # "");      return out.toString();  }    public List<String> decode(String s) {      List strs = new ArrayList();      String[] array = s.split("" # ""  -1);      for (int i=0; i<array.length-1; ++i)          strs.add(array[i].replace(""##""  ""#""));      return strs;  }  Or with streaming:  public String encode(List<String> strs) {      return strs.stream()                 .map(s -> s.replace(""#""  ""##"") + "" # "")                 .collect(Collectors.joining());  }    public List<String> decode(String s) {      List strs = Stream.of(s.split("" # ""  -1))                        .map(t -> t.replace(""##""  ""#""))                        .collect(Collectors.toList());      strs.remove(strs.size() - 1);      return strs;  } ####LeetCode#### class Codec:        def encode(self  strs):          return ''.join('%d:' % len(s) + s for s in strs)        def decode(self  s):          strs = []          i = 0          while i < len(s):              j = s.find(':'  i)              i = j + 1 + int(s[i:j])              strs.append(s[j+1:i])          return strs ####LeetCode#### "
Ques_220,Given a non-empty binary search tree and a target value  find k values in the BST that are closest to the target. Note: Given target value is a floating point. You may assume k is always valid  that is: k â‰¤ total nodes. You are guaranteed to have only one unique set of k values in the BST that are closest to the target. Example: Input: root = [4 2 5 1 3]  target = 3.714286  and k = 2      4    / \   2   5  / \ 1   3  Output: [4 3] Follow up: Assume that the BST is balanced  could you solve it in less than O(n) runtime (where n = total nodes)?,Hard,Subscription Needed ####LeetCode####  The idea is to compare the predecessors and successors of the closest node to the target  we can use two stacks to track the predecessors and successors  then like what we do in merge sort  we compare and pick the closest one to the target and put it to the result list.  As we know  inorder traversal gives us sorted predecessors  whereas reverse-inorder traversal gives us sorted successors.  We can use iterative inorder traversal rather than recursion  but to keep the code clean  here is the recursion version.  public List<Integer> closestKValues(TreeNode root  double target  int k) {    List<Integer> res = new ArrayList<>();      Stack<Integer> s1 = new Stack<>(); // predecessors    Stack<Integer> s2 = new Stack<>(); // successors      inorder(root  target  false  s1);    inorder(root  target  true  s2);        while (k-- > 0) {      if (s1.isEmpty())        res.add(s2.pop());      else if (s2.isEmpty())        res.add(s1.pop());      else if (Math.abs(s1.peek() - target) < Math.abs(s2.peek() - target))        res.add(s1.pop());      else        res.add(s2.pop());    }        return res;  }    // inorder traversal  void inorder(TreeNode root  double target  boolean reverse  Stack<Integer> stack) {    if (root == null) return;      inorder(reverse ? root.right : root.left  target  reverse  stack);    // early terminate  no need to traverse the whole tree    if ((reverse && root.val <= target) || (!reverse && root.val > target)) return;    // track the value of current node    stack.push(root.val);    inorder(reverse ? root.left : root.right  target  reverse  stack);  } ####LeetCode#### public class Solution {      public List<Integer> closestKValues(TreeNode root  double target  int k) {          List<Integer> ret = new LinkedList<>();          Stack<TreeNode> succ = new Stack<>();          Stack<TreeNode> pred = new Stack<>();          initializePredecessorStack(root  target  pred);          initializeSuccessorStack(root  target  succ);          if(!succ.isEmpty() && !pred.isEmpty() && succ.peek().val == pred.peek().val) {              getNextPredecessor(pred);          }          while(k-- > 0) {              if(succ.isEmpty()) {                  ret.add(getNextPredecessor(pred));              } else if(pred.isEmpty()) {                  ret.add(getNextSuccessor(succ));              } else {                  double succ_diff = Math.abs((double)succ.peek().val - target);                  double pred_diff = Math.abs((double)pred.peek().val - target);                  if(succ_diff < pred_diff) {                      ret.add(getNextSuccessor(succ));                  } else {                      ret.add(getNextPredecessor(pred));                  }              }          }          return ret;      }        private void initializeSuccessorStack(TreeNode root  double target  Stack<TreeNode> succ) {          while(root != null) {              if(root.val == target) {                  succ.push(root);                  break;              } else if(root.val > target) {                  succ.push(root);                  root = root.left;              } else {                  root = root.right;              }          }      }        private void initializePredecessorStack(TreeNode root  double target  Stack<TreeNode> pred) {          while(root != null){              if(root.val == target){                  pred.push(root);                  break;              } else if(root.val < target){                  pred.push(root);                  root = root.right;              } else{                  root = root.left;              }          }      }            private int getNextSuccessor(Stack<TreeNode> succ) {          TreeNode curr = succ.pop();          int ret = curr.val;          curr = curr.right;          while(curr != null) {              succ.push(curr);              curr = curr.left;          }          return ret;      }        private int getNextPredecessor(Stack<TreeNode> pred) {          TreeNode curr = pred.pop();          int ret = curr.val;          curr = curr.left;          while(curr != null) {              pred.push(curr);              curr = curr.right;          }          return ret;      }  } ####LeetCode#### Following the hint  I use a predecessor stack and successor stack. I do a logn traversal to initialize them until I reach the null node. Then I use the getPredecessor and getSuccessor method to pop k closest nodes and update the stacks.  Time complexity is O(klogn)  since k BST traversals are needed and each is bounded by O(logn) time. Note that it is not O(logn + k) which is the time complexity for k closest numbers in a linear array.  Space complexity is O(klogn)  since each traversal brings O(logn) new nodes to the stack.  public class Solution {      public List<Integer> closestKValues(TreeNode root  double target  int k) {          List<Integer> result = new LinkedList<Integer>();          // populate the predecessor and successor stacks           Stack<TreeNode> pred = new Stack<TreeNode>();          Stack<TreeNode> succ = new Stack<TreeNode>();          TreeNode curr = root;          while (curr != null) {              if (target <= curr.val) {                  succ.push(curr);                  curr = curr.left;              } else {                  pred.push(curr);                  curr = curr.right;              }          }          while (k > 0) {              if (pred.empty() && succ.empty()) {                  break;               } else if (pred.empty()) {                  result.add( getSuccessor(succ) );              } else if (succ.empty()) {                  result.add( getPredecessor(pred) );              } else if (Math.abs(target - pred.peek().val) < Math.abs(target - succ.peek().val)) {                  result.add( getPredecessor(pred) );                                  } else {                  result.add( getSuccessor(succ) );              }              k--;          }          return result;       }        private int getPredecessor(Stack<TreeNode> st) {          TreeNode popped = st.pop();          TreeNode p = popped.left;          while (p != null) {              st.push(p);              p = p.right;          }          return popped.val;      }        private int getSuccessor(Stack<TreeNode> st) {          TreeNode popped = st.pop();          TreeNode p = popped.right;          while (p != null) {              st.push(p);              p = p.left;          }          return popped.val;      }  } ####LeetCode#### 
Ques_221,"Convert a non-negative integer to its english words representation. Given input is guaranteed to be less than 231 - 1. Example 1: Input: 123 Output: ""One Hundred Twenty Three"" Example 2: Input: 12345 Output: ""Twelve Thousand Three Hundred Forty Five"" Example 3: Input: 1234567 Output: ""One Million Two Hundred Thirty Four Thousand Five Hundred Sixty Seven"" Example 4: Input: 1234567891 Output: ""One Billion Two Hundred Thirty Four Million Five Hundred Sixty Seven Thousand Eight Hundred Ninety One""",Hard,"Solution Approach 1: Divide and conquer Let's simplify the problem by representing it as a set of simple sub-problems. One could split the initial integer 1234567890 on the groups containing not more than three digits 1.234.567.890. That results in representation 1 Billion 234 Million 567 Thousand 890 and reduces the initial problem to how to convert 3-digit integer to English word. One could split further 234 -> 2 Hundred 34 into two sub-problems : convert 1-digit integer and convert 2-digit integer. The first one is trivial. The second one could be reduced to the first one for all 2-digit integers but the ones from 10 to 19 which should be considered separately. 1 / 11 Complexity Analysis Time complexity : \mathcal{O}(N) O(N). Intuitively the output is proportional to the number N of digits in the input. Space complexity : \mathcal{O}(1) O(1) since the output is just a string. Analysis written by @liaison and @andvary1 class Solution { 2   public String one(int num) { 3     switch(num) { 4       case 1: return ""One""; 5       case 2: return ""Two""; 6       case 3: return ""Three""; 7       case 4: return ""Four""; 8       case 5: return ""Five""; 9       case 6: return ""Six""; 10       case 7: return ""Seven""; 11       case 8: return ""Eight""; 12       case 9: return ""Nine""; 13     } 14     return """"; 15   } 16 17   public String twoLessThan20(int num) { 18     switch(num) { 19       case 10: return ""Ten""; 20       case 11: return ""Eleven""; 21       case 12: return ""Twelve""; 22       case 13: return ""Thirteen""; 23       case 14: return ""Fourteen""; 24       case 15: return ""Fifteen""; 25       case 16: return ""Sixteen""; 26       case 17: return ""Seventeen""; 27 29     } 30     return """"; 31   } 32 33   public String ten(int num) { 34     switch(num) { 35       case 2: return ""Twenty""; 36       case 3: return ""Thirty""; ####LeetCode####  private final String[] LESS_THAN_20 = {""""  ""One""  ""Two""  ""Three""  ""Four""  ""Five""  ""Six""  ""Seven""  ""Eight""  ""Nine""  ""Ten""  ""Eleven""  ""Twelve""  ""Thirteen""  ""Fourteen""  ""Fifteen""  ""Sixteen""  ""Seventeen""  ""Eighteen""  ""Nineteen""};  private final String[] TENS = {""""  ""Ten""  ""Twenty""  ""Thirty""  ""Forty""  ""Fifty""  ""Sixty""  ""Seventy""  ""Eighty""  ""Ninety""};  private final String[] THOUSANDS = {""""  ""Thousand""  ""Million""  ""Billion""};    public String numberToWords(int num) {      if (num == 0) return ""Zero"";        int i = 0;      String words = """";            while (num > 0) {          if (num % 1000 != 0)           words = helper(num % 1000) +THOUSANDS[i] + "" "" + words;       num /= 1000;       i++;      }            return words.trim();  }    private String helper(int num) {      if (num == 0)          return """";      else if (num < 20)          return LESS_THAN_20[num] + "" "";      else if (num < 100)          return TENS[num / 10] + "" "" + helper(num % 10);      else          return LESS_THAN_20[num / 100] + "" Hundred "" + helper(num % 100);  } ####LeetCode#### public class Solution {      private final String[] belowTen = new String[] {""""  ""One""  ""Two""  ""Three""  ""Four""  ""Five""  ""Six""  ""Seven""  ""Eight""  ""Nine""};      private final String[] belowTwenty = new String[] {""Ten""  ""Eleven""  ""Twelve""  ""Thirteen""  ""Fourteen""  ""Fifteen""  ""Sixteen""  ""Seventeen""  ""Eighteen""  ""Nineteen""};      private final String[] belowHundred = new String[] {""""  ""Ten""  ""Twenty""  ""Thirty""  ""Forty""  ""Fifty""  ""Sixty""  ""Seventy""  ""Eighty""  ""Ninety""};            public String numberToWords(int num) {          if (num == 0) return ""Zero"";          return helper(num);       }            private String helper(int num) {          String result = new String();          if (num < 10) result = belowTen[num];          else if (num < 20) result = belowTwenty[num -10];          else if (num < 100) result = belowHundred[num/10] + "" "" + helper(num % 10);          else if (num < 1000) result = helper(num/100) + "" Hundred "" +  helper(num % 100);          else if (num < 1000000) result = helper(num/1000) + "" Thousand "" +  helper(num % 1000);          else if (num < 1000000000) result = helper(num/1000000) + "" Million "" +  helper(num % 1000000);          else result = helper(num/1000000000) + "" Billion "" + helper(num % 1000000000);          return result.trim();      }  } ####LeetCode#### def numberToWords(self  num):      to19 = 'One Two Three Four Five Six Seven Eight Nine Ten Eleven Twelve ' \             'Thirteen Fourteen Fifteen Sixteen Seventeen Eighteen Nineteen'.split()      tens = 'Twenty Thirty Forty Fifty Sixty Seventy Eighty Ninety'.split()      def words(n):          if n < 20:              return to19[n-1:n]          if n < 100:              return [tens[n/10-2]] + words(n%10)          if n < 1000:              return [to19[n/100-1]] + ['Hundred'] + words(n%100)          for p  w in enumerate(('Thousand'  'Million'  'Billion')  1):              if n < 1000**(p+1):                  return words(n/1000**p) + [w] + words(n%1000**p)      return ' '.join(words(num)) or 'Zero' ####LeetCode#### "
Ques_222,There is a fence with n posts  each post can be painted with one of the k colors. You have to paint all the posts such that no more than two adjacent fence posts have the same color. Return the total number of ways you can paint the fence. Note: n and k are non-negative integers. Example: Input: n = 3  k = 2 Output: 6 Explanation: Take c1 as color 1  c2 as color 2. All possible ways are:              post1  post2  post3        -----      -----  -----  -----           1         c1     c1     c2     2         c1     c2     c1     3         c1     c2     c2     4         c2     c1     c1      5         c2     c1     c2    6         c2     c2     c1,Easy,"Subscription Needed ####LeetCode####  public int numWays(int n  int k) {      if(n == 0) return 0;      else if(n == 1) return k;      int diffColorCounts = k*(k-1);      int sameColorCounts = k;      for(int i=2; i<n; i++) {          int temp = diffColorCounts;          diffColorCounts = (diffColorCounts + sameColorCounts) * (k-1);          sameColorCounts = temp;      }      return diffColorCounts + sameColorCounts;  }  We divided it into two cases.  the last two posts have the same color  the number of ways to paint in this case is sameColorCounts.  the last two posts have different colors  and the number of ways in this case is diffColorCounts.  The reason why we have these two cases is that we can easily compute both of them  and that is all I do. When adding a new post  we can use the same color as the last one (if allowed) or different color. If we use different color  there're k-1 options  and the outcomes shoule belong to the diffColorCounts category. If we use same color  there's only one option  and we can only do this when the last two have different colors (which is the diffColorCounts). There we have our induction step.  Here is an example  let's say we have 3 posts and 3 colors. The first two posts we have 9 ways to do them  (1 1)  (1 2)  (1 3)  (2 1)  (2 2)  (2 3)  (3 1)  (3 2)  (3 3). Now we know that  diffColorCounts = 6;  And  sameColorCounts = 3;  Now for the third post  we can compute these two variables like this:  If we use different colors than the last one (the second one)  these ways can be added into diffColorCounts  so if the last one is 3  we can use 1 or 2  if it's 1  we can use 2 or 3  etc. Apparently there are (diffColorCounts + sameColorCounts) * (k-1) possible ways.  If we use the same color as the last one  we would trigger a violation in these three cases (1 1 1)  (2 2 2) and (3 3 3). This is because they already used the same color for the last two posts. So is there a count that rules out these kind of cases? YES  the diffColorCounts. So in cases within diffColorCounts  we can use the same color as the last one without worrying about triggering the violation. And now as we append a same-color post to them  the former diffColorCounts becomes the current sameColorCounts.  Then we can keep going until we reach the n. And finally just sum up these two variables as result.  Hope this would be clearer. ####LeetCode#### If you are painting the ith post  you have two options:  make it different color as i-1th post  make it same color as i-1 th post (if you are allowed!)  simply add these for your answer:  num_ways(i) = num_ways_diff(i) + num_ways_same(i)  Now just think of how to calculate each of those functions.  The first one is easy. If you are painting the ith post  how many ways can you paint it to make it different from the i-1 th post? k-1  num_ways_diff(i) = num_ways(i-1) * (k-1)  The second one is hard  but not so hard when you think about it.  If you are painting the ith post  how many ways can you paint it to make it the same as the i-1th post? At first  we should think the answer is 1 -- it must be whatever color the last one was.  num_ways_same(i) = num_ways(i-1) * 1  But no! This will fail in the cases where painting the last post the same results in three adjacent posts of the same color! We need to consider ONLY the cases where the last two colors were different. But we can do that!  num_ways_diff(i-1) <- all the cases where the i-1th and i-2th are different.  THESE are the cases where can just plop the same color to the end  and no longer worry about causing three in a row to be the same.  num_ways_same(i) = num_ways_diff(i-1) * 1  We sum these for our answer  like I said before:  num_ways(i) = num_ways_diff(i) + num_ways_same(i)  = num_ways(i-1) * (k-1) + num_ways_diff(i-1)  We know how to compute num_ways_diff  so we can substitute:  num_ways(i) = num_ways(i-1) * (k-1) + num_ways(i-2) * (k-1)  We can even simplify a little more:  num_ways(i) = (num_ways(i-1) + num_ways(i-2)) * (k-1)  As a note  trying to intuitively understand that last line is impossible. If you think you understand it intuitively  you are fooling yourself. Only the original equation makes intuitive sense.  Once you have this  the code is trivial (but overall  this problem is not an easy problem  despite the leetcode tag!):  class Solution {      public int numWays(int n  int k) {          // if there are no posts  there are no ways to paint them          if (n == 0) return 0;                    // if there is only one post  there are k ways to paint it          if (n == 1) return k;                    // if there are only two posts  you can't make a triplet  so you           // are free to paint however you want.          // first post  k options. second post  k options          if (n == 2) return k*k;                    int table[] = new int[n+1];          table[0] = 0;          table[1] = k;          table[2] = k*k;          for (int i = 3; i <= n; i++) {              // the recursive formula that we derived              table[i] = (table[i-1] + table[i-2]) * (k-1);          }          return table[n];      }  } ####LeetCode#### If n == 1  there would be k-ways to paint.  if n == 2  there would be two situations:  2.1 You paint same color with the previous post: k*1 ways to paint  named it as same  2.2 You paint differently with the previous post: k*(k-1) ways to paint this way  named it as dif  So  you can think  if n >= 3  you can always maintain these two situations   You either paint the same color with the previous one  or differently.  Since there is a rule: ""no more than two adjacent fence posts have the same color.""  We can further analyze:  from 2.1  since previous two are in the same color  next one you could only paint differently  and it would form one part of ""paint differently"" case in the n == 3 level  and the number of ways to paint this way would equal to same*(k-1).  from 2.2  since previous two are not the same  you can either paint the same color this time (dif*1) ways to do so  or stick to paint differently (dif*(k-1)) times.  Here you can conclude  when seeing back from the next level  ways to paint the same  or variable same would equal to dif*1 = dif  and ways to paint differently  variable dif  would equal to same*(k-1)+dif*(k-1) = (same + dif)*(k-1)  So we could write the following codes:      if n == 0:          return 0      if n == 1:          return k      same  dif = k  k*(k-1)      for i in range(3  n+1):          same  dif = dif  (same+dif)*(k-1)      return same + dif ####LeetCode#### "
Ques_223,You are a product manager and currently leading a team to develop a new product. Unfortunately  the latest version of your product fails the quality check. Since each version is developed based on the previous version  all the versions after a bad version are also bad. Suppose you have n versions [1  2  ...  n] and you want to find out the first bad one  which causes all the following ones to be bad. You are given an API bool isBadVersion(version) which will return whether version is bad. Implement a function to find the first bad version. You should minimize the number of calls to the API. Example: Given n = 5  and version = 4 is the first bad version.  call isBadVersion(3) -> false call isBadVersion(5) -> true call isBadVersion(4) -> true  Then 4 is the first bad version. ,Easy,"Summary This is a very simple problem. There is a subtle trap that you may fall into if you are not careful. Other than that  it is a direct application of a very famous algorithm. Solution Approach #1 (Linear Scan) [Time Limit Exceeded] The straight forward way is to brute force it by doing a linear scan. Complexity analysis Time complexity : O(n) O(n). Assume that isBadVersion(version) isBadVersion(version) takes constant time to check if a version is bad. It takes at most n - 1 nâˆ’1 checks  therefore the overall time complexity is O(n) O(n). Space complexity : O(1) O(1). Approach #2 (Binary Search) [Accepted] It is not difficult to see that this could be solved using a classic algorithm - Binary search. Let us see how the search space could be halved each time below. Scenario #1: isBadVersion(mid) => false   1 2 3 4 5 6 7 8 9  G G G G G G B B B       G = Good  B = Bad  |       |       | left    mid    right Let us look at the first scenario above where isBadVersion(mid) \Rightarrow false isBadVersion(mid)â‡’false. We know that all versions preceding and including mid mid are all good. So we set left = mid + 1 left=mid+1 to indicate that the new search space is the interval [mid + 1  right] [mid+1 right] (inclusive). Scenario #2: isBadVersion(mid) => true   1 2 3 4 5 6 7 8 9  G G G B B B B B B       G = Good  B = Bad  |       |       | left    mid    right The only scenario left is where isBadVersion(mid) \Rightarrow true isBadVersion(mid)â‡’true. This tells us that mid mid may or may not be the first bad version  but we can tell for sure that all versions after mid mid can be discarded. Therefore we set right = mid right=mid as the new search space of interval [left mid] [left mid] (inclusive). In our case  we indicate left left and right right as the boundary of our search space (both inclusive). This is why we initialize left = 1 left=1 and right = n right=n. How about the terminating condition? We could guess that left left and right right eventually both meet and it must be the first bad version  but how could you tell for sure? The formal way is to prove by induction  which you can read up yourself if you are interested. Here is a helpful tip to quickly prove the correctness of your binary search algorithm during an interview. We just need to test an input of size 2. Check if it reduces the search space to a single element (which must be the answer) for both of the scenarios above. If not  your algorithm will never terminate. If you are setting mid = \frac{left + right}{2} mid= 2 left+right   you have to be very careful. Unless you are using a language that does not overflow such as Python  left + right left+right could overflow. One way to fix this is to use left + \frac{right - left}{2} left+ 2 rightâˆ’left instead. If you fall into this subtle overflow bug  you are not alone. Even Jon Bentley's own implementation of binary search had this overflow bug and remained undetected for over twenty years. Complexity analysis Time complexity : O(\log n) O(logn). The search space is halved each time  so the time complexity is O(\log n) O(logn). Space complexity : O(1) O(1).1 public int firstBadVersion(int n) { 2     for (int i = 1; i < n; i++) { 3         if (isBadVersion(i)) { 4             return i; 5         } 6     } 7     return n; 8 }1 public int firstBadVersion(int n) { 2     int left = 1; 3     int right = n; 4     while (left < right) { 5         int mid = left + (right - left) / 2; 6         if (isBadVersion(mid)) { 7             right = mid; 8         } else { 9             left = mid + 1; 10         } 11     } 12     return left; 13 } ####LeetCode####  The binary search code:  public int firstBadVersion(int n) {      int start = 1  end = n;      while (start < end) {          int mid = start + (end-start) / 2;          if (!isBadVersion(mid)) start = mid + 1;          else end = mid;                  }              return start;  } ####LeetCode#### Before this problem  I have always use    mid = (start+end)) / 2;  To get the middle value  but this can caused OVERFLOW !  when start and end are all about INT_MAX   then (start+end) of course will be overflow !  To avoid the problem we can use    mid =  start+(end-start)/2;  Here is the AC implementation  // Forward declaration of isBadVersion API.  bool isBadVersion(int version);  class Solution {  public:      int firstBadVersion(int n) {          int start=0  end=n;          cout<<end-start<<end;          while(end-start>1){              int mid=start+(end-start)/2;              /** mid = (start+end)) / 2; **/              if(isBadVersion(mid))  end=mid;              else  start=mid;          }          return end;      }  }; ####LeetCode#### Is there any difference between "" ( low + high ) / 2 "" and "" low + ( high - low ) / 2 ""?  When I use the first one  it told me ""time limit exceed"" but if I use the second one  it worked! ####LeetCode#### "
Ques_224,"Given a string that contains only digits 0-9 and a target value  return all possibilities to add binary operators (not unary) +  -  or * between the digits so they evaluate to the target value. Example 1: Input: num = ""123""  target = 6 Output: [""1+2+3""  ""1*2*3""]  Example 2: Input: num = ""232""  target = 8 Output: [""2*3+2""  ""2+3*2""] Example 3: Input: num = ""105""  target = 5 Output: [""1*0+5"" ""10-5""] Example 4: Input: num = ""00""  target = 0 Output: [""0+0""  ""0-0""  ""0*0""] Example 5: Input: num = ""3456237490""  target = 9191 Output: []",Hard,"Subscription Needed ####LeetCode####  This problem has a lot of edge cases to be considered:  overflow: we use a long type once it is larger than Integer.MAX_VALUE or minimum  we get over it.  0 sequence: because we can't have numbers with multiple digits started with zero  we have to deal with it too.  a little trick is that we should save the value that is to be multiplied in the next recursion.  public class Solution {      public List<String> addOperators(String num  int target) {          List<String> rst = new ArrayList<String>();          if(num == null || num.length() == 0) return rst;          helper(rst  """"  num  target  0  0  0);          return rst;      }      public void helper(List<String> rst  String path  String num  int target  int pos  long eval  long multed){          if(pos == num.length()){              if(target == eval)                  rst.add(path);              return;          }          for(int i = pos; i < num.length(); i++){              if(i != pos && num.charAt(pos) == '0') break;              long cur = Long.parseLong(num.substring(pos  i + 1));              if(pos == 0){                  helper(rst  path + cur  num  target  i + 1  cur  cur);              }              else{                  helper(rst  path + ""+"" + cur  num  target  i + 1  eval + cur   cur);                                    helper(rst  path + ""-"" + cur  num  target  i + 1  eval -cur  -cur);                                    helper(rst  path + ""*"" + cur  num  target  i + 1  eval - multed + multed * cur  multed * cur );              }          }      }  } ####LeetCode#### class Solution {  private:      // cur: {string} expression generated so far.      // pos: {int}    current visiting position of num.      // cv:  {long}   cumulative value so far.      // pv:  {long}   previous operand value.      // op:  {char}   previous operator used.      void dfs(std::vector<string>& res  const string& num  const int target  string cur  int pos  const long cv  const long pv  const char op) {          if (pos == num.size() && cv == target) {              res.push_back(cur);          } else {              for (int i=pos+1; i<=num.size(); i++) {                  string t = num.substr(pos  i-pos);                  long now = stol(t);                  if (to_string(now).size() != t.size()) continue;                  dfs(res  num  target  cur+'+'+t  i  cv+now  now  '+');                  dfs(res  num  target  cur+'-'+t  i  cv-now  now  '-');                  dfs(res  num  target  cur+'*'+t  i  (op == '-') ? cv+pv - pv*now : ((op == '+') ? cv-pv + pv*now : pv*now)  pv*now  op);              }          }      }    public:      vector<string> addOperators(string num  int target) {          vector<string> res;          if (num.empty()) return res;          for (int i=1; i<=num.size(); i++) {              string s = num.substr(0  i);              long cur = stol(s);              if (to_string(cur).size() != s.size()) continue;              dfs(res  num  target  s  i  cur  cur  '#');         // no operator defined.          }            return res;      }  }; ####LeetCode#### dfs() parameters:  num: remaining num string  temp: temporally string with operators added  cur: current result of ""temp"" string  last: last multiply-level number in ""temp"". if next operator is ""multiply""  ""cur"" and ""last"" will be updated  res: result to return  def addOperators(self  num  target):      res  self.target = []  target      for i in range(1 len(num)+1):          if i == 1 or (i > 1 and num[0] != ""0""): # prevent ""00*"" as a number              self.dfs(num[i:]  num[:i]  int(num[:i])  int(num[:i])  res) # this step put first number in the string      return res    def dfs(self  num  temp  cur  last  res):      if not num:          if cur == self.target:              res.append(temp)          return      for i in range(1  len(num)+1):          val = num[:i]          if i == 1 or (i > 1 and num[0] != ""0""): # prevent ""00*"" as a number              self.dfs(num[i:]  temp + ""+"" + val  cur+int(val)  int(val)  res)              self.dfs(num[i:]  temp + ""-"" + val  cur-int(val)  -int(val)  res)              self.dfs(num[i:]  temp + ""*"" + val  cur-last+last*int(val)  last*int(val)  res) ####LeetCode#### "
Ques_225,Given an array nums  write a function to move all 0's to the end of it while maintaining the relative order of the non-zero elements. Example: Input: [0 1 0 3 12] Output: [1 3 12 0 0] Note: You must do this in-place without making a copy of the array. Minimize the total number of operations.,Easy,"Solution This question comes under a broad category of ""Array Transformation"". This category is the meat of tech interviews. Mostly because arrays are such a simple and easy to use data structure. Traversal or representation doesn't require any boilerplate code and most of your code will look like the Pseudocode itself. The 2 requirements of the question are: Move all the 0's to the end of array. All the non-zero elements must retain their original order. It's good to realize here that both the requirements are mutually exclusive  i.e.  you can solve the individual sub-problems and then combine them for the final solution. Approach #1 (Space Sub-Optimal) [Accepted] C++ void moveZeroes(vector<int>& nums) {     int n = nums.size();      // Count the zeroes     int numZeroes = 0;     for (int i = 0; i < n; i++) {         numZeroes += (nums[i] == 0);     }      // Make all the non-zero elements retain their original order.     vector<int> ans;     for (int i = 0; i < n; i++) {         if (nums[i] != 0) {             ans.push_back(nums[i]);         }     }      // Move all zeroes to the end     while (numZeroes--) {         ans.push_back(0);     }      // Combine the result     for (int i = 0; i < n; i++) {         nums[i] = ans[i];     } } Complexity Analysis Space Complexity : O(n) O(n). Since we are creating the ""ans"" array to store results. Time Complexity: O(n) O(n). However  the total number of operations are sub-optimal. We can achieve the same result in less number of operations. If asked in an interview  the above solution would be a good start. You can explain the interviewer(not code) the above and build your base for the next Optimal Solution. Approach #2 (Space Optimal  Operation Sub-Optimal) [Accepted] This approach works the same way as above  i.e.   first fulfills one requirement and then another. The catch? It does it in a clever way. The above problem can also be stated in alternate way  "" Bring all the non 0 elements to the front of array keeping their relative order same"". This is a 2 pointer approach. The fast pointer which is denoted by variable ""cur"" does the job of processing new elements. If the newly found element is not a 0  we record it just after the last found non-0 element. The position of last found non-0 element is denoted by the slow pointer ""lastNonZeroFoundAt"" variable. As we keep finding new non-0 elements  we just overwrite them at the ""lastNonZeroFoundAt + 1"" 'th index. This overwrite will not result in any loss of data because we already processed what was there(if it were non-0 it already is now written at it's corresponding index or if it were 0 it will be handled later in time). After the ""cur"" index reaches the end of array  we now know that all the non-0 elements have been moved to beginning of array in their original order. Now comes the time to fulfil other requirement  ""Move all 0's to the end"". We now simply need to fill all the indexes after the ""lastNonZeroFoundAt"" index with 0. C++ void moveZeroes(vector<int>& nums) {     int lastNonZeroFoundAt = 0;     // If the current element is not 0  then we need to     // append it just in front of last non 0 element we found.      for (int i = 0; i < nums.size(); i++) {         if (nums[i] != 0) {             nums[lastNonZeroFoundAt++] = nums[i];         }     }   // After we have finished processing new elements    // all the non-zero elements are already at beginning of array.   // We just need to fill remaining array with 0's.     for (int i = lastNonZeroFoundAt; i < nums.size(); i++) {         nums[i] = 0;     } } Complexity Analysis Space Complexity : O(1) O(1). Only constant space is used. Time Complexity: O(n) O(n). However  the total number of operations are still sub-optimal. The total operations (array writes) that code does is n n (Total number of elements). Approach #3 (Optimal) [Accepted] The total number of operations of the previous approach is sub-optimal. For example  the array which has all (except last) leading zeroes: [0  0  0  ...  0  1].How many write operations to the array? For the previous approach  it writes 0's n-1 nâˆ’1 times  which is not necessary. We could have instead written just once. How? ..... By only fixing the non-0 element i.e.  1. The optimal approach is again a subtle extension of above solution. A simple realization is if the current element is non-0  its' correct position can at best be it's current position or a position earlier. If it's the latter one  the current position will be eventually occupied by a non-0  or a 0  which lies at a index greater than 'cur' index. We fill the current position by 0 right away so that unlike the previous solution  we don't need to come back here in next iteration. In other words  the code will maintain the following invariant: All elements before the slow pointer (lastNonZeroFoundAt) are non-zeroes. All elements between the current and slow pointer are zeroes. Therefore  when we encounter a non-zero element  we need to swap elements pointed by current and slow pointer  then advance both pointers. If it's zero element  we just advance current pointer. With this invariant in-place  it's easy to see that the algorithm will work. C++ void moveZeroes(vector<int>& nums) {     for (int lastNonZeroFoundAt = 0  cur = 0; cur < nums.size(); cur++) {         if (nums[cur] != 0) {             swap(nums[lastNonZeroFoundAt++]  nums[cur]);         }     } } Complexity Analysis Space Complexity : O(1) O(1). Only constant space is used. Time Complexity: O(n) O(n). However  the total number of operations are optimal. The total operations (array writes) that code does is Number of non-0 elements.This gives us a much better best-case (when most of the elements are 0) complexity than last solution. However  the worst-case (when all elements are non-0) complexity for both the algorithms is same. Analysis written by: @spandan.pathak ####LeetCode####  // Shift non-zero values as far forward as possible  // Fill remaining space with zeros    public void moveZeroes(int[] nums) {      if (nums == null || nums.length == 0) return;                int insertPos = 0;      for (int num: nums) {          if (num != 0) nums[insertPos++] = num;      }                while (insertPos < nums.length) {          nums[insertPos++] = 0;      }  } ####LeetCode#### class Solution {  public:      void moveZeroes(vector<int>& nums) {          int j = 0;          // move all the nonzero elements advance          for (int i = 0; i < nums.size(); i++) {              if (nums[i] != 0) {                  nums[j++] = nums[i];              }          }          for (;j < nums.size(); j++) {              nums[j] = 0;          }      }  }; ####LeetCode#### public class Solution {  public void moveZeroes(int[] nums) {        int j = 0;      for(int i = 0; i < nums.length; i++) {          if(nums[i] != 0) {              int temp = nums[j];              nums[j] = nums[i];              nums[i] = temp;              j++;          }      }  }  } ####LeetCode#### "
Ques_226,"Given a pattern and a string str  find if str follows the same pattern. Here follow means a full match  such that there is a bijection between a letter in pattern and a non-empty substring in str. Example 1: Input: pattern = ""abab""  str = ""redblueredblue"" Output: true Example 2: Input: pattern = pattern = ""aaaa""  str = ""asdasdasdasd"" Output: true Example 3: Input: pattern = ""aabb""  str = ""xyzabcxzyabc"" Output: false Notes: You may assume both pattern and str contains only lowercase letters.",Hard,"Subscription Needed ####LeetCode####  We can solve this problem using backtracking  we just have to keep trying to use a character in the pattern to match different length of substrings in the input string  keep trying till we go through the input string and the pattern.  For example  input string is ""redblueredblue""  and the pattern is ""abab""  first let's use 'a' to match ""r""  'b' to match ""e""  then we find that 'a' does not match ""d""  so we do backtracking  use 'b' to match ""ed""  so on and so forth ...  When we do the recursion  if the pattern character exists in the hash map already  we just have to see if we can use it to match the same length of the string. For example  let's say we have the following map:  'a': ""red""  'b': ""blue""  now when we see 'a' again  we know that it should match ""red""  the length is 3  then let's see if str[i ... i+3] matches 'a'  where i is the current index of the input string. Thanks to StefanPochmann's suggestion  in Java we can elegantly use str.startsWith(s  i) to do the check.  Also thanks to i-tikhonov's suggestion  we can use a hash set to avoid duplicate matches  if character a matches string ""red""  then character b cannot be used to match ""red"". In my opinion though  we can say apple (pattern 'a') is ""fruit""  orange (pattern 'o') is ""fruit""  so they can match the same string  anyhow  I guess it really depends on how the problem states.  The following code should pass OJ now  if we don't need to worry about the duplicate matches  just remove the code that associates with the hash set.  public class Solution {        public boolean wordPatternMatch(String pattern  String str) {      Map<Character  String> map = new HashMap<>();      Set<String> set = new HashSet<>();            return isMatch(str  0  pattern  0  map  set);    }        boolean isMatch(String str  int i  String pat  int j  Map<Character  String> map  Set<String> set) {      // base case      if (i == str.length() && j == pat.length()) return true;      if (i == str.length() || j == pat.length()) return false;            // get current pattern character      char c = pat.charAt(j);            // if the pattern character exists      if (map.containsKey(c)) {        String s = map.get(c);                // then check if we can use it to match str[i...i+s.length()]        if (!str.startsWith(s  i)) {          return false;        }                // if it can match  great  continue to match the rest        return isMatch(str  i + s.length()  pat  j + 1  map  set);      }            // pattern character does not exist in the map      for (int k = i; k < str.length(); k++) {        String p = str.substring(i  k + 1);          if (set.contains(p)) {          continue;        }          // create or update it        map.put(c  p);        set.add(p);                // continue to match the rest        if (isMatch(str  k + 1  pat  j + 1  map  set)) {          return true;        }          // backtracking        map.remove(c);        set.remove(p);      }            // we've tried our best but still no luck      return false;    }      } ####LeetCode#### The idea might not be so different  but I tried to optimize the solution as much as I could. In concrete:  Instead of using HashMap  I use a String array to store the character --> String mapping  Instead of trying all combinations  I only try necessary/possible ones.  I'd like to explain the second point a little bit more: Suppose we are successful in mapping the first i characters in pattern  and we are now at the j location of str. If i+1's character in pattern is not already mapped  then we would want to try all possible substrings in str  that is  the substring could be of length 1 (j's character)  or length 2 (j and j+1)  etc. Normally we would try up to the end of str  but this is really not necessary  because we have to spare enough space future characters in pattern. If this is not clear enough  let's take the following as an example:  pattern = ""abca""  str = ""xxxyyzzxxx""  Suppose we have successfully matched a to xxx and b to yy  and now we are at the third character of pattern  i.e.  character c. We have not mapped c to anything  so we could try any of the following:  z  zz  zzx  zzxx  zzxxx  But do we really need to try them all? The answer is NO. Because we have already mapped a and b  and under this constraint  we have to save enough space for the fourth character of pattern  i.e.  a. In other words  we can at most try z and zz  otherwise we are doomed to return false when we reach the fourth character a. This is what endPoint is about in the code.  Code in Java:  public boolean wordPatternMatch(String pattern  String str) {      String[] map = new String[26]; // mapping of characters 'a' - 'z'      HashSet<String> set = new HashSet<>(); // mapped result of 'a' - 'z'      return wordPatternMatch(pattern  str  map  set  0  str.length()-1  0  pattern.length()-1);  }  private boolean wordPatternMatch(String pattern  String str  String[] map  HashSet<String> set  int start  int end  int startP  int endP) {   if(startP==endP+1 && start==end+1) return true; // both pattern and str are exhausted   if((startP>endP && start<=end) || (startP<endP && start>end)) return false; // either of pattern or str is exhausted     char ch = pattern.charAt(startP);   String matched = map[ch-'a'];   if(matched!=null) { // ch is already mapped  then continue    int count = matched.length();    return start+count<=end+1 && matched.equals(str.substring(start  start+count)) // substring equals previously mapped string      && wordPatternMatch(pattern  str  map  set  start+matched.length()  end  startP+1  endP); // moving forward   }   else {       int endPoint = end;       for(int i=endP; i>startP; i--) {           endPoint -= map[pattern.charAt(i)-'a']==null ? 1 : map[pattern.charAt(i)-'a'].length();       }    for(int i=start; i<=endPoint; i++) { // try every possible mapping  from 1 character to the end     matched = str.substring(start  i+1);     if(set.contains(matched)) continue; // different pattern cannot map to same substring       map[ch-'a'] = matched; // move forward  add corresponding mapping and set content     set.add(matched);       if(wordPatternMatch(pattern  str  map  set  i+1  end  startP+1  endP)) return true;       else { // backtracking  remove corresponding mapping and set content      map[ch-'a'] = null;      set.remove(matched);     }    }   }   return false; // exhausted  }  If you are interested in my other posts  please feel free to check my Github page here: https://github.com/F-L-A-G/Algorithms-in-Java ####LeetCode#### public class Solution {  Map<Character String> map =new HashMap();  Set<String> set =new HashSet();  public boolean wordPatternMatch(String pattern  String str) {      if(pattern.isEmpty()) return str.isEmpty();      if(map.containsKey(pattern.charAt(0))){          String value= map.get(pattern.charAt(0));          if(str.length()<value.length() || !str.substring(0 value.length()).equals(value)) return false;          if(wordPatternMatch(pattern.substring(1) str.substring(value.length()))) return true;      }else{          for(int i=1;i<=str.length();i++){              if(set.contains(str.substring(0 i))) continue;              map.put(pattern.charAt(0) str.substring(0 i));              set.add(str.substring(0 i));              if(wordPatternMatch(pattern.substring(1) str.substring(i))) return true;              set.remove(str.substring(0 i));              map.remove(pattern.charAt(0));          }      }      return false;  }  } ####LeetCode#### "
Ques_227,Median is the middle value in an ordered integer list. If the size of the list is even  there is no middle value. So the median is the mean of the two middle value. For example  [2 3 4]  the median is 3 [2 3]  the median is (2 + 3) / 2 = 2.5 Design a data structure that supports the following two operations: void addNum(int num) - Add a integer number from the data stream to the data structure. double findMedian() - Return the median of all elements so far.   Example: addNum(1) addNum(2) findMedian() -> 1.5 addNum(3)  findMedian() -> 2   Follow up: If all integer numbers from the stream are between 0 and 100  how would you optimize it? If 99% of all integer numbers from the stream are between 0 and 100  how would you optimize it?,Hard,"Solution Approach 1: Simple Sorting Intuition Do what the question says. Algorithm Store the numbers in a resize-able container. Every time you need to output the median  sort the container and output the median. Complexity Analysis Time complexity: O(n\log n) + O(1) \simeq O(n\log n) O(nlogn)+O(1)â‰ƒO(nlogn). Adding a number takes amortized O(1) O(1) time for a container with an efficient resizing scheme. Finding the median is primarily dependent on the sorting that takes place. This takes O(n\log n) O(nlogn) time for a standard comparative sort. Space complexity: O(n) O(n) linear space to hold input in a container. No extra space other than that needed (since sorting can usually be done in-place). Approach 2: Insertion Sort Intuition Keeping our input container always sorted (i.e. maintaining the sorted nature of the container as an invariant). Algorithm Which algorithm allows a number to be added to a sorted list of numbers and yet keeps the entire list sorted? Well  for one  insertion sort! We assume that the current list is already sorted. When a new number comes  we have to add it to the list while maintaining the sorted nature of the list. This is achieved easily by finding the correct place to insert the incoming number  using a binary search (remember  the list is always sorted). Once the position is found  we need to shift all higher elements by one space to make room for the incoming number. This method would work well when the amount of insertion queries is lesser or about the same as the amount of median finding queries. Complexity Analysis Time complexity: O(n) + O(\log n) \approx O(n) O(n)+O(logn)â‰ˆO(n). Binary Search takes O(\log n) O(logn) time to find correct insertion position. Insertion can take up to O(n) O(n) time since elements have to be shifted inside the container to make room for the new element. Pop quiz: Can we use a linear search instead of a binary search to find insertion position  without incurring any significant runtime penalty? Space complexity: O(n) O(n) linear space to hold input in a container. Approach 3: Two Heaps Intuition The above two approaches gave us some valuable insights on how to tackle this problem. Concretely  one can infer two things: If we could maintain direct access to median elements at all times  then finding the median would take a constant amount of time. If we could find a reasonably fast way of adding numbers to our containers  additional penalties incurred could be lessened. But perhaps the most important insight  which is not readily observable  is the fact that we only need a consistent way to access the median elements. Keeping the entire input sorted is not a requirement. Well  if only there were a data structure which could handle our needs. As it turns out there are two data structures for the job: Heaps (or Priority Queues [1]) Self-balancing Binary Search Trees (we'll talk more about them in Approach 4) Heaps are a natural ingredient for this dish! Adding elements to them take logarithmic order of time. They also give direct access to the maximal/minimal elements in a group. If we could maintain two heaps in the following way: A max-heap to store the smaller half of the input numbers A min-heap to store the larger half of the input numbers This gives access to median values in the input: they comprise the top of the heaps! Wait  what? How? If the following conditions are met: Both the heaps are balanced (or nearly balanced) The max-heap contains all the smaller numbers while the min-heap contains all the larger numbers then we can say that: All the numbers in the max-heap are smaller or equal to the top element of the max-heap (let's call it x x) All the numbers in the min-heap are larger or equal to the top element of the min-heap (let's call it y y) Then x x and/or y y are smaller than (or equal to) almost half of the elements and larger than (or equal to) the other half. That is the definition of median elements. This leads us to a huge point of pain in this approach: balancing the two heaps! Algorithm Two priority queues: A max-heap lo to store the smaller half of the numbers A min-heap hi to store the larger half of the numbers The max-heap lo is allowed to store  at worst  one more element more than the min-heap hi. Hence if we have processed k k elements: If k = 2*n + 1 \quad (\forall \  n \in \mathbb{Z}) k=2âˆ—n+1(âˆ€nâˆˆZ)  then lo is allowed to hold n+1 n+1 elements  while hi can hold n n elements. If k = 2*n \quad (\forall \  n \in \mathbb{Z}) k=2âˆ—n(âˆ€nâˆˆZ)  then both heaps are balanced and hold n n elements each. This gives us the nice property that when the heaps are perfectly balanced  the median can be derived from the tops of both heaps. Otherwise  the top of the max-heap lo holds the legitimate median. Adding a number num: Add num to max-heap lo. Since lo received a new element  we must do a balancing step for hi. So remove the largest element from lo and offer it to hi. The min-heap hi might end holding more elements than the max-heap lo  after the previous operation. We fix that by removing the smallest element from hi and offering it to lo. The above step ensures that we do not disturb the nice little size property we just mentioned. A little example will clear this up! Say we take input from the stream [41  35  62  5  97  108]. The run-though of the algorithm looks like this: Adding number 41 MaxHeap lo: [41]           // MaxHeap stores the largest value at the top (index 0) MinHeap hi: []             // MinHeap stores the smallest value at the top (index 0) Median is 41 ======================= Adding number 35 MaxHeap lo: [35] MinHeap hi: [41] Median is 38 ======================= Adding number 62 MaxHeap lo: [41  35] MinHeap hi: [62] Median is 41 ======================= Adding number 4 MaxHeap lo: [35  4] MinHeap hi: [41  62] Median is 38 ======================= Adding number 97 MaxHeap lo: [41  35  4] MinHeap hi: [62  97] Median is 41 ======================= Adding number 108 MaxHeap lo: [41  35  4] MinHeap hi: [62  97  108] Median is 51.5 Complexity Analysis Time complexity: O(5 \cdot \log n) + O(1) \approx O(\log n) O(5â‹…logn)+O(1)â‰ˆO(logn). At worst  there are three heap insertions and two heap deletions from the top. Each of these takes about O(\log n) O(logn) time. Finding the mean takes constant O(1) O(1) time since the tops of heaps are directly accessible. Space complexity: O(n) O(n) linear space to hold input in containers. Approach 4: Multiset and Two Pointers Intuition Self-balancing Binary Search Trees (like an AVL Tree) have some very interesting properties. They maintain the tree's height to a logarithmic bound. Thus inserting a new element has reasonably good time performance. The median always winds up in the root of the tree and/or one of its children. Solving this problem using the same approach as Approach 3 but using a Self-balancing BST seems like a good choice. Except the fact that implementing such a tree is not trivial and prone to errors. Why reinvent the wheel? Most languages implement a multiset class which emulates such behavior. The only problem remains keeping track of the median elements. That is easily solved with pointers! [2] We maintain two pointers: one for the lower median element and the other for the higher median element. When the total number of elements is odd  both the pointers point to the same median element (since there is only one median in this case). When the number of elements is even  the pointers point to two consecutive elements  whose mean is the representative median of the input. Algorithm Two iterators/pointers lo_median and hi_median  which iterate over the data multiset. While adding a number num  three cases arise: The container is currently empty. Hence we simply insert num and set both pointers to point to this element. The container currently holds an odd number of elements. This means that both the pointers currently point to the same element. If num is not equal to the current median element  then num goes on either side of it. Whichever side it goes  the size of that part increases and hence the corresponding pointer is updated. For example  if num is less than the median element  the size of the lesser half of input increases by 1 1 on inserting num. Thus it makes sense to decrement lo_median. If num is equal to the current median element  then the action taken is dependent on how num is inserted into data. NOTE: In our given C++ code example  std::multiset::insert inserts an element after all elements of equal value. Hence we increment hi_median. The container currently holds an even number of elements. This means that the pointers currently point to consecutive elements. If num is a number between both median elements  then num becomes the new median. Both pointers must point to it. Otherwise  num increases the size of either the lesser or higher half of the input. We update the pointers accordingly. It is important to remember that both the pointers must point to the same element now. Finding the median is easy! It is simply the mean of the elements pointed to by the two pointers lo_median and hi_median. A much shorter (but harder to understand)  one pointer version [3] of this solution is given below: Complexity Analysis Time complexity: O(\log n) + O(1) \approx O(\log n) O(logn)+O(1)â‰ˆO(logn). Inserting a number takes O(\log n) O(logn) time for a standard multiset scheme. [4] Finding the mean takes constant O(1) O(1) time since the median elements are directly accessible from the two pointers. Space complexity: O(n) O(n) linear space to hold input in container. Further Thoughts There are so many ways around this problem  that frankly  it is scary. Here are a few more that I came across: Buckets! If the numbers in the stream are statistically distributed  then it is easier to keep track of buckets where the median would land  than the entire array. Once you know the correct bucket  simply sort it find the median. If the bucket size is significantly smaller than the size of input processed  this results in huge time saving. @mitbbs8080 has an interesting implementation here. Reservoir Sampling. Following along the lines of using buckets: if the stream is statistically distributed  you can rely on Reservoir Sampling. Basically  if you could maintain just one good bucket (or reservoir) which could hold a representative sample of the entire stream  you could estimate the median of the entire stream from just this one bucket. This means good time and memory performance. Reservoir Sampling lets you do just that. Determining a ""good"" size for your reservoir? Now  that's a whole other challenge. A good explanation for this can be found in this StackOverflow answer. Segment Trees are a great data structure if you need to do a lot of insertions or a lot of read queries over a limited range of input values. They allow us to do all such operations fast and in roughly the same amount of time  always. The only problem is that they are far from trivial to implement. Take a look at my introductory article on Segment Trees if you are interested. Order Statistic Trees are data structures which seem to be tailor-made for this problem. They have all the nice features of a BST  but also let you find the k^{th} k th order element stored in the tree. They are a pain to implement and no standard interview would require you to code these up. But they are fun to use if they are already implemented in the language of your choice. [5] Analysis written by @babhishek21. Priority Queues queue out elements based on a predefined priority. They are an abstract concept and can  as such  be implemented in many different ways. Heaps are an efficient way to implement Priority Queues. â†©ï¸Ž Shout-out to @pharese for this approach. â†©ï¸Ž Inspired from this post by @StefanPochmann. â†©ï¸Ž Hinting can reduce that to amortized constant O(1) O(1) time. â†©ï¸Ž GNU libstdc++ users are in luck! Take a look at this StackOverflow answer. â†©ï¸Ž1 class MedianFinder { 2     vector<double> store; 3 4 public: 5     // Adds a number into the data structure. 6     void addNum(int num) 7     { 8         store.push_back(num); 9     } 10 11     // Returns the median of current data stream 12     double findMedian() 13     { 14         sort(store.begin()  store.end()); 15 16         int n = store.size(); 17         return (n & 1 ? store[n / ] : ([    ]  [  ])  ); 18     } 19 };1 class MedianFinder { 2     vector<int> store; // resize-able container 3 4 public: 5     // Adds a number into the data structure. 6     void addNum(int num) 7     { 8         if (store.empty()) 9             store.push_back(num); 10         else 11             store.insert(lower_bound(store.begin()  store.end()  num)  num);     // binary search and insertion combined 12     } 13 14     // Returns the median of current data stream 17         int n = store.size(); 18         return n & 1 ? store[n / 2] : (store[n / 2 - 1] + store[n / 2]) * 0.5; 19     } 20 };1 class MedianFinder { 2     priority_queue<int> lo;                              // max heap 3     priority_queue<int  vector<int>  greater<int>> hi;   // min heap 4 5 public: 6     // Adds a number into the data structure. 7     void addNum(int num) 8     { 9         lo.push(num);                                    // Add to max heap 10 11         hi.push(lo.top());                               // balancing step 12         lo.pop(); 13 14         if (lo.size() < hi.size()) {                     // maintain size property 15             lo.push(hi.top()); 16             hi.pop(); 19 20     // Returns the median of current data stream 21     double findMedian() 22     { 23         return lo.size() > hi.size() ? (double) lo.top() : (lo.top() + hi.top()) * 0.5; 24     } 25 };1 class MedianFinder { 2     multiset<int> data; 3     multiset<int>::iterator lo_median  hi_median; 4 5 public: 6     MedianFinder() 7         : lo_median(data.end()) 8           hi_median(data.end()) 9     { 10     } 11 12     void addNum(int num) 13     { 14         const size_t n = data.size();   // store previous size 15 16         data.insert(num);               // insert into multiset 17 18         if (!n) { 19             // no elements before  one element now 20             lo_median = hi_median = data.begin(); 21         } 22 24 25             if (num < *lo_median)       // num < lo 26                 lo_median--; 27             else                        // num >= hi 28                 hi_median++;            // insertion at end of equal range 29         } 30         else { 31             // even size before (i.e. hi = lo + 1)  odd size now (i.e. lo == hi) 321 class MedianFinder { 2     multiset<int> data; 3     multiset<int>::iterator mid; 4 5 public: 6     MedianFinder() 7         : mid(data.end()) 8     { 9     } 10 11     void addNum(int num) 12     { 13         const int n = data.size(); 14         data.insert(num); 15 16         if (!n)                                 // first element inserted 17             mid = data.begin(); 18         else if (num < *mid)                    // median is decreased 19             mid = (n & 1 ? mid : prev(mid)); 20         else                                    // median is increased 21             mid = (n & 1 ? next(mid) : mid); 22 24     double findMedian() 25     { 26         const int n = data.size(); 27         return (*mid + *next(mid  n % 2 - 1)) * 0.5; 28     } 29 }; ####LeetCode####  I keep two heaps (or priority queues):  Max-heap small has the smaller half of the numbers.  Min-heap large has the larger half of the numbers.  This gives me direct access to the one or two middle values (they're the tops of the heaps)  so getting the median takes O(1) time. And adding a number takes O(log n) time.  Supporting both min- and max-heap is more or less cumbersome  depending on the language  so I simply negate the numbers in the heap in which I want the reverse of the default order. To prevent this from causing a bug with -231 (which negated is itself  when using 32-bit ints)  I use integer types larger than 32 bits.  Using larger integer types also prevents an overflow error when taking the mean of the two middle numbers. I think almost all solutions posted previously have that bug.  Update: These are pretty short already  but by now I wrote even shorter ones.  Java  class MedianFinder {        private Queue<Long> small = new PriorityQueue()                           large = new PriorityQueue();        public void addNum(int num) {          large.add((long) num);          small.add(-large.poll());          if (large.size() < small.size())              large.add(-small.poll());      }        public double findMedian() {          return large.size() > small.size()                 ? large.peek()                 : (large.peek() - small.peek()) / 2.0;      }  };  Props to larrywang2014's solution for making me aware that I can use Queue in the declaration instead of PriorityQueue (that's all I got from him  though (just saying because I just saw he changed his previously longer addNum and it's now equivalent to mine)).  C++  class MedianFinder {      priority_queue<long> small  large;  public:        void addNum(int num) {          small.push(num);          large.push(-small.top());          small.pop();          if (small.size() < large.size()) {              small.push(-large.top());              large.pop();          }      }        double findMedian() {          return small.size() > large.size()                 ? small.top()                 : (small.top() - large.top()) / 2.0;      }  };  Big thanks to jianchao.li.fighter for telling me that C++'s priority_queue is a max-queue (see comments below).  Python  from heapq import *    class MedianFinder:        def __init__(self):          self.heaps = []  []        def addNum(self  num):          small  large = self.heaps          heappush(small  -heappushpop(large  num))          if len(large) < len(small):              heappush(large  -heappop(small))        def findMedian(self):          small  large = self.heaps          if len(large) > len(small):              return float(large[0])          return (large[0] - small[0]) / 2.0 ####LeetCode#### The invariant of the algorithm is two heaps  small and large  each represent half of the current list. The length of smaller half is kept to be n / 2 at all time and the length of the larger half is either n / 2 or n / 2 + 1 depend on n's parity.  This way we only need to peek the two heaps' top number to calculate median.  Any time before we add a new number  there are two scenarios  (total n numbers  k = n / 2):  (1) length of (small  large) == (k  k)  (2) length of (small  large) == (k  k + 1)  After adding the number  total (n + 1) numbers  they will become:  (1) length of (small  large) == (k  k + 1)  (2) length of (small  large) == (k + 1  k + 1)  Here we take the first scenario for example  we know the large will gain one more item and small will remain the same size  but we cannot just push the item into large. What we should do is we push the new number into small and pop the maximum item from small then push it into large (all the pop and push here are heappop and heappush). By doing this kind of operations for the two scenarios we can keep our invariant.  Therefore to add a number  we have 3 O(log n) heap operations. Luckily the heapq provided us a function ""heappushpop"" which saves some time by combine two into one. The document says:  Push item on the heap  then pop and return the smallest item from the heap. The combined action runs more efficiently than heappush() followed by a separate call to heappop().  Alltogether  the add operation is O(logn)  The findMedian operation is O(1).  Note that the heapq in python is a min heap  thus we need to invert the values in the smaller half to mimic a ""max heap"".  A further observation is that the two scenarios take turns when adding numbers  thus it is possible to combine the two into one. For this please see stefan's post  Java  private PriorityQueue<Integer> small = new PriorityQueue<>(Collections.reverseOrder());  private PriorityQueue<Integer> large = new PriorityQueue<>();  private boolean even = true;    public double findMedian() {      if (even)          return (small.peek() + large.peek()) / 2.0;      else          return small.peek();  }    public void addNum(int num) {      if (even) {          large.offer(num);          small.offer(large.poll());      } else {          small.offer(num);          large.offer(small.poll());      }      even = !even;  }  Python  from heapq import *      class MedianFinder:      def __init__(self):          self.small = []  # the smaller half of the list  max heap (invert min-heap)          self.large = []  # the larger half of the list  min heap        def addNum(self  num):          if len(self.small) == len(self.large):              heappush(self.large  -heappushpop(self.small  -num))          else:              heappush(self.small  -heappushpop(self.large  num))        def findMedian(self):          if len(self.small) == len(self.large):              return float(self.large[0] - self.small[0]) / 2.0          else:              return float(self.large[0])    # 18 / 18 test cases passed.  # Status: Accepted  # Runtime: 388 ms ####LeetCode#### Not sure why it is marked as hard  i think this is one of the easiest questions on leetcode.  class MedianFinder {      // max queue is always larger or equal to min queue      PriorityQueue<Integer> min = new PriorityQueue();      PriorityQueue<Integer> max = new PriorityQueue(1000  Collections.reverseOrder());      // Adds a number into the data structure.      public void addNum(int num) {          max.offer(num);          min.offer(max.poll());          if (max.size() < min.size()){              max.offer(min.poll());          }      }        // Returns the median of current data stream      public double findMedian() {          if (max.size() == min.size()) return (max.peek() + min.peek()) /  2.0;          else return max.peek();      }  }; ####LeetCode#### "
Ques_228,A group of two or more people wants to meet and minimize the total travel distance. You are given a 2D grid of values 0 or 1  where each 1 marks the home of someone in the group. The distance is calculated using Manhattan Distance  where distance(p1  p2) = |p2.x - p1.x| + |p2.y - p1.y|. Example: Input:   1 - 0 - 0 - 0 - 1 |   |   |   |   | 0 - 0 - 0 - 0 - 0 |   |   |   |   | 0 - 0 - 1 - 0 - 0  Output: 6   Explanation: Given three people living at (0 0)  (0 4)  and (2 2):              The point (0 2) is an ideal meeting point  as the total travel distance               of 2+2+2=6 is minimal. So return 6.,Hard,Solution Approach #1 (Breadth-first Search) [Time Limit Exceeded] A brute force approach is to evaluate all possible meeting points in the grid. We could apply breadth-first search originating from each of the point. While inserting a point into the queue  we need to record the distance of that point from the meeting point. Also  we need an extra visited table to record which point had already been visited to avoid being inserted into the queue again. public int minTotalDistance(int[][] grid) {     int minDistance = Integer.MAX_VALUE;     for (int row = 0; row < grid.length; row++) {         for (int col = 0; col < grid[0].length; col++) {             int distance = search(grid  row  col);             minDistance = Math.min(distance  minDistance);         }     }     return minDistance; }  private int search(int[][] grid  int row  int col) {     Queue<Point> q = new LinkedList<>();     int m = grid.length;     int n = grid[0].length;     boolean[][] visited = new boolean[m][n];     q.add(new Point(row  col  0));     int totalDistance = 0;     while (!q.isEmpty()) {         Point point = q.poll();         int r = point.row;         int c = point.col;         int d = point.distance;         if (r < 0 || c < 0 || r >= m || c >= n || visited[r][c]) {             continue;         }         if (grid[r][c] == 1) {             totalDistance += d;         }         visited[r][c] = true;         q.add(new Point(r + 1  c  d + 1));         q.add(new Point(r - 1  c  d + 1));         q.add(new Point(r  c + 1  d + 1));         q.add(new Point(r  c - 1  d + 1));     }     return totalDistance; }  public class Point {     int row;     int col;     int distance;     public Point(int row  int col  int distance) {         this.row = row;         this.col = col;         this.distance = distance;     } } Complexity analysis Time complexity : O(m^2n^2) O(m 2 n 2 ). For each point in the m \times n mÃ—n size grid  the breadth-first search takes at most m \times n mÃ—n steps to reach all points. Therefore the time complexity is O(m^2n^2) O(m 2 n 2 ). Space complexity : O(mn) O(mn). The visited table consists of m \times n mÃ—n elements map to each point in the grid. We insert at most m \times n mÃ—n points into the queue. Approach #2 (Manhattan Distance Formula) [Time Limit Exceeded] You may notice that breadth-first search is unnecessary. You can just calculate the Manhattan distance using the formula: distance(p1  p2) = \left | p2.x - p1.x \right | + \left | p2.y - p1.y \right | distance(p1 p2)=âˆ£p2.xâˆ’p1.xâˆ£+âˆ£p2.yâˆ’p1.yâˆ£ public int minTotalDistance(int[][] grid) {     List<Point> points = getAllPoints(grid);     int minDistance = Integer.MAX_VALUE;     for (int row = 0; row < grid.length; row++) {         for (int col = 0; col < grid[0].length; col++) {             int distance = calculateDistance(points  row  col);             minDistance = Math.min(distance  minDistance);         }     }     return minDistance; }  private int calculateDistance(List<Point> points  int row  int col) {     int distance = 0;     for (Point point : points) {         distance += Math.abs(point.row - row) + Math.abs(point.col - col);     }     return distance; }  private List<Point> getAllPoints(int[][] grid) {     List<Point> points = new ArrayList<>();     for (int row = 0; row < grid.length; row++) {         for (int col = 0; col < grid[0].length; col++) {             if (grid[row][col] == 1) {                 points.add(new Point(row  col));             }         }     }     return points; }  public class Point {     int row;     int col;     public Point(int row  int col) {         this.row = row;         this.col = col;     } } Complexity analysis Time complexity : O(m^2n^2) O(m 2 n 2 ). Assume that k k is the total number of houses. For each point in the m \times n mÃ—n size grid  we calculate the manhattan distance in O(k) O(k). Therefore the time complexity is O(mnk) O(mnk). But do note that there could be up to m \times n mÃ—n houses  making the worst case time complexity to be O(m^2n^2) O(m 2 n 2 ). Space complexity : O(mn) O(mn). Approach #3 (Sorting) [Accepted] Finding the best meeting point in a 2D grid seems difficult. Let us take a step back and solve the 1D case which is much simpler. Notice that the Manhattan distance is the sum of two independent variables. Therefore  once we solve the 1D case  we can solve the 2D case as two independent 1D problems. Let us look at some 1D examples below: Case #1: 1-0-0-0-1  Case #2: 0-1-0-1-0 We know the best meeting point must locate somewhere between the left-most and right-most point. For the above two cases  we would select the center point at x = 2 x=2 as the best meeting point. How about choosing the mean of all points as the meeting point? Consider this case: Case #3: 1-0-0-0-0-0-0-1-1 Using the mean gives us \bar{x} = \frac{0 + 7 + 8}{3} = 5 x Ë‰ = 3 0+7+8 =5 as the meeting point. The total distance is 10 10. But the best meeting point should be at x = 7 x=7 and the total distance is 8 8. You may argue that the mean is close to the optimal point. But imagine a larger case with many 1's congregating on the right side and just a single 1 on the left-most side. Using the mean as the meeting point would be far from optimal. Besides mean  what is a better way to represent the distribution of points? Would median be a better representation? Indeed. In fact  the median must be the optimal meeting point. Case #4: 1-1-0-0-1 To see why this is so  let us look at case #4 above and choose the median x = 1 x=1 as our initial meeting point. Assume that the total distance traveled is d. Note that we have equal number of points distributed to its left and to its right. Now let us move one step to its right where x = 2 x=2 and notice how the distance changes accordingly. Since there are two points to the left of x = 2 x=2  we add 2 * (+1) 2âˆ—(+1) to d. And d is offset by â€“1 since there is one point to the right. This means the distance had overall increased by 1. Therefore  it is clear that: As long as there is equal number of points to the left and right of the meeting point  the total distance is minimized. Case #5: 1-1-0-0-1-1 One may think that the optimal meeting point must fall on one of the 1's. This is true for cases with odd number of 1's  but not necessarily true when there are even number of 1's  just like case #5 does. You can choose any of the x = 1 x=1 to x = 4 x=4 points and the total distance is minimized. Why? The implementation is direct. First we collect both the row and column coordinates  sort them and select their middle elements. Then we calculate the total distance as the sum of two independent 1D problems. public int minTotalDistance(int[][] grid) {     List<Integer> rows = new ArrayList<>();     List<Integer> cols = new ArrayList<>();     for (int row = 0; row < grid.length; row++) {         for (int col = 0; col < grid[0].length; col++) {             if (grid[row][col] == 1) {                 rows.add(row);                 cols.add(col);             }         }     }     int row = rows.get(rows.size() / 2);     Collections.sort(cols);     int col = cols.get(cols.size() / 2);     return minDistance1D(rows  row) + minDistance1D(cols  col); }  private int minDistance1D(List<Integer> points  int origin) {     int distance = 0;     for (int point : points) {         distance += Math.abs(point - origin);     }     return distance; } Note that in the code above we do not need to sort rows  why? Complexity analysis Time complexity : O(mn \log mn) O(mnlogmn). Since there could be at most m \times n mÃ—n points  therefore the time complexity is O(mn \log mn) O(mnlogmn) due to sorting. Space complexity : O(mn) O(mn). Approach #4 (Collect Coordinates in Sorted Order) [Accepted] We could use the Selection algorithm to select the median in O(mn) O(mn) time  but there is an easier way. Notice that we can collect both the row and column coordinates in sorted order. public int minTotalDistance(int[][] grid) {     List<Integer> rows = collectRows(grid);     List<Integer> cols = collectCols(grid);     int row = rows.get(rows.size() / 2);     int col = cols.get(cols.size() / 2);     return minDistance1D(rows  row) + minDistance1D(cols  col); }  private int minDistance1D(List<Integer> points  int origin) {     int distance = 0;     for (int point : points) {         distance += Math.abs(point - origin);     }     return distance; }  private List<Integer> collectRows(int[][] grid) {     List<Integer> rows = new ArrayList<>();     for (int row = 0; row < grid.length; row++) {         for (int col = 0; col < grid[0].length; col++) {             if (grid[row][col] == 1) {                 rows.add(row);             }         }     }     return rows; }  private List<Integer> collectCols(int[][] grid) {     List<Integer> cols = new ArrayList<>();     for (int col = 0; col < grid[0].length; col++) {         for (int row = 0; row < grid.length; row++) {             if (grid[row][col] == 1) {                 cols.add(col);             }         }     }     return cols; }  You can calculate the distance without knowing the median using a two pointer approach. This neat approach is inspired by [@larrywang2014's solution](https://leetcode.com/discuss/65336/14ms-java-solution). public int minTotalDistance(int[][] grid) {     List<Integer> rows = collectRows(grid);     List<Integer> cols = collectCols(grid);     return minDistance1D(rows) + minDistance1D(cols); }  private int minDistance1D(List<Integer> points) {     int distance = 0;     int i = 0;     int j = points.size() - 1;     while (i < j) {         distance += points.get(j) - points.get(i);         i++;         j--;     }     return distance; } Complexity analysis Time complexity : O(mn) O(mn). Space complexity : O(mn) O(mn). ####LeetCode####  public int minTotalDistance(int[][] grid) {      int m = grid.length;      int n = grid[0].length;            List<Integer> I = new ArrayList<>(m);      List<Integer> J = new ArrayList<>(n);            for(int i = 0; i < m; i++){          for(int j = 0; j < n; j++){              if(grid[i][j] == 1){                  I.add(i);                  J.add(j);              }          }      }            return getMin(I) + getMin(J);  }    private int getMin(List<Integer> list){      int ret = 0;            Collections.sort(list);            int i = 0;      int j = list.size() - 1;      while(i < j){          ret += list.get(j--) - list.get(i++);      }            return ret;  } ####LeetCode#### When I first saw this question  intuitively I know shortest meeting point should be found in two separate dimension  however  even if on 1-D  how could I find the shortest meeting point? Then I clicked discuss and found out everybody's solution was using median to get shortest meeting point? WHY?  Actually  there is a famous conclusion in statistics that the median minimizes the sum of absolute deviations. ####LeetCode#### Before solving the 2D problem we first consider a 1D case. The solution is quite simple. Just find the median of all the x coordinates and calculate the distance to the median.  Alternatively  we can also use two pointers to solve the 1D problem. left and right are how many people one left/right side of coordinates i/j. If we have more people on the left we let j decrease otherwise increase i. The time complexity is O(n) and space is O(1).  To be more clear  a better view is we can think i and j as two meet points. All the people in [0  i] go to meet at i and all the people in [j  n - 1] meet at j. We let left = sum(vec[:i+1])  right = sum(vec[j:])  which are the number of people at each meet point  and d is the total distance for the left people meet at i and right people meet at j.  Our job is to let i == j with minimum d.  If we increase i by 1  the distance will increase by left since there are 'left' people at i and they just move 1 step. The same applies to j  when decrease j by 1  the distance will increase by right. To make sure the total distance d is minimized we certainly want to move the point with less people. And to make sure we do not skip any possible meet point options we need to move one by one.  For the 2D cases we first need to sum the columns and rows into two vectors and call the 1D algorithm.  The answer is the sum of the two. The time is then O(mn) and extra space is O(m+n)  Moreover  the solution is still O(mn) with the follow up:  What if there are people sharing same home?  In other words the number in the grid can be more than 1.  Java  public class Solution {      public int minTotalDistance(int[][] grid) {          int m = grid.length  n = grid[0].length;          int[] row_sum = new int[n]  col_sum = new int[m];            for (int i = 0; i < m; ++i)              for (int j = 0; j < n; ++j) {                  row_sum[j] += grid[i][j];                  col_sum[i] += grid[i][j];              }            return minDistance1D(row_sum) + minDistance1D(col_sum);      }        public int minDistance1D(int[] vector) {          int i = -1  j = vector.length;          int d = 0  left = 0  right = 0;            while (i != j) {              if (left < right) {                  d += left;                  left += vector[++i];              }              else {                  d += right;                  right += vector[--j];              }          }          return d;      }    }  // Runtime: 2ms  Python  def minTotalDistance(self  grid):      row_sum = map(sum  grid)      col_sum = map(sum  zip(*grid)) # syntax sugar learned from stefan :-)        def minTotalDistance1D(vec):          i  j = -1  len(vec)          d = left = right = 0          while i != j:              if left < right:                  d += left                  i += 1                  left += vec[i]              else:                  d += right                  j -= 1                  right += vec[j]          return d        return minTotalDistance1D(row_sum) + minTotalDistance1D(col_sum)      # 57 / 57 test cases passed.  # Status: Accepted  # Runtime: 40 ms ####LeetCode#### 
Ques_229,"Serialization is the process of converting a data structure or object into a sequence of bits so that it can be stored in a file or memory buffer  or transmitted across a network connection link to be reconstructed later in the same or another computer environment. Design an algorithm to serialize and deserialize a binary tree. There is no restriction on how your serialization/deserialization algorithm should work. You just need to ensure that a binary tree can be serialized to a string and this string can be deserialized to the original tree structure. Example:  You may serialize the following tree:      1    / \   2   3      / \     4   5  as ""[1 2 3 null null 4 5]"" Clarification: The above format is the same as how LeetCode serializes a binary tree. You do not necessarily need to follow this format  so please be creative and come up with different approaches yourself. Note: Do not use class member/global/static variables to store states. Your serialize and deserialize algorithms should be stateless.",Hard,"Solution Approach 1: Depth First Search (DFS) Intuition The serialization of a Binary Search Tree is essentially to encode its values and more importantly its structure. One can traverse the tree to accomplish the above task. And it is well know that we have two general strategies to do so: Breadth First Search (BFS) We scan through the tree level by level  following the order of height  from top to bottom. The nodes on higher level would be visited before the ones with lower levels. Depth First Search (DFS) In this strategy  we adopt the depth as the priority  so that one would start from a root and reach all the way down to certain leaf  and then back to root to reach another branch. The DFS strategy can further be distinguished as preorder  inorder  and postorder depending on the relative order among the root node  left node and right node. In this task  however  the DFS strategy is more adapted for our needs  since the linkage among the adjacent nodes is naturally encoded in the order  which is rather helpful for the later task of deserialization. Therefore  in this solution  we demonstrate an example with the preorder DFS strategy. One can check out more tutorial about Binary Search Tree on the LeetCode Explore. Algorithm First of all  here is the definition of the TreeNode which we would use in the following implementation. The preorder DFS traverse follows recursively the order of root -> left subtree -> right subtree. As an example  let's serialize the following tree. Note that serialization contains information about the node values as well as the information about the tree structure. We start from the root  node 1  the serialization string is 1 . Then we jump to its left subtree with the root node 2  and the serialization string becomes 1 2 . Now starting from node 2  we visit its left node 3 (1 2 3 None None ) and right node 4 (1 2 3 None None 4 None None) sequentially. Note that None None  appears for each leaf to mark the absence of left and right child node  this is how we save the tree structure during the serialization. And finally  we get back to the root node 1 and visit its right subtree which happens to be a leaf node 5. Finally  the serialization string is done as 1 2 3 None None 4 None None 5 None None . Now let's deserialize the serialization string constructed above 1 2 3 None None 4 None None 5 None None . It goes along the string  initiate the node value and then calls itself to construct its left and right child nodes. Complexity Analysis Time complexity : in both serialization and deserialization functions  we visit each node exactly once  thus the time complexity is O(N) O(N)  where N N is the number of nodes  i.e. the size of tree. Space complexity : in both serialization and deserialization functions  we keep the entire tree  either at the beginning or at the end  therefore  the space complexity is O(N) O(N). The solutions with BFS or other DFS strategies normally will have the same time and space complexity. Further Space Optimization In the above solution  we store the node value and the references to None child nodes  which means N \cdot V + 2N Nâ‹…V+2N complexity  where V V is the size of value. That is called natural serialization  and has was implemented above. The N \cdot V Nâ‹…V component here is the encoding of values  can't be optimized further  but there is a way to reduce 2N 2N part which is the encoding of the tree structure. The number of unique binary tree structures that can be constructed using n nodes is C(n) C(n)  where C(n) C(n) is the nth Catalan number. Please refer to this article for more information. There are C(n) C(n) possible structural configurations of a binary tree with n nodes  so the largest index value that we might need to store is C(n) - 1 C(n)âˆ’1. That means storing the index value could require up to 1 bit for n \leq 2 nâ‰¤2  or \lceil log_2(C(n) - 1) \rceil âŒˆlog 2 (C(n)âˆ’1)âŒ‰ bits for n > 2 n>2. In this way one could reduce the encoding of the tree structure by \log N logN. More precisely  the Catalan numbers grow as C(n) \sim \frac{4^n}{n^{3/2}\sqrt{\pi}} C(n)âˆ¼ n 3/2 Ï€ 4 n and hence the theoretical minimum of storage for the tree structure that could be achieved is log(C(n)) \sim 2n - \frac{3}{2}\log(n) - \frac{1}{2}\log(\pi) log(C(n))âˆ¼2nâˆ’ 2 3 log(n)âˆ’ 2 1 log(Ï€) Analysis written by @liaison and @andvary1 /* Definition for a binary tree node. */ 2 public class TreeNode { 3   int val; 4   TreeNode left; 5   TreeNode right; 6 7   TreeNode(int x) { 8     val = x; 9   } 10 }1 // Serialization 2 public class Codec { 3   public String rserialize(TreeNode root  String str) { 4     // Recursive serialization. 5     if (root == null) { 6       str += ""null ""; 7     } else { 8       str += str.valueOf(root.val) + "" ""; 9       str = rserialize(root.left  str); 10       str = rserialize(root.right  str); 11     } 12     return str; 13   } 14 15   // Encodes a tree to a single string. 16   public String serialize(TreeNode ) { 18   } 19 };1 public class Codec { 2   public TreeNode rdeserialize(List<String> l) { 3     // Recursive deserialization. 4     if (l.get(0).equals(""null"")) { 5       l.remove(0); 6       return null; 7     } 8 9     TreeNode root = new TreeNode(Integer.valueOf(l.get(0))); 10     l.remove(0); 11     root.left = rdeserialize(l); 12     root.right = rdeserialize(l); 13 14     return root; 15   } 16 17   // Decodes your encoded data to tree. 18   public TreeNode deserialize(String data) { 19     String[] data_array = .();         LinkedList<String>(Arrays.asList(data_array)); 21     return rdeserialize(data_list); 22   } 23 }; ####LeetCode####  The idea is simple: print the tree in pre-order traversal and use ""X"" to denote null node and split node with "" "". We can use a StringBuilder for building the string on the fly. For deserializing  we use a Queue to store the pre-order traversal and since we have ""X"" as null node  we know exactly how to where to end building subtress.  public class Codec {      private static final String spliter = "" "";      private static final String NN = ""X"";        // Encodes a tree to a single string.      public String serialize(TreeNode root) {          StringBuilder sb = new StringBuilder();          buildString(root  sb);          return sb.toString();      }        private void buildString(TreeNode node  StringBuilder sb) {          if (node == null) {              sb.append(NN).append(spliter);          } else {              sb.append(node.val).append(spliter);              buildString(node.left  sb);              buildString(node.right sb);          }      }      // Decodes your encoded data to tree.      public TreeNode deserialize(String data) {          Deque<String> nodes = new LinkedList<>();          nodes.addAll(Arrays.asList(data.split(spliter)));          return buildTree(nodes);      }            private TreeNode buildTree(Deque<String> nodes) {          String val = nodes.remove();          if (val.equals(NN)) return null;          else {              TreeNode node = new TreeNode(Integer.valueOf(val));              node.left = buildTree(nodes);              node.right = buildTree(nodes);              return node;          }      }  } ####LeetCode#### Python  class Codec:        def serialize(self  root):          def doit(node):              if node:                  vals.append(str(node.val))                  doit(node.left)                  doit(node.right)              else:                  vals.append('#')          vals = []          doit(root)          return ' '.join(vals)        def deserialize(self  data):          def doit():              val = next(vals)              if val == '#':                  return None              node = TreeNode(int(val))              node.left = doit()              node.right = doit()              return node          vals = iter(data.split())          return doit()  C++  class Codec {  public:        string serialize(TreeNode* root) {          ostringstream out;          serialize(root  out);          return out.str();      }        TreeNode* deserialize(string data) {          istringstream in(data);          return deserialize(in);      }    private:        void serialize(TreeNode* root  ostringstream& out) {          if (root) {              out << root->val << ' ';              serialize(root->left  out);              serialize(root->right  out);          } else {              out << ""# "";          }      }        TreeNode* deserialize(istringstream& in) {          string val;          in >> val;          if (val == ""#"")              return nullptr;          TreeNode* root = new TreeNode(stoi(val));          root->left = deserialize(in);          root->right = deserialize(in);          return root;      }  }; ####LeetCode#### Here I use typical BFS method to handle a binary tree. I use string n to represent null values. The string of the binary tree in the example will be ""1 2 3 n n 4 5 n n n n "".  When deserialize the string  I assign left and right child for each not-null node  and add the not-null children to the queue  waiting to be handled later.  public class Codec {      public String serialize(TreeNode root) {          if (root == null) return """";          Queue<TreeNode> q = new LinkedList<>();          StringBuilder res = new StringBuilder();          q.add(root);          while (!q.isEmpty()) {              TreeNode node = q.poll();              if (node == null) {                  res.append(""n "");                  continue;              }              res.append(node.val + "" "");              q.add(node.left);              q.add(node.right);          }          return res.toString();      }        public TreeNode deserialize(String data) {          if (data == """") return null;          Queue<TreeNode> q = new LinkedList<>();          String[] values = data.split("" "");          TreeNode root = new TreeNode(Integer.parseInt(values[0]));          q.add(root);          for (int i = 1; i < values.length; i++) {              TreeNode parent = q.poll();              if (!values[i].equals(""n"")) {                  TreeNode left = new TreeNode(Integer.parseInt(values[i]));                  parent.left = left;                  q.add(left);              }              if (!values[++i].equals(""n"")) {                  TreeNode right = new TreeNode(Integer.parseInt(values[i]));                  parent.right = right;                  q.add(right);              }          }          return root;      }  } ####LeetCode#### "
Ques_230,What is Data Purging ?,Easy,Data purging is an important step in maintaining appropriate data in the database .Basically deleting unnecessary data or rows which have NULL values from the database is nothing but data purging. So if there is a need to load fresh data into the database table we need to utilized database purging activity. This will clear all unnecessary data in the database and helps in maintaining clean and meaningful data.Data purging is a process where junk data that exists in the database gets cleared out. 
Ques_231,What is time series algorithm in data mining?Easy,Easy,This algorithm is a perfect fit for type of data where the values changes continuous based on the time. For example : Age. If the algorithm is skilled and tuned to predict the data set  then it will be successfully keep a track of the continuous data and predict the right data. This algorithm generates a specific model which is capable of predicting the future trends of the the data based on the real original data sets. In between the process new data can also be added in part of trend analysis. 
Ques_232,What is sequence clustering algorithm?,Medium,As the name itself states that the data is collected at different points which occurs at  sequence of events. The different data sets are analyzed based on the sequence of data sets that occur based on the events. The data sets are analyzed and then best possible data input will be determined for clustering. 
Ques_233,What Are Cubes? ,Medium,A data cube stores data in a summarized version which helps in a faster analysis of data. The data is stored in such a way that it allows reporting easily.E.g. using a data cube A user may want to analyze weekly  monthly performance of an employee. Here  month and week could be considered as the dimensions of the cube.
Ques_234,What is a Sting,Hard,Statistical Information Grid is called as STING; it is a grid based multi resolution clustering method. In STING method  all the objects are contained into rectangular cells  these cells are kept into various levels of resolutions and these levels are arranged in a hierarchical structure.
Ques_235,What Is ODS? ,Hard,ODS means Operational Data Store. A collection of operation or bases data that is extracted from operation databases and standardized  cleansed  consolidated  transformed  and loaded into an enterprise data architecture. An ODS is used to support data mining of operational data  or as the store for base data that is summarized for a data warehouse. The ODS may also be used to audit the data warehouse to assure summarized and derived data is calculated properly. The ODS may further become the enterprise shared operational database  allowing operational systems that are being reengineered to use the ODS as there operation databases.
Ques_236,What is Standalone mode in Hadoop ?,Easy,The default mode of Hadoop  it uses local file system for input and output operations. This mode is mainly used for the debugging purpose  and it does not support the use of HDFS. Further  in this mode  there is no custom configuration required for mapred-site.xml  core-site.xml  and hdfs-site.xml files. This mode works much faster when compared to other modes.
Ques_237,What is Pseudo-distributed mode in Hadoop ?,Easy,In this case  you need configuration for all the three files mentioned above. In this case  all daemons are running on one node  and thus both Master and Slave nodes are the same.
Ques_238,What is distributed cache?,Easy,Distributed cache in Hadoop is a service by MapReduce framework to cache files when needed.Once a file is cached for a specific job  Hadoop will make it available on each DataNode both in system and in memory  where map and reduce tasks are executing. Later  you can easily access and read the cache file and populate any collection (like array  hashmap) in your code.
Ques_239,What is NameNode ?,Medium,NameNode is the core of HDFS that manages the metadata—the information of which file maps to which block locations and which blocks are stored on which DataNode. In simple terms  it’s the data about the data being stored. NameNode supports a directory tree-like structure consisting of all the files present in HDFS on a Hadoop cluster.
Ques_240,What is Checkpoint NameNode ?,Medium,Checkpoint NameNode has the same directory structure as NameNode and creates Checkpoints for namespace at regular intervals by downloading the fsimage  editing files  and margining them within the local directory. The new image after merging is then uploaded to NameNode. There is a similar node like Checkpoint  commonly known as the Secondary Node  but it does not support the ‘upload to NameNode’ functionality.
Ques_241,What is Backup Node ?,Medium,Backup Node provides similar functionality as Checkpoint  enforcing synchronization with NameNode. It maintains an up-to-date in-memory copy of the file system namespace and doesn’t require getting hold of changes after regular intervals. The Backup Node needs to save the current state in-memory to an image file to create a new Checkpoint.
Ques_242,What is DataNode ?,Medium,DataNode stores data in HDFS; it is a node where actual data resides in the file system. Each DataNode sends a heartbeat message to notify that it is alive. If the NameNode does not receive a message from the DataNode for 10 minutes  the NameNode considers the DataNode to be dead or out of place and starts the replication of blocks that were hosted on that DataNode such that they are hosted on some other DataNode. A BlockReport contains a list of the all blocks on a DataNode. Now  the system starts to replicate what were stored in the dead DataNode.
Ques_243,What is a SequenceFile in Hadoop?,Hard,Extensively used in MapReduce I/O formats  SequenceFile is a flat file containing binary key–value pairs. The map outputs are stored as SequenceFile internally. It provides Reader  Writer  and Sorter classes. The three SequenceFile formats are : Uncompressed key–value records Record compressed key–value records—only ‘values’ are compressed here Block compressed key–value records—both keys and values are collected in ‘blocks’ separately and compressed. The size of the ‘block’ is configurable
Ques_244,What is RecordReader in Hadoop?,Hard,Though InputSplit defines a slice of work  it does not describe how to access it. Here is where the RecordReader class comes into the picture  which takes the byte-oriented data from its source and converts it into record-oriented key–value pairs such that it is fit for the Mapper task to read it. Meanwhile  InputFormat defines this Hadoop RecordReader instance.
Ques_245,What is Speculative Execution in Hadoop?,Hard,One limitation of Hadoop is that by distributing the tasks on several nodes  there are chances that few slow nodes limit the rest of the program. There are various reasons for the tasks to be slow  which are sometimes not easy to detect. Instead of identifying and fixing the slow-running tasks  Hadoop tries to detect when the task runs slower than expected and then launches other equivalent tasks as backup. This backup mechanism in Hadoop is speculative execution.It creates a duplicate task on another disk. The same input can be processed multiple times in parallel. When most tasks in a job comes to completion  the speculative execution mechanism schedules duplicate copies of the remaining tasks (which are slower) across the nodes that are free currently. When these tasks are finished  it is intimated to the JobTracker. If other copies are executing speculatively  Hadoop notifies the TaskTrackers to quit those tasks and reject their output.
Ques_246,What is data modeling?,Easy,A data model is a conceptual representation of business requirement (logical data model) or database objects (physical) required for a database and are very powerful in expressing and communicating the business requirements and database objects. The approach by which data models are created is called as data modeling.
Ques_247,What is  logical data modeling?,Easy,A logical data model is the version of a data model that represents the business requirements (entire or part of an organization). This is the actual implementation and extension of a conceptual data model. Logical Data Models contain Entity  Attributes  Super Type  Sub Type  Primary Key  Alternate Key  Inversion Key Entry  Rule  Relationship  Definition
Ques_248,What is a surrogate key?,Medium,In normal practice  a numerical attribute is enforced a primary key which is called as surrogate key. Surrogate key is a substitute for natural keys. Instead of having primary key or composite primary keys  the data modelers create a surrogate key; this is very useful for creating SQL queries  uniquely identify a record and good performance.
Ques_249,What is  star flake schema ?,Medium,The star schema is where there are one or more fact tables referencing any number of dimension tables in a star schema. Usually  the fact tables in a star schema are created from the third normal form (3NF) with foreign keys and aggregates (sometimes called “measures.”)
Ques_250,What’s a Junk Dimension?,Hard,This is a grouping of low-cardinality attributes like indicators and flags  removed from other tables  and subsequently “junked” into an abstract dimension table. They are often used to initiate Rapidly Changing Dimensions within data warehouses.
Ques_251,What is Data Mart?,Medium,A data mart is the most straightforward set of data warehousing and is used to focus on one functional area of any given business. Data marts are a subset of data warehouses oriented to a specific line of business or functional area of an organization (e.g.  marketing  finance  sales). Data enters data marts by an assortment of transactional systems  other data warehouses  or even external sources.
Ques_252,What is Granularity?,Medium,Granularity represents the level of information stored in a table. Granularity is defined as high or low. High granularity data contains transaction-level data. Low granularity has low-level information only  such as that found in fact tables.
Ques_253,What is Slowly Changing Dimensions? ,Hard,Slowly Changing dimensions are dimensions which are used to manage both historical data as well as the current data in data-warehousing.
Ques_254,What is DNS?,Easy,“The Domain Name System is a hierarchical and decentralised naming system for computers  services  or other resources connected to the Internet or a private network. It associates various information with domain names assigned to each of the participating entities.”
Ques_255,What is a Load Balancer ?,Easy,“A load balancer is a device that distributes network or application traffic across a cluster of servers. Load balancing improves responsiveness and increases availability of applications.A load balancer sits between the client and the server farm accepting incoming network and application traffic and distributing the traffic across multiple backend servers using various methods. By balancing application requests across multiple servers  a load balancer reduces individual server load and prevents any one application server from becoming a single point of failure  thus improving overall application availability and responsiveness.”
Ques_256,What is a Web Application Servers?,Easy,A web server‘s fundamental job is to accept and fulfils requests from clients for static content from a website (HTML pages  files  images  video  and so on). The client is almost always a browser or mobile application and the request takes the form of a Hypertext Transfer Protocol (HTTP) message  as does the web server’s response.”
Ques_257,What is  a Caching Service ?,Medium,“In computing  a cache is a high-speed data storage layer which stores a subset of data  typically transient in nature  so that future requests for that data are served up faster than is possible by accessing the data’s primary storage location. Caching allows you to efficiently reuse previously retrieved or computed data.”
Ques_258,What are Job Queues?,Medium,“In system software  a job queue  is a data structure maintained by job scheduler software containing jobs to run. Users submit their programs that they want executed  “jobs”  to the queue for batch processing. The scheduler software maintains the queue as the pool of jobs available for it to run.”
Ques_259,What is CDN ?,Easy,“A content delivery network (CDN) refers to a geographically distributed group of servers which work together to provide fast delivery of Internet content. A CDN allows for the quick transfer of assets needed for loading Internet content including HTML pages  javascript files  stylesheets  images  and videos. The popularity of CDN services continues to grow  and today the majority of web traffic is served through CDNs  including traffic from major sites like Facebook  Netflix  and Amazon.”
Ques_260,What is Dependency Injection?,Hard,In software engineering  dependency injection is a technique whereby one object supplies the dependencies of another object. A dependency is an object that can be used. An injection is the passing of a dependency to a dependent object that would use it. The service is made part of the client’s state. Dependency injection is also a creational design pattern.
Ques_261,What Is CAP Theorem? ,Hard,The CAP Theorem states that it is not possible for a distributed computer system to simultaneously provide all three of the following guarantees -Consistency Availability Partition tolerance.
Ques_262,What is continuous deployment?,Medium,Continuous deployment goes one step further than continuous delivery. With this practice  every change that passes all stages of your production pipeline is released to your customers. There’s no human intervention  and only a failed test will prevent a new change to be deployed to production.
Ques_263,What is Abstract Factory pattern?,Easy,Abstract Factory patterns work around a super-factory which creates other factories. This factory is also called as factory of factories. This type of design pattern comes under creational pattern as this pattern provides one of the best ways to create an object.
Ques_264,What is Singleton pattern?,Easy,Singleton pattern is one of the simplest design patterns in Java. This type of design pattern comes under creational pattern as this pattern provides one of the best ways to create an object.
Ques_265,What is Builder pattern?,Easy,Builder pattern builds a complex object using simple objects and using a step by step approach. This builder is independent of other objects.
Ques_266,What is Mediator pattern?,Medium,Mediator pattern is used to reduce communication complexity between multiple objects or classes. This pattern provides a mediator class which normally handles all the communications between different classes and supports easy maintenance of the code by loose coupling. Mediator pattern falls under behavioral pattern category.
Ques_267,What is Bridge pattern?,Medium,Bridge is used when we need to decouple an abstraction from its implementation so that the two can vary independently. This type of design pattern comes under structural pattern as this pattern decouples implementation class and abstract class by providing a bridge structure between them.
Ques_268,What is Filter pattern?,Medium,Filter pattern or Criteria pattern is a design pattern that enables developers to filter a set of objects using different criteria and chaining them in a decoupled way through logical operations. This type of design pattern comes under structural pattern as this pattern combines multiple criteria to obtain single criteria.
Ques_269,What is Strategy pattern?,Medium,In Strategy pattern  we create objects which represent various strategies and a context object whose behavior varies as per its strategy object. The strategy object changes the executing algorithm of the context object.
Ques_270,What is Business Delegate pattern?,Hard,Business Delegate Pattern is used to decouple presentation tier and business tier. It is basically use to reduce communication or remote lookup functionality to business tier code in presentation tier code. In business tier we have following entities -Client Business Delegate LookUp Service  Business Service.
Ques_271,What is Composite Entity pattern?,Hard,Composite Entity pattern is used in EJB persistence mechanism. A Composite entity is an EJB entity bean which represents a graph of objects. When a composite entity is updated  internally dependent objects beans get updated automatically as being managed by EJB entity bean. Following are the participants in Composite Entity Bean - Composite Entity Coarse-Grained Object Dependent Object  Strategies.
Ques_272,What is Service Locator pattern?,Hard,The service locator design pattern is used when we want to locate various services using JNDI lookup. Considering high cost of looking up JNDI for a service  Service Locator pattern makes use of caching technique. For the first time a service is required  Service Locator looks up in JNDI and caches the service object. Further lookup or same service via Service Locator is done in its cache which improves the performance of application to great extent. 
Ques_273,What is Sashimi? ,Easy,Sashimi in scrum methodology means every phase of the software development cycle in a sprint which includes requirement analysis  planning &  design  development  testing  documentation is complete or not and the product is ready to be displayed  
Ques_274,What is the Release candidate? ,Easy,The release candidate is a code /version /build released to make sure that during the last development period  no critical problem is left behind. It is used for testing and is equivalent to the final build.
Ques_275,What is Kanban?,Medium,Kanban is a tool which helps the team to keep a close eye the work i.e.  to measure its progress. Apart from the progress  the status of a development story can be seamlessly described with the help of ‘kanban board’.Kanban board aids in writing the whole scenario of a project at a single place to give a perfect picture of the bottleneck  a task done  workflow progress. It helps in the continuous delivery of the product without overburdening the team. 
Ques_276,What is Zero Sprint in Agile ?,Medium,In Agile Methodology  Zero Sprint refers to the first step that comes before the first sprint. So  it is more like a pre-step to the first sprint. Thus  Zero Sprint would include a host of activities that are to be completed before starting a project  including setting up the development environment  preparing backlog  and other such tasks that are usually done before beginning the actual development process. 
Ques_277,What is Spike in Agile ?,Hard, Spike is the type of story that can be taken between the sprints. Spikes are commonly used for the activities related to the design or technical issues such as research  design  prototyping  and exploration. There are two types of spikes – functional spikes and technical spikes.
Ques_278,What is Black-box testing?,Easy,It is a testing strategy based solely on requirements and specifications. In this strategy  it requires no knowledge of internal paths  structures  or implementation of the software being tested.
Ques_279,What is White box testing?,Easy,It is a testing strategy based on internal paths  code structures  and implementation of the software being tested. White box testing generally requires detailed programming skills.
Ques_280,What is usability testing?,Easy,It is a testing methodology where the end customer is asked to use the software to see if the product is easy to use  to see the customer’s perception and task time. An accurate way to finalize the customer point of view for usability is by using prototype or mock-up software during the initial stages.
Ques_281,What is meant by Defect Cascading?,Medium,Defect cascading is a defect which is caused by another defect. One defect triggers the other defect. When a defect is present in any stage but is not identified  hide to other phases without getting noticed. This will result in an increase in the number of defects.
Ques_282,What is Test Harness?,Medium,A test harness is the collection of software and test data configured to test a program unit by running it under varying conditions which involves monitoring the output with expected output.
Ques_283,What is Smoke Testing?,Medium,Smoke Testing is done to make sure if the build we received from the development team is testable or not. It is also called as “Day 0” check. It is done at the “build level”. It helps not to waste the testing time to simply testing the whole application when the key features don’t work or the key bugs have not been fixed yet.
Ques_284,What is Fuzz Testing?,Medium,Fuzz testing is used to identify coding errors and security loopholes in an application. By inputting massive amount of random data to the system in an attempt to make it crash to identify if anything breaks in the application.
Ques_285,What is Pesticide Paradox?,Hard,Pesticide Paradox in software testing is the process of repeating the same test cases  again and again  eventually  the same test cases will no longer find new bugs. So to overcome this Pesticide Paradox  it is necessary to review the test cases regularly and add or update them to find more defects.
Ques_286,What is Error Seeding?,Hard,Error seeding is a process of adding known errors intendedly in a program to identify the rate of error detection. It helps in the process of estimating the tester skills of finding bugs and also to know the ability of the application (how well the application is working when it has errors
Ques_287,What is Equivalence Class Partition?,Hard,Equivalence Partitioning is also known as Equivalence Class Partitioning. In equivalence partitioning  inputs to the software or system are divided into groups that are expected to exhibit similar behavior  so they are likely to be proposed in the same way. Hence selecting one input from each group to design the test cases
Ques_288,What Is Dns?,Easy,Domain Naming System(DNS) resolves IP address to the hostname(domain) and hostname(domain name) to IP address.
Ques_289,What Is Dhcp?,Easy,Dynamic Host Configuration Protocol (DHCP) helps in automatically allocation IP address over the network with defined scope.
Ques_290,What is the difference between Transmission Control Protocol (TCP) and User Datagram Protocol (UDP)? When should you use each,Medium,TCP is known for reliability  and is a commonly used protocol on the internet. It’s used for web browsing  email and remote admin. Its built-in QoS (Quality of Service) guarantees the recipient receives the message in a specified order  and that the sender gets confirmation from the recipient. If there’s a disruption  the recipient will get an error message  and messages will be re-sent until the sender gets confirmation. TCP also has error checks  which guarantees packets aren’t corrupted during transmission. “UDP represents speed over reliability. It doesn’t have error checks or QoS; packets are just sent in a constant stream to the recipient  without any confirmation. If there’s a disruption during the stream  UDP jumps to the next current packet. It’s most commonly used for online games and live streaming.
Ques_291,Explain what is RAID in Windows Server?,Medium,For storing the same data at a different place RAID or Redundant Array of Independent Disks strategy is used. It is a strategy for building fault tolerance and increase storage capacity. On separate drives it allows you to combine one or more volumes so that they are accessed by a single drive letter
Ques_292,Explain what is the major difference between NTFS ( New Technology File System) or FAT (File Allocation Table) on a local server?,Hard,For local users FAT (File Allocation Table) and FAT32 provides security  while NTFS ( New Technology File System) provides security for domain users as well as local users.  NTFS provides file level security which is not possible through FAT32.
Ques_293,How  to find WWN numbers of HBA cards in Linux Server ?,Hard,We can find the WWN numbers of HBA cards using the command ‘systool -c fc_host -v | grep port_name’
Ques_294,How do you define health informatics?,Easy,In simplest terms  health informatics is the science that deals with health information  its structure  acquisition and use.
Ques_295,What are some specific applications of health informatics?,Easy,Health informatics can be thought of as having three major branches. The first  bioinformatics  focuses on the molecular level. It is the use of information technologies to understand molecular and cellular processes. We have amazing new ways to measure molecules. We can sequence people’s genes; we can measure the expression of those genes; we can even measure the expression of the proteins they encode. We have huge amounts of publicly available data. And people are trying to use these new measurements to figure out what the mechanisms are that underlie health and disease. Medical informatics  the second branch  is largely about how to use patient information. Every day we treat thousands of patients across the street [at Johns Hopkins Hospital] and there is a medical record about each encounter. In the past those were written down on paper. Now they are digitized and stored on computer. So now we have all this phenotypic information—everything from lab results  X-rays  MRI images and genetic information to basic symptoms—that represents the health status of patients. The question is how to use the information to first figure out the mechanisms of the disease and then work out better ways to treat each patient. Finally  public health informatics deals with monitoring and measuring the health of a population. For example  one of the United Nations’ Millennium Development Goals is to substantially reduce infant mortality worldwide. To know whether we are attaining the goal  we must measure this outcome. Public health informatics can provide better tools for acquiring  managing and using the requisite data for tracking this important goal. In all three types of informatics  the question is how best to utilize modern technology to measure more effectively and then to manage and analyze the data to understand what is going on.
Ques_296,What are the key models of health informatics,Medium,There are several key models in health informatics that approach how to effectively integrate technology systems and people • These include: – Technology Acceptance Model – Disruptive Innovation – Diffusion of Innovation – Sociotechnical Theory Model
Ques_297,What are Clinical informatics?,Medium,Clinical informatics* is a subset of health informatics used by clinicians in the application deliver healthcare services • It blends information technology into clinical care processes  usually within a health system • It is also referred to as applied clinical informatics and operational informatics – Clinical informatics includes a wide range of topics ranging from clinical decision support to visual images (e.g.  radiological  pathological  dermatological  ophthalmological  etc.); from clinical documentation to provider order entry systems; and from system design to system implementation and adoption issues
Ques_298,What is Technology Acceptance Model (TAM)?,Hard,Technology Acceptance Model (TAM) is an information systems’ theory that models how users come to accept and use a new technology when presented. Because new technologies are constantly introduced to healthcare  the rate of user acceptance depends heavily on perceived purpose and accessibility of the technology
Ques_299,What is Sociotechnical Theory Model ?,Hard,Sociotechnical Theory Model is an information systems’ model that recognizes the interaction between social and technical sub-systems. These two factors are then compared and used together to form the design of a technology
Ques_300,what are the phases of project management life cycle,Easy,There are five phases of project management. These phases are as follows: 1.Initiation: This marks the beginning of the project and is where the project charter is developed and where stakeholders are identified. 2. Planning: This is where the project plan is developed. That means costs are estimated  resources are determined  and requirements (scope  and work breakdown structure) are defined. This is also where risk is identified and planned for  and where communications are built. 3. Execution: This is where the project is carried out  all while procuring resources and managing stakeholder expectations. 4. Performance monitoring: Often this phase is carried out simultaneously with execution because this is where quality is monitored  as well as scope creep  and cost/time allocations are watched in order to keep things within budget. 5. Closure: This is where the project is finalized  where the deliverable is given to the customer  where stakeholders are told of the completion of the project  and all resources are released back to their resource managers.
Ques_301,What is project management?,Easy,Project management is the application of processes  methods  skills  knowledge and experience to achieve specific project objectives according to the project acceptance criteria within agreed parameters. Project management has final deliverables that are constrained to a finite timescale and budget. A key factor that distinguishes project management from just 'management' is that it has this final deliverable and a finite timespan  unlike management which is an ongoing process. Because of this a project professional needs a wide range of skills; often technical skills  and certainly people management skills and good business awareness.
Ques_302,Which of the options is a conflict resolution technique?,Medium,Conflict resolution techniques that may be used on a project include confronting  smoothing  forcing  and Withdrawing.
Ques_303,A schedule performance index  SPI  of 0.75 means:,Medium,Project is progressing at 75% of the rate originally planned
Ques_304,A project manager is quantifying risk for her project. She needs expert opinion in this process and related experts are spread over to different geographical locations. How can she continue?,Hard,Applying the Delphi Technique
Ques_305,A project manager is in the middle of creating a request for proposal (RFP). What part of the procurement process is she in?,Hard,she is in plan procurements process. Because procurement statement of work  procurement documents are prepared in plan procurements management process. And RFP is a procurement document as well. Therefore  it is prepared in plan procurements process. RFP was defining what the buyer is requiring from the seller.
Ques_306,What is Active Directory?,Easy,Active Directory provides a centralised control for network administration and security. Server computers configured with Active Directory are known as domain controllers. Active Directory stores all information and settings for a deployment in a central database  and allows administrators to assign policies and deploy and update software.
Ques_307,What is a Domain?,Easy,A domain is defined as a logical group of network objects (computers  users  devices) that share the same Active Directory database. A tree can have multiple domains.
Ques_308,Which port is used for the ping command?,Medium,The ping command uses ICMP. Specifically  it uses ICMP echo requests and ICMP echo reply packets. ICMP does not use either UDP or TCP communication services: Instead  it uses raw IP communication services. This means that the ICMP message is carried directly in an IP datagram data field.
Ques_309,What is the difference between a router and a gateway? What is the default gateway?,Medium,"Router describes the general technical function (layer 3 forwarding)  or a hardware device intended for that purpose  while gateway describes the function for the local segment (providing connectivity to elsewhere). You could also state that you ""set up a router as a gateway."" Another term is hop  which describes forwarding between subnets. The term default gateway is used to mean the router on your LAN  which has the responsibility of being the first point of contact for traffic to computers outside the LAN."
Ques_310,How do you protect your system from getting hacked?,Hard,By following the principle of least privileges and these practices: Encrypt with public keys  which provides excellent security. Enforce password complexity. Understand why you are making exceptions to the rules above. Review your exceptions regularly. Hold someone to account for failure. (It keeps you on your toes.)
Ques_311,What are sticky ports?,Hard,Sticky ports are one of the network administrator’s best friends and worst headaches. They allow you to set up your network so that each port on a switch only permits one (or a number that you specify) computer to connect on that port  by locking it to a particular MAC address.
Ques_312,What is Counterintelligence?,Easy,Counterintelligence  or CI  as defined by Executive Order 12333  as amended  is “information gathered and activities conducted to identify  deceive  exploit  disrupt  or protect against espionage  other intelligence activities  sabotage  or assassinations conducted by or on behalf of foreign powers  organizations or persons or their agents  or international terrorist organizations or activities.”
Ques_313,What is the Core Concerns of Counterintelligence?,Easy,The core concerns of CI are the intelligence entities of foreign states and similar organizations of non-state actors  such as terrorist organizations and the trusted insider.
Ques_314,What is a Security Anomaly?,Medium,“Foreign power activity or knowledge which is inconsistent with the expected norm that suggests that foreign powers have knowledge of U.S. national security.” Examples of anomalies include: • An adversary conducts activities with precision that indicates prior knowledge. • An adversary uses technical countermeasures to block a previously undisclosed or classified U.S. intercept technology. • Foreign officials reveal details they should not have known. • An adversary is able to anticipate DoD plans and activities • Media is waiting where a sensitive DoD program will be tested
Ques_315,What are Indicators of Foreign Intelligence Entities  Targeting?,Medium,Some indicators of Foreign Intelligence Entities (FIE) targeting are: • Being invited to lecture/attend a conference in a foreign country • Singled out for socializing or special attention • Meeting a foreign national and becoming romantically involved  and • Becoming personally involved with known/suspected foreign intelligence officer or foreign intelligence entity
Ques_316,What does Intelligence Collection Tradecraft include?,Hard,Many nations’ intelligence organizations target defense information  and they will do all they can to obtain it. As government employees  our greatest vulnerabilities are those things we take for granted. For example  Foreign Intelligence Entities use: • Intercepts of cell phones  or other wireless signals • Intercepts of open telephone lines • Intercepts in hotels while TDY • Looking through the trash • Simple conversations  online or in person  and • Hacking into unclassified or classified systems
Ques_317,What does methods of operation or “MO” frequently used by foreign intelligence to collect information include?,Hard,Some methods of operation or “MO” frequently used by foreign intelligence to collect information include: • Elicitation • Unsolicited requests for information • Visits to DoD installations or facilities • International conventions  seminars  and exhibits • Solicitation and marketing of services  and • Cyber Intelligence Gathering
Ques_318,What is Multivariate Statistical Analysis?,Easy,Multivariate statistical analysis refers to multiple advanced techniques for examining relationships among multiple variables at the same time. Researchers use multivariate procedures in studies that involve more than one dependent variable (also known as the outcome or phenomenon of interest)  more than one independent variable (also known as a predictor) or both. Upper-level undergraduate courses and graduate courses in statistics teach multivariate statistical analysis. This type of analysis is desirable because researchers often hypothesize that a given outcome of interest is effected or influenced by more than one thing.
Ques_319,What Is Bayesian?,Easy,Bayesians condition on the data actually observed and consider the probability distribution on the hypotheses.
Ques_320,What Is P-value?,Easy,In statistical significance testing  the p-value is the probability of obtaining a test statistic at least as extreme as the one that was actually observed  assuming that the null hypothesis is true. If the p-value is less than 0.05 or 0.01  corresponding respectively to a 5% or 1% chance of rejecting the null hypothesis when it is true.
Ques_321,What is the difference between inferential statistics and descriptive statistics?,Medium,Descriptive statistics – provides exact and accurate information. Inferential statistics – provides information of a sample and we need to inferential statistics to reach to a conclusion about the population.
Ques_322,How to convert normal distribution to standard normal distribution?,Medium,Standardized normal distribution has mean = 0 and standard deviation = 1 To convert normal distribution to standard normal distribution we can use the formula X (standardized) = (x-µ) / σ
Ques_323,Why we need sample statistics?,Hard,Population parameters are usually unknown hence we need sample statistics.
Ques_324,How to find the mean length of all fishes in the sea?,Hard,Define the confidence level (most common is 95%) Take a sample of fishes from the sea (to get better results the number of fishes > 30) Calculate the mean length and standard deviation of the lengths Calculate t-statistics Get the confidence interval in which the mean length of all the fishes should be.
Ques_325,What is Rapid Prototyping?,Easy,Rapid Prototyping is a manufacturing technology that quickly builds a prototype part. Many different technologies are available that are considered Rapid Prototyping  and many can also be used for production manufacturing. Although most Rapid Prototyping systems use a form of layered additive manufacturing  they can also use a variety of other methods such as high-speed machining  molding  casting  and extruding.
Ques_326,What is Rapid Tooling and how is it Different from Rapid Prototyping?,Easy,The only difference between Rapid Tooling and Rapid Manufacturing is the end use of the parts produced with the process. Both use rapid prototyping technologies to quickly make a part. But for Rapid Tooling  the part is used in another manufacturing process as a tool.
Ques_327,What is 3D Printing and how is it Different from Rapid Prototyping?,Medium,3D Printing refers to a subset of rapid prototyping that goes directly from a 3D computer model to a prototype with very little user interaction other than defining some preferences. The process is designed to be as easy as printing from a computer to paper. In many ways the name is a marketing label to clearly emphasize the affordability and ease of making prototypes using systems that are labeled as 3D Printers. It is also meant to appeal to a larger  less engineering and manufacturing oriented audience. PADT uses 3D Printing systems as well as Rapid Prototyping and Manufacturing systems.
Ques_328,What is Layered Manufacturing and why do most Rapid Prototyping Technologies Use it?,Medium,Layered Manufacturing builds parts up  one thin layer at a time. Most traditional manufacturing methods start with a block and remove material  or shapes material using a tool of some kind. Layered manufacturing is often called Additive Manufacturing because it adds material rather than taking it away or molding it. The best way to visualize layered manufacturing is to think of taking a real part and chopping it into very thin layers. Then stack those layers back up one on top of the other. Layered manufacturing does the chopping in a computer program  and tells a machine how to create each layer.
Ques_329,My buddy has a MakerBot/RepRap/Build-your-Own-3D-Printer. How is that different from these commercial Rapid Prototyping systems?,Hard,There has been an explosion of do it yourself RP systems at around 2010-2011. Most of these are based on the fact that the patent for Fused Deposition modeling ran out. The majority of homemade systems  or personal systems  are variations on the systems made for decades by Stratasys. They differ from commercial or industrial systems in two ways: lower cost  and fewer capabilities. In general  the parts made on these systems are not usable for engineering or even visualization models because the material is too soft  the material does not fully harden or bond  there is considerable shrinkage or warping  and the actual precision of the device is low.
Ques_330,Is there free software out there that I can use to look at my model before I send it to you? Can I convert a file I made for animation or rendering to a file you can use?,Hard,Yes. Meshlab is a tool for dealing with all types of faced data and it works with STL files as well. It can be sued for translating  repair and visualization. [http://meshlab.sourceforge.net/] MiniMagics is a free STL viewer from Materialise [http://software.materialise.com/minimagics].
Ques_331, What is Microsoft Excel?,Easy,Microsoft Excel is an electronic spreadsheet application that enables users to store  organize  calculate and manipulate the data with formulas using a spreadsheet system broken up by rows and columns. It also provides the flexibility to use an external database to do analysis  make reports  etc. thus saving lots of time.
Ques_332,What is ribbon?,Easy,Ribbon refers to the topmost area of the application that contains menu items and toolbars available in MS-Excel. Ribbon can be shown/hidden using CTRL+F1. The ribbon runs on the top of the application and is the replacement for the toolbars and menus. The ribbons have various tabs on the top  and each tab has its own group of commands.
Ques_333,Difference between COUNT  COUNTA  COUNTIF and COUNTBLANK in Ms-Excel.,Medium,COUNT is used to count cells containing numbers  dates  etc. any value stored as number excluding blanks. COUNTA or Count All is used to count any cell value containing numbers  text  logical values  etc. any type of value excluding blanks. COUNTBLANK count blank cells or cells with an empty string. COUNTIF and COUNTIFS count cells matching a certain criteria.
Ques_334,What is the use of LOOKUP function in Excel?,Medium,In Microsoft Excel  the LOOKUP function returns a value from a range or an array.
Ques_335,How cell reference is useful in the calculation?,Hard,In order to avoid writing the data again and again for calculating purpose  cell reference is used. When you write any formula  for specific function  you need to direct Excel the specific location of that data. This location is referred as  cell reference. So  every time a new value added to the cell  the cell will calculate according to the reference cell formula.
Ques_336,What filter will you use  if you want more than two conditions or if you want to analyze the list using database function?,Hard,You will use Advanced Criteria Filter  to analyze the list or if more than two conditions should be tested.
